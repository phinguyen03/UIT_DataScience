{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8e55535a13da4b3d95269cf10da80c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2bae44cf673f40d3a20ec9ca8f1374d7",
              "IPY_MODEL_020d65bc86744457aeea90c3c60c2196",
              "IPY_MODEL_83ead130d62949a386a77d636902b65d"
            ],
            "layout": "IPY_MODEL_48e87188873f4d98ac2bdef2caab5aeb"
          }
        },
        "2bae44cf673f40d3a20ec9ca8f1374d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d071061d78d843fe81d39e62a2e5fb9a",
            "placeholder": "​",
            "style": "IPY_MODEL_9482f444f2e846bd9b75f96d9b9b590d",
            "value": "config.json: 100%"
          }
        },
        "020d65bc86744457aeea90c3c60c2196": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2991976398441c79a7b6f1df2f88ecd",
            "max": 631,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be0f8b60d0b4422cbebfe501bb32f7ee",
            "value": 631
          }
        },
        "83ead130d62949a386a77d636902b65d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2a4d8210957420eb36817f09b0f8eae",
            "placeholder": "​",
            "style": "IPY_MODEL_115c8f7239284d43b42fa1911d9781ef",
            "value": " 631/631 [00:00&lt;00:00, 31.6kB/s]"
          }
        },
        "48e87188873f4d98ac2bdef2caab5aeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d071061d78d843fe81d39e62a2e5fb9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9482f444f2e846bd9b75f96d9b9b590d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2991976398441c79a7b6f1df2f88ecd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be0f8b60d0b4422cbebfe501bb32f7ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2a4d8210957420eb36817f09b0f8eae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "115c8f7239284d43b42fa1911d9781ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a0503460b0941ffbb65dba52439fdf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8399452c47543f4b18bde487f272cb2",
              "IPY_MODEL_27839091c91946c191714bf97af69564",
              "IPY_MODEL_ad2046faa5244eaa8358c71be1c11b31"
            ],
            "layout": "IPY_MODEL_770b2de78ace44ab9d8e57ff7a07ad83"
          }
        },
        "b8399452c47543f4b18bde487f272cb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_caede153314140da943b965b8cd71379",
            "placeholder": "​",
            "style": "IPY_MODEL_f2435bb4301c4b2e98513792de39c196",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "27839091c91946c191714bf97af69564": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d240b59a5374c3aa1a13e834c86dade",
            "max": 448356189,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d415f05371a64d339a8a5a8bf2a5e8f2",
            "value": 448356189
          }
        },
        "ad2046faa5244eaa8358c71be1c11b31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7eb3ddcf7bcf47ad872d184d9dfcbda3",
            "placeholder": "​",
            "style": "IPY_MODEL_884b6d85f86942f1812249be4efc2fa5",
            "value": " 448M/448M [00:34&lt;00:00, 7.87MB/s]"
          }
        },
        "770b2de78ace44ab9d8e57ff7a07ad83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "caede153314140da943b965b8cd71379": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2435bb4301c4b2e98513792de39c196": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d240b59a5374c3aa1a13e834c86dade": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d415f05371a64d339a8a5a8bf2a5e8f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7eb3ddcf7bcf47ad872d184d9dfcbda3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "884b6d85f86942f1812249be4efc2fa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57c8981d26c44f30962bab92e575238d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8a14ad946d3488db1dc677f89345f9f",
              "IPY_MODEL_caa6a58e63334d758e4951e768574513",
              "IPY_MODEL_5a8f7f8517a84e16a98b44df7f3240c8"
            ],
            "layout": "IPY_MODEL_c552adef3466493e8a2145b9bab0b8d9"
          }
        },
        "a8a14ad946d3488db1dc677f89345f9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01733bdd6f544210a64e4fecd10655db",
            "placeholder": "​",
            "style": "IPY_MODEL_99b88962681c4bc18f7a80ec144076f4",
            "value": "config.json: 100%"
          }
        },
        "caa6a58e63334d758e4951e768574513": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7b15053c4524031af278e2455b2788a",
            "max": 557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da0ef2eea0cb4665829113e7b974177c",
            "value": 557
          }
        },
        "5a8f7f8517a84e16a98b44df7f3240c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f18e33b7723247be97ddb77829393a50",
            "placeholder": "​",
            "style": "IPY_MODEL_d53c9fd1e8fa4c32999cec1032c83238",
            "value": " 557/557 [00:00&lt;00:00, 37.4kB/s]"
          }
        },
        "c552adef3466493e8a2145b9bab0b8d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01733bdd6f544210a64e4fecd10655db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99b88962681c4bc18f7a80ec144076f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7b15053c4524031af278e2455b2788a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da0ef2eea0cb4665829113e7b974177c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f18e33b7723247be97ddb77829393a50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d53c9fd1e8fa4c32999cec1032c83238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c6ae9ad8d514e1d83c5da8eba0cb4f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed19d3c6cc774b0a81133f4b9bb59306",
              "IPY_MODEL_8a179329f89243b88d57200a58cf84cc",
              "IPY_MODEL_6c843da549a24d3385f4b9c54e5b6675"
            ],
            "layout": "IPY_MODEL_f9a5e4989296406dbe6e9057d5012cbe"
          }
        },
        "ed19d3c6cc774b0a81133f4b9bb59306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32cf42639b6642eaaa9fcdd742b7f23d",
            "placeholder": "​",
            "style": "IPY_MODEL_92cb03b127c94193b5ef6d1ba54674a1",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "8a179329f89243b88d57200a58cf84cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1983c74f0fcc496eb0e6d93ae83ef01f",
            "max": 542923308,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_090ad52ce9964babb7ac6726a6fd1742",
            "value": 542923308
          }
        },
        "6c843da549a24d3385f4b9c54e5b6675": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af84f31944bc46bebb5acdc12b7baaa6",
            "placeholder": "​",
            "style": "IPY_MODEL_6d2ae9f3837b4852a5a9fac603dd557c",
            "value": " 543M/543M [00:03&lt;00:00, 173MB/s]"
          }
        },
        "f9a5e4989296406dbe6e9057d5012cbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32cf42639b6642eaaa9fcdd742b7f23d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92cb03b127c94193b5ef6d1ba54674a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1983c74f0fcc496eb0e6d93ae83ef01f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "090ad52ce9964babb7ac6726a6fd1742": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af84f31944bc46bebb5acdc12b7baaa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d2ae9f3837b4852a5a9fac603dd557c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d54fa4e6e8774ed1a686d64128d478a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98a3952e95144efc94c43ccb7c66a702",
              "IPY_MODEL_8e7d5be9ddd745aa8e2e49f0a4fda505",
              "IPY_MODEL_0a0eb7485eb743bc81bcf407b6d42267"
            ],
            "layout": "IPY_MODEL_1cbd9975645549aaae8373ced3db4ae5"
          }
        },
        "98a3952e95144efc94c43ccb7c66a702": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52abcbe567724afab50d6bcb9af8ba6c",
            "placeholder": "​",
            "style": "IPY_MODEL_134179b4b9eb4841a69d5eedd7608a3a",
            "value": "vocab.txt: 100%"
          }
        },
        "8e7d5be9ddd745aa8e2e49f0a4fda505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e314f8f820b4660b3437104e08c3489",
            "max": 895321,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f7f221acd7e4d7185e7db8f796dd0c5",
            "value": 895321
          }
        },
        "0a0eb7485eb743bc81bcf407b6d42267": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb4880df420e44848134be75cdd34199",
            "placeholder": "​",
            "style": "IPY_MODEL_8665d34ffcef4f4a8c5e3b5162c6eaf8",
            "value": " 895k/895k [00:00&lt;00:00, 4.17MB/s]"
          }
        },
        "1cbd9975645549aaae8373ced3db4ae5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52abcbe567724afab50d6bcb9af8ba6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "134179b4b9eb4841a69d5eedd7608a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e314f8f820b4660b3437104e08c3489": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f7f221acd7e4d7185e7db8f796dd0c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb4880df420e44848134be75cdd34199": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8665d34ffcef4f4a8c5e3b5162c6eaf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c679c9a9dd2491999cfd12beef887a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_386c5ee4467840789fc559628b7aade5",
              "IPY_MODEL_30f3895f78ee4141b8694a77caad0dac",
              "IPY_MODEL_d7769cf25a5447288744b906637b3434"
            ],
            "layout": "IPY_MODEL_c571ab994d484f2196f9284b55598102"
          }
        },
        "386c5ee4467840789fc559628b7aade5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e6addda3f0040fe9ca9892601232646",
            "placeholder": "​",
            "style": "IPY_MODEL_3c4c71500d8a4bb4a3a006b82138318e",
            "value": "bpe.codes: 100%"
          }
        },
        "30f3895f78ee4141b8694a77caad0dac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f8432f5a3b547be91817c70fe792eaf",
            "max": 1135173,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c17750ae8ee4a5287e671fce4d42b17",
            "value": 1135173
          }
        },
        "d7769cf25a5447288744b906637b3434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf61a0f764db414a83557620a70d7da8",
            "placeholder": "​",
            "style": "IPY_MODEL_37f7191e0ccd4227b5320c27ae4e4219",
            "value": " 1.14M/1.14M [00:00&lt;00:00, 1.36MB/s]"
          }
        },
        "c571ab994d484f2196f9284b55598102": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e6addda3f0040fe9ca9892601232646": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c4c71500d8a4bb4a3a006b82138318e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f8432f5a3b547be91817c70fe792eaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c17750ae8ee4a5287e671fce4d42b17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf61a0f764db414a83557620a70d7da8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37f7191e0ccd4227b5320c27ae4e4219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c3601016cb44fbda1c1aedd5519e78e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_201d031b32204092b4130cd416805fc7",
              "IPY_MODEL_21a4e421252e4b32bf01edc988087c38",
              "IPY_MODEL_efe1a1ec2eb147bca770f5d3469ec757"
            ],
            "layout": "IPY_MODEL_d87960635bad4c8590f57ac41787c023"
          }
        },
        "201d031b32204092b4130cd416805fc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_963b25ddf3414b578935f470ec28c5c2",
            "placeholder": "​",
            "style": "IPY_MODEL_fc519e5fcdb546f08d7c498a6c537281",
            "value": "tokenizer.json: 100%"
          }
        },
        "21a4e421252e4b32bf01edc988087c38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e356da2be3b4913bcf64c79f719078e",
            "max": 3132320,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57af0a2fbc66462bb30412b1d6be6918",
            "value": 3132320
          }
        },
        "efe1a1ec2eb147bca770f5d3469ec757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40fa9d2a28c44a8ca2392432c03bc0aa",
            "placeholder": "​",
            "style": "IPY_MODEL_d29c9b56d7d2474f9f821c73273a66c2",
            "value": " 3.13M/3.13M [00:00&lt;00:00, 4.88MB/s]"
          }
        },
        "d87960635bad4c8590f57ac41787c023": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "963b25ddf3414b578935f470ec28c5c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc519e5fcdb546f08d7c498a6c537281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e356da2be3b4913bcf64c79f719078e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57af0a2fbc66462bb30412b1d6be6918": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40fa9d2a28c44a8ca2392432c03bc0aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d29c9b56d7d2474f9f821c73273a66c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K49dl-DmWbTw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import json\n",
        "import math\n",
        "\n",
        "import os\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ik60e-jVKt5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "y1v1_qK2WJlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ijyMezUJGue2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = '/content/drive/MyDrive/public-test-images.zip'\n",
        "\n",
        "# Specify the directory where you want to extract the contents\n",
        "extract_to_dir = '/content/CustomDataset_Test'\n",
        "\n",
        "# Open the ZIP file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extract all the contents of the ZIP file\n",
        "    zip_ref.extractall(extract_to_dir)\n",
        "    print(\"Files extracted successfully to\", extract_to_dir)\n"
      ],
      "metadata": {
        "id": "709fI_9hXIKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0461c4f9-d999-4948-d1a5-b2e1f9b1f8c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted successfully to /content/CustomDataset_Test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_TRAIN_DIR = '/content/CustomDataset_Train/train-images'\n",
        "JSON_TRAIN_PATH = '/content/drive/MyDrive/vimmsd-train.json'\n",
        "IMAGE_TEST_DIR = '/content/CustomDataset_Test/dev-images'\n",
        "JSON_TEST_PATH = '/content/drive/MyDrive/vimmsd-public-test.json'"
      ],
      "metadata": {
        "id": "T2J9AjIiXMwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def access_json_path(JSON_PATH):\n",
        "  with open(JSON_PATH, 'r') as file:\n",
        "      data = json.load(file)\n",
        "      return data"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rDwxL9prYI7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomMultiModalDataset(Dataset):\n",
        "  \"\"\"\n",
        "    Init:\n",
        "\n",
        "      Load DATA_DIR\n",
        "      JSON_PATH\n",
        "      -> Get list string keys\n",
        "\n",
        "    Getitem:\n",
        "\n",
        "      Image\n",
        "      Caption\n",
        "      Label\n",
        "\n",
        "    Return:\n",
        "      Transform_image(4 dimensions), Caption, Label\n",
        "  \"\"\"\n",
        "  def __init__(self, data_folder, json_path, max_length=512, image_transform=None, text_transform=None):\n",
        "        self.data_folder = data_folder\n",
        "        self.json_path = json_path\n",
        "        self.max_length = max_length\n",
        "        self.keys = list(json_path.keys())\n",
        "\n",
        "        # self.text_transform = text_transform\n",
        "\n",
        "        # Define any image transformations if needed\n",
        "        # Transformation -> Step 2\n",
        "        self.image_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.Grayscale(num_output_channels=3),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # AutoImageProcessor expects the mean\n",
        "                                                                   # and standard deviation values to match the number of channels in the input images.\n",
        "        ])\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.keys)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        # Get index\n",
        "        keys = self.keys[idx]\n",
        "        image_idx = self.json_path[keys]['image']\n",
        "\n",
        "        # Read image from hard drive\n",
        "        image_path = os.path.join(self.data_folder, image_idx)\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        # Transform image\n",
        "        transformed_image = self.image_transform(image).unsqueeze(0)\n",
        "\n",
        "        # Caption & Label\n",
        "        caption = self.json_path[keys]['caption']\n",
        "        label_str = self.json_path[keys]['label']\n",
        "\n",
        "        if label_str == 'multi-sarcasm':\n",
        "            label = torch.tensor([0, 1, 0, 0], dtype=torch.float32)\n",
        "        elif label_str == 'image-sarcasm':\n",
        "            label = torch.tensor([0, 0, 1, 0], dtype=torch.float32)\n",
        "        elif label_str == 'text-sarcasm':\n",
        "            label = torch.tensor([0, 0, 0, 1], dtype=torch.float32)\n",
        "        else:\n",
        "            label = torch.tensor([1, 0, 0, 0], dtype=torch.float32)\n",
        "\n",
        "        return {\n",
        "            'image_transformed': transformed_image,\n",
        "            'caption': caption,\n",
        "            'label': label\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Ywd7N3hqXglv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomMultiModalDataset(data_folder=IMAGE_TRAIN_DIR, json_path=access_json_path(JSON_TRAIN_PATH))"
      ],
      "metadata": {
        "id": "zhQe5STa44aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=16,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0) # Setting num_workers to 0 will make the DataLoader run in the main process, which avoids issues related to forking and threading."
      ],
      "metadata": {
        "id": "HCPea3R75Jm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = CustomMultiModalDataset(data_folder=IMAGE_TEST_DIR, json_path=access_json_path(JSON_TEST_PATH))"
      ],
      "metadata": {
        "id": "LTQy1NyIUnIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(test_dataset,\n",
        "                          batch_size=16,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0) # Setting num_workers to 0 will make the DataLoader run in the main process, which avoids issues related to forking and threading."
      ],
      "metadata": {
        "id": "Vd6-9qPFUno0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "UByXNLB2AEO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainloader(train_loader):\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        print(f\"Batch {i}:\")\n",
        "        print(f\"image: {batch['image_transformed']}\")\n",
        "        print(f\"caption: {batch['caption']}\")\n",
        "        print(f\"label: {batch['label']}\")\n",
        "\n",
        "trainloader(train_loader)"
      ],
      "metadata": {
        "id": "-Oi3Xmp_5Odc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14f4512c-04b0-4536-f65f-d95dd0b57b29",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0:\n",
            "image: tensor([[[[[ 0.6706,  0.6784,  0.6784,  ...,  0.6784,  0.6784,  0.6784],\n",
            "           [ 0.6784,  0.6784,  0.6784,  ...,  0.6784,  0.6784,  0.6784],\n",
            "           [ 0.6941,  0.6941,  0.6941,  ...,  0.6941,  0.6941,  0.6941],\n",
            "           ...,\n",
            "           [-0.5294, -0.5294, -0.5294,  ..., -0.5294, -0.5294, -0.5294],\n",
            "           [-0.5294, -0.5294, -0.5294,  ..., -0.5294, -0.5294, -0.5294],\n",
            "           [-0.5294, -0.5294, -0.5294,  ..., -0.5294, -0.5294, -0.5294]],\n",
            "\n",
            "          [[ 0.6706,  0.6784,  0.6784,  ...,  0.6784,  0.6784,  0.6784],\n",
            "           [ 0.6784,  0.6784,  0.6784,  ...,  0.6784,  0.6784,  0.6784],\n",
            "           [ 0.6941,  0.6941,  0.6941,  ...,  0.6941,  0.6941,  0.6941],\n",
            "           ...,\n",
            "           [-0.5294, -0.5294, -0.5294,  ..., -0.5294, -0.5294, -0.5294],\n",
            "           [-0.5294, -0.5294, -0.5294,  ..., -0.5294, -0.5294, -0.5294],\n",
            "           [-0.5294, -0.5294, -0.5294,  ..., -0.5294, -0.5294, -0.5294]],\n",
            "\n",
            "          [[ 0.6706,  0.6784,  0.6784,  ...,  0.6784,  0.6784,  0.6784],\n",
            "           [ 0.6784,  0.6784,  0.6784,  ...,  0.6784,  0.6784,  0.6784],\n",
            "           [ 0.6941,  0.6941,  0.6941,  ...,  0.6941,  0.6941,  0.6941],\n",
            "           ...,\n",
            "           [-0.5294, -0.5294, -0.5294,  ..., -0.5294, -0.5294, -0.5294],\n",
            "           [-0.5294, -0.5294, -0.5294,  ..., -0.5294, -0.5294, -0.5294],\n",
            "           [-0.5294, -0.5294, -0.5294,  ..., -0.5294, -0.5294, -0.5294]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[-0.4039, -0.4039, -0.4039,  ..., -0.3804, -0.3804, -0.3882],\n",
            "           [-0.3961, -0.3961, -0.3961,  ..., -0.3804, -0.3804, -0.3882],\n",
            "           [-0.3961, -0.3961, -0.3961,  ..., -0.3804, -0.3725, -0.3882],\n",
            "           ...,\n",
            "           [-0.3020, -0.3961, -0.3333,  ..., -0.0039,  0.0039, -0.0196],\n",
            "           [-0.1451, -0.1529, -0.1059,  ...,  0.0118,  0.0039, -0.0118],\n",
            "           [-0.0118,  0.0039, -0.0196,  ...,  0.0118,  0.0196,  0.0118]],\n",
            "\n",
            "          [[-0.4039, -0.4039, -0.4039,  ..., -0.3804, -0.3804, -0.3882],\n",
            "           [-0.3961, -0.3961, -0.3961,  ..., -0.3804, -0.3804, -0.3882],\n",
            "           [-0.3961, -0.3961, -0.3961,  ..., -0.3804, -0.3725, -0.3882],\n",
            "           ...,\n",
            "           [-0.3020, -0.3961, -0.3333,  ..., -0.0039,  0.0039, -0.0196],\n",
            "           [-0.1451, -0.1529, -0.1059,  ...,  0.0118,  0.0039, -0.0118],\n",
            "           [-0.0118,  0.0039, -0.0196,  ...,  0.0118,  0.0196,  0.0118]],\n",
            "\n",
            "          [[-0.4039, -0.4039, -0.4039,  ..., -0.3804, -0.3804, -0.3882],\n",
            "           [-0.3961, -0.3961, -0.3961,  ..., -0.3804, -0.3804, -0.3882],\n",
            "           [-0.3961, -0.3961, -0.3961,  ..., -0.3804, -0.3725, -0.3882],\n",
            "           ...,\n",
            "           [-0.3020, -0.3961, -0.3333,  ..., -0.0039,  0.0039, -0.0196],\n",
            "           [-0.1451, -0.1529, -0.1059,  ...,  0.0118,  0.0039, -0.0118],\n",
            "           [-0.0118,  0.0039, -0.0196,  ...,  0.0118,  0.0196,  0.0118]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 0.8902,  0.8902,  0.8902,  ...,  0.8980,  0.8902,  0.8980],\n",
            "           [ 0.8902,  0.8902,  0.8902,  ...,  0.8824,  0.8745,  0.8667],\n",
            "           [ 0.9059,  0.9059,  0.9059,  ...,  0.9059,  0.8980,  0.8824],\n",
            "           ...,\n",
            "           [ 0.9216,  0.9059,  0.8980,  ...,  0.8980,  0.8902,  0.8745],\n",
            "           [ 0.8980,  0.8902,  0.8824,  ...,  0.8902,  0.8902,  0.8902],\n",
            "           [ 0.9137,  0.9059,  0.8980,  ...,  0.8824,  0.8824,  0.8824]],\n",
            "\n",
            "          [[ 0.8902,  0.8902,  0.8902,  ...,  0.8980,  0.8902,  0.8980],\n",
            "           [ 0.8902,  0.8902,  0.8902,  ...,  0.8824,  0.8745,  0.8667],\n",
            "           [ 0.9059,  0.9059,  0.9059,  ...,  0.9059,  0.8980,  0.8824],\n",
            "           ...,\n",
            "           [ 0.9216,  0.9059,  0.8980,  ...,  0.8980,  0.8902,  0.8745],\n",
            "           [ 0.8980,  0.8902,  0.8824,  ...,  0.8902,  0.8902,  0.8902],\n",
            "           [ 0.9137,  0.9059,  0.8980,  ...,  0.8824,  0.8824,  0.8824]],\n",
            "\n",
            "          [[ 0.8902,  0.8902,  0.8902,  ...,  0.8980,  0.8902,  0.8980],\n",
            "           [ 0.8902,  0.8902,  0.8902,  ...,  0.8824,  0.8745,  0.8667],\n",
            "           [ 0.9059,  0.9059,  0.9059,  ...,  0.9059,  0.8980,  0.8824],\n",
            "           ...,\n",
            "           [ 0.9216,  0.9059,  0.8980,  ...,  0.8980,  0.8902,  0.8745],\n",
            "           [ 0.8980,  0.8902,  0.8824,  ...,  0.8902,  0.8902,  0.8902],\n",
            "           [ 0.9137,  0.9059,  0.8980,  ...,  0.8824,  0.8824,  0.8824]]]],\n",
            "\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 0.2863,  0.2863,  0.2863,  ...,  0.2784,  0.2784,  0.2784],\n",
            "           [ 0.2863,  0.2863,  0.2863,  ...,  0.2784,  0.2784,  0.2784],\n",
            "           [ 0.3804,  0.4118,  0.3804,  ...,  0.2784,  0.2784,  0.2784],\n",
            "           ...,\n",
            "           [-0.7725, -0.0510,  0.2235,  ...,  0.2000,  0.2000,  0.2549],\n",
            "           [-0.0510,  0.2078,  0.2235,  ...,  0.2000,  0.2078,  0.2549],\n",
            "           [ 0.2235,  0.2392,  0.2235,  ...,  0.2000,  0.2078,  0.2549]],\n",
            "\n",
            "          [[ 0.2863,  0.2863,  0.2863,  ...,  0.2784,  0.2784,  0.2784],\n",
            "           [ 0.2863,  0.2863,  0.2863,  ...,  0.2784,  0.2784,  0.2784],\n",
            "           [ 0.3804,  0.4118,  0.3804,  ...,  0.2784,  0.2784,  0.2784],\n",
            "           ...,\n",
            "           [-0.7725, -0.0510,  0.2235,  ...,  0.2000,  0.2000,  0.2549],\n",
            "           [-0.0510,  0.2078,  0.2235,  ...,  0.2000,  0.2078,  0.2549],\n",
            "           [ 0.2235,  0.2392,  0.2235,  ...,  0.2000,  0.2078,  0.2549]],\n",
            "\n",
            "          [[ 0.2863,  0.2863,  0.2863,  ...,  0.2784,  0.2784,  0.2784],\n",
            "           [ 0.2863,  0.2863,  0.2863,  ...,  0.2784,  0.2784,  0.2784],\n",
            "           [ 0.3804,  0.4118,  0.3804,  ...,  0.2784,  0.2784,  0.2784],\n",
            "           ...,\n",
            "           [-0.7725, -0.0510,  0.2235,  ...,  0.2000,  0.2000,  0.2549],\n",
            "           [-0.0510,  0.2078,  0.2235,  ...,  0.2000,  0.2078,  0.2549],\n",
            "           [ 0.2235,  0.2392,  0.2235,  ...,  0.2000,  0.2078,  0.2549]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[-0.3333, -0.3490, -0.3490,  ...,  0.6863,  0.6941,  0.7020],\n",
            "           [-0.3412, -0.3490, -0.3490,  ...,  0.6863,  0.6941,  0.6941],\n",
            "           [-0.3333, -0.3490, -0.3647,  ...,  0.6784,  0.6863,  0.6863],\n",
            "           ...,\n",
            "           [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "           [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "           [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n",
            "\n",
            "          [[-0.3333, -0.3490, -0.3490,  ...,  0.6863,  0.6941,  0.7020],\n",
            "           [-0.3412, -0.3490, -0.3490,  ...,  0.6863,  0.6941,  0.6941],\n",
            "           [-0.3333, -0.3490, -0.3647,  ...,  0.6784,  0.6863,  0.6863],\n",
            "           ...,\n",
            "           [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "           [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "           [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n",
            "\n",
            "          [[-0.3333, -0.3490, -0.3490,  ...,  0.6863,  0.6941,  0.7020],\n",
            "           [-0.3412, -0.3490, -0.3490,  ...,  0.6863,  0.6941,  0.6941],\n",
            "           [-0.3333, -0.3490, -0.3647,  ...,  0.6784,  0.6863,  0.6863],\n",
            "           ...,\n",
            "           [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "           [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "           [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[-0.2392, -0.2392, -0.2392,  ..., -0.2314, -0.2314, -0.2314],\n",
            "           [-0.2392, -0.2392, -0.2392,  ..., -0.2157, -0.2235, -0.2392],\n",
            "           [-0.2314, -0.2314, -0.2314,  ..., -0.1922, -0.2314, -0.2863],\n",
            "           ...,\n",
            "           [ 0.0510,  0.0431,  0.0353,  ...,  0.2784,  0.2784,  0.2784],\n",
            "           [ 0.0588,  0.0510,  0.0039,  ...,  0.2941,  0.2941,  0.2863],\n",
            "           [ 0.0275,  0.0118, -0.0275,  ...,  0.2549,  0.2549,  0.2471]],\n",
            "\n",
            "          [[-0.2392, -0.2392, -0.2392,  ..., -0.2314, -0.2314, -0.2314],\n",
            "           [-0.2392, -0.2392, -0.2392,  ..., -0.2157, -0.2235, -0.2392],\n",
            "           [-0.2314, -0.2314, -0.2314,  ..., -0.1922, -0.2314, -0.2863],\n",
            "           ...,\n",
            "           [ 0.0510,  0.0431,  0.0353,  ...,  0.2784,  0.2784,  0.2784],\n",
            "           [ 0.0588,  0.0510,  0.0039,  ...,  0.2941,  0.2941,  0.2863],\n",
            "           [ 0.0275,  0.0118, -0.0275,  ...,  0.2549,  0.2549,  0.2471]],\n",
            "\n",
            "          [[-0.2392, -0.2392, -0.2392,  ..., -0.2314, -0.2314, -0.2314],\n",
            "           [-0.2392, -0.2392, -0.2392,  ..., -0.2157, -0.2235, -0.2392],\n",
            "           [-0.2314, -0.2314, -0.2314,  ..., -0.1922, -0.2314, -0.2863],\n",
            "           ...,\n",
            "           [ 0.0510,  0.0431,  0.0353,  ...,  0.2784,  0.2784,  0.2784],\n",
            "           [ 0.0588,  0.0510,  0.0039,  ...,  0.2941,  0.2941,  0.2863],\n",
            "           [ 0.0275,  0.0118, -0.0275,  ...,  0.2549,  0.2549,  0.2471]]]]])\n",
            "caption: ['HƯỚNG DẪN CÁCH BẮT POKEMON RỒNG XANH - GYARADOS \\nNếu như không nuôi từ Magikarp rồi tiến hóa lên thì để tự nhiên xong bắt được con rồng xanh này khó vllll. Nhưng nếu như bạn có Nest Ball, Safari Ball hoặc bất kỳ quả Poke Ball nào màu lục giống màu áo Công An thì tỷ lệ trúng sẽ là 99%', 'VTV ĐẶC BIỆT: ĐIỆN BIÊN PHỦ - NHÌN TỪ NƯỚC PHÁP\\nKhai thác kho tư liệu đồ sộ về Chiến dịch Điện Biên Phủ  \\nChỉ có 4 tháng để nghiên cứu và thực hiện ghi hình tại Việt Nam và Pháp, VTV đặc biệt Điện Biên Phủ - Nhìn từ nước Pháp là hành trình gấp rút của nhóm sản xuất Ban Truyền hình đối ngoại, Đài truyền hình Việt Nam. Ê-kíp đã khai thác khối tư liệu đồ sộ về chiến dịch Điện Biên Phủ tại Trung tâm Lưu trữ của Bộ Quốc phòng Pháp, Quốc hội Pháp để tiếp cận nhiều tài liệu mới được giải mật, nhiều thông tin chưa hoặc ít được nhắc tới về sự kiện lịch sử này. Có thể kể tới những văn bản tối mật từ phía Pháp ghi lại những thừa nhận và lý giải vì sao Pháp thất bại tại Điện Biên Phủ, hay câu chuyện tiếp diễn sau khi chiến dịch kết thúc với chiến thắng thuộc về Việt Nam…\\n(Mời quý vị click vào ảnh để đọc chi tiết)\\n------\\n\"Điện Biên Phủ - Nhìn từ nước Pháp\" là phim tài liệu VTV Đặc biệt được thực hiện bởi ê-kíp của Ban Truyền hình Đối ngoại, Đài Truyền hình Việt Nam, hướng đến sự kiện đặc biệt kỷ niệm 70 năm chiến thắng Điện Biên Phủ.\\nPhim sẽ được phát sóng lúc 20h10 ngày 7/5 trên VTV4 và 20h10 ngày 7/5 trên VTV1.\\nKính mời quý vị khán giả đón xem!\\n#70namDienBienPhu #VTV', 'Hôn một con ếch chưa chắc nó đã biến thành hoàng tử. Hãy nghĩ tới bản thân mình trước 😉', '-= TUYỂN TẬP NHỮNG CA KHÚC NÊN HÁT NẾU VÀO KARAOKE =-', 'Đoàn thể thao Việt Nam đã vượt chỉ tiêu 120 HCV và vững vàng ngôi đầu bảng tại SEA Games 32.\\nHôm nay là ngày thi đấu cuối cùng tại SEA Games 32', 'Kỷ niệm 70 năm Chiến thắng Điện Biên Phủ, Đài Truyền hình Việt Nam thực hiện chùm chương trình trọng điểm, trong đó có cầu truyền hình Dưới lá cờ Quyết Thắng. Chương trình có 5 điểm cầu - Điện Biên, Hà Nội, Thanh Hóa, Kon Tum và TP Hồ Chí Minh - sẽ được truyền hình trực tiếp lúc 20h00 ngày 05/5/2024 trên kênh VTV1.\\nVới thời lượng hơn 110 phút, cầu truyền hình Dưới lá cờ Quyết Thắng sẽ đưa khán giả quay trở lại năm tháng hào hùng với những ký ức không thể nào quên của một thời bom đạn. Những dấu mốc quan trọng của 70 năm trước sẽ được thể hiện thông qua sự kết hợp giữa những trải nghiệm hiện tại và hồi tưởng quá khứ, giữa nghệ thuật và phân tích, đánh giá.\\n#70namDienBienPhu #VTV', 'FANTASTIC, WONDERFUL, SIGNIFICANT, MAGNIFICENT, OUTSTANDING, CLASS OF TITANS!!! “DUNE: HÀNH TINH CÁT - PHẦN HAI” CHÍNH LÀ WORLD CLASS!!  ✨\\nNếu như phần 1 Dune được đánh giá là phim khoa học viễn tưởng xuất sắc khi “ẵm” hẳn 6 giải Oscar thì Dune: Part Two cũng tiếp tục làm cho người xem phải đê mê như “đi cảnh” ngoài vũ trụ vì những khung hình tưởng chừng như chỉ thể tự mường tượng ra bằng con chữ.\\nĐể nói riêng về phần kỹ xảo thôi đã quá hoành tráng và chân thực đến từng chi tiết như những đoạn cưỡi sâu đất khổng lồ, thánh chiến toàn vũ trụ và những nghi lễ mới đúng làm người xem nổi hết da gà da vịt. Góc quay cũng đậm chất nghệ thuật đến nỗi cắt đại vài cảnh trong phim ra cài wallpapers vẫn đẹp như thường. Cộng thêm phần âm thanh do phù thuỷ âm nhạc Hans Zimmer thổi phép vào thì đúng là xem mà phê đến từng giác quan!\\nVề nội dung thì phần hai này chính là thứ mà khán giả đã chờ đợi suốt hơn 2 năm nay để xem Paul đẹp zai bung skill cực bá nhằm báo thù cho cả tộc ra sao, cuộc thánh chiến sẽ khốc liệt như thế nào, các mối quan hệ mẹ - con, vợ - chồng, quân - thần của nam chính sẽ phát triển theo chiều hướng gì? Thêm hai điều đặc biệt trong Dune: Part Two là sự xuất hiện của những nhân vật mới mang quyền lực bí ẩn không kém, tiêu biểu là công chúa Irula - tình địch mới của Chani và thế giới nội tâm phức tạp của Paul khi đã ở một “cương vị mới hoàn toàn” so với phần 1. Ngoài hấp dẫn ra thì nội dung phần 2 cũng “đẹp” không kém phần 1 vì toàn gương mặt xịn như Timothée Chalamet, Zendaya, Florence Pugh, Rebecca Ferguson,… \\nDune: Part Two được quay 100% bằng máy quay IMAX sẽ mang đến trải nghiệm đẳng cấp. Cỡ bom tấn đầu tư khủng như vậy mà xem trên màn hình nhỏ chắc chắn sẽ không cảm được hết độ kỳ vĩ của Hành Tinh Cát.\\nTầm này mà anh em tranh vé IMAX là cũng hơi bị nhọc rồi đấy, nên thật lòng tôi khuyến anh em nên tranh thủ ra rạp xem siêu phẩm điện ảnh này nhé, tiền đi làm mệt mỏi tiêu vào đây nó xứng lắm anh em ạ. \\n#DunePartTwo #DuneHanhTinhCat #DuneMovie', 'BÉ TRAI 9 TUỔI BỎ NHÀ ĐI VÌ BUỒN CHUYỆN GIA ĐÌNH…\\nMới đây, trong lúc thực hiện nhiệm vụ tuần tra kiểm soát trên đường Điện Biên Phủ (quận Bình Thạnh), Đội CSGT Tuần tra phát hiện 1 bé trai 9 tuổi đang đi bộ hướng từ vòng xoay Nguyễn Bỉnh Khiêm về vòng xoay Hàng Xanh.\\nThời điểm này, cháu bé đi ngược chiều dưới lòng đường. Nhận thấy cháu bé dễ gặp nguy hiểm, tổ tuần tra đã dừng xe, đưa cháu vào lề đường.\\nQua thăm hỏi được biết cháu bé tên 😭.A (quận 3), do buồn chuyện gia đình nên cháu đã bỏ nhà, một mình đến tòa nhà Landmark 81 để chơi.\\nSau khi nắm được sự việc, 😭.A được đưa về Công an phường 15, quận Bình Thạnh để phối hợp liên lạc với gia đình. Khoảng 45 phút sau, ba mẹ bé trai mang theo các giấy tờ cần thiết đến trụ sở công an để đón cháu về nhà.\\nQua đây, mong các bậc cha mẹ nên quan tâm đến con cái và cẩn thận hơn khi ứng xử với nhau trước mặt con. Cũng may là cháu đã an toàn về với gia đình.', 'Phải sau 8 năm thì chúng ta mới được ra rạp với tâm thế đi xem The Hunger Games.\\nRất nhiều người mông lung về THE BALLAD OF SONGBIRDS AND SNAKES bởi Lucy Gray được thủ vai bởi Rachel Zegler nhưng khi gạt bỏ sự racket sang một bên thì bạn sẽ thấy Rechel chính là chim ca đích thực khi ngoài diễn xuất và giọng ca của cô cũng tuyệt vời hơn những gì ta vốn nghĩ rất nhiều.\\nCòn với Tổng thống Snow của 64 năm về trước thì khi xem, chúng ta sẽ hiểu vì sao tựa đề phim ngoài Chim Ca thì còn có Rắn bởi hành trình phát triển để trờ thành phản diện của gã thật sự không thể rời mắt dù chỉ nửa giây.\\nTất nhiên so với truyện gốc thì phim sẽ có phần nào đó hơi nhanh hơn một chút, khiến bạn cảm thấy hơi vội vàng nhưng về tổng quan chung thì vẫn rất đáng xem và minh chứng là hiện tại Khúc Hát Của Chim Và Rắn đang đạt 91% tươi đến từ người dùng trên Rottentomatoes!\\nĐừng chần chừ, hay lăn tăn gì cả. Hãy ra các cụm rạp trên khắp cả nước để có được trải nghiệm điện ảnh thật sự tuyệt vời với The Hunger Games!!!', 'TUẤN HƯNG SUY SỤP, PHẢI NHẬP VIỆN SAU KHI BỊ HỦY LIVESHOW NGỰA HOANG \\nLiveshow \"Ngựa hoang\" kỷ niệm 20 năm ca hát của ca sĩ Tuấn Hưng dự kiến được tổ chức tại Cung thể thao tổng hợp Quần Ngựa. Tuy nhiên, trước khi biểu diễn đúng 2 tiếng thì UBND Quận Ba Đình đã có công văn tạm dừng biểu diễn. Trong văn bản báo cáo của Công an quận Ba Đình, lý do dừng là không đảm bảo được an ninh trật tự và phòng cháy chữa cháy.\\nNgay khi nhận tin, nam ca sĩ Tìm Lại Bầu Trời đã vô cùng suy sụp đến mức phải nhập viện và tại đây, ngoài việc điều trị bằng các phương pháp chuyên môn thì các bác sĩ còn áp dụng cả liệu pháp tinh thần khi bật rất nhiều ca khúc nhẹ nhàng, nhạc Hoa - lời Đức để động viên anh.\\nĐược biết đây là phác đồ điều trị Á Âu hỗn hợp, sức khỏe của Ca sĩ Tuấn Hưng đã được cải thiện rõ rệt và sau khi xuất viện, ngoài ca hát, rất có thể từ đây trong anh sẽ trỗi dậy niềm đam mê với quốc gia thiên triều ❤ ❤', 'Part 3: Tôi xin phép được nói Tiếng Việt vì nói tiếng Anh sợ người Mỹ họ không hiểu. \\n-The Deep\\n#Interpool', 'Ủa thiệt hả? 🤨😁😁😁\\n#phetphaikhong', 'Tóm tắt 3 năm trong 1 bức hình. Hy vọng vào 2023 ✈️ \\nCre on pic.', 'Nhiều khi ta muốn ta được thiếu nợ\\nĐể khi đi trốn có người đi tìmm', 'Tác giả \"Túy Âm\" Xesi vừa tung track dizz sau drama nợ tiền với tác giả Kellie \"Em ngồi đây và em khok huhu\". :)))', 'Từng kinh qua 99 81 kiếp nạn để sang Tây Thiên thỉnh kinh\\nTừng là Tổng lãnh Thiên Thần chỉ đứng sau Chúa Giê Su\\nSinh ra ở Phần Lan, sở hữu 8 con tuần lộc\\nRồi lại đi tu để luân hồi vòng lặp trên.\\nNhân tình thế thái, chẳng mấy ai bằng Minh']\n",
            "label: tensor([[0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "Batch 1:\n",
            "image: tensor([[[[[ 0.7804,  0.7804,  0.7804,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           [ 0.7804,  0.7804,  0.7804,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           [ 0.7804,  0.7804,  0.7804,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           ...,\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.8510,  0.8510,  0.8510],\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.8510,  0.8510,  0.8510],\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.8510,  0.8510,  0.8510]],\n",
            "\n",
            "          [[ 0.7804,  0.7804,  0.7804,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           [ 0.7804,  0.7804,  0.7804,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           [ 0.7804,  0.7804,  0.7804,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           ...,\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.8510,  0.8510,  0.8510],\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.8510,  0.8510,  0.8510],\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.8510,  0.8510,  0.8510]],\n",
            "\n",
            "          [[ 0.7804,  0.7804,  0.7804,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           [ 0.7804,  0.7804,  0.7804,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           [ 0.7804,  0.7804,  0.7804,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           ...,\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.8510,  0.8510,  0.8510],\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.8510,  0.8510,  0.8510],\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.8510,  0.8510,  0.8510]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [ 0.3961,  0.3961,  0.3882,  ...,  0.5765,  0.6392,  0.6706],\n",
            "           [ 0.3961,  0.3961,  0.3804,  ...,  0.6078,  0.6627,  0.6941],\n",
            "           [ 0.4039,  0.4039,  0.3804,  ...,  0.6157,  0.6706,  0.7020]],\n",
            "\n",
            "          [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [ 0.3961,  0.3961,  0.3882,  ...,  0.5765,  0.6392,  0.6706],\n",
            "           [ 0.3961,  0.3961,  0.3804,  ...,  0.6078,  0.6627,  0.6941],\n",
            "           [ 0.4039,  0.4039,  0.3804,  ...,  0.6157,  0.6706,  0.7020]],\n",
            "\n",
            "          [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [ 0.3961,  0.3961,  0.3882,  ...,  0.5765,  0.6392,  0.6706],\n",
            "           [ 0.3961,  0.3961,  0.3804,  ...,  0.6078,  0.6627,  0.6941],\n",
            "           [ 0.4039,  0.4039,  0.3804,  ...,  0.6157,  0.6706,  0.7020]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 0.4588,  0.4588,  0.4588,  ..., -0.0431, -0.1765, -0.4118],\n",
            "           [ 0.4196,  0.4039,  0.3961,  ..., -0.3725, -0.1686, -0.1843],\n",
            "           [ 0.2706,  0.2549,  0.2392,  ..., -0.2549, -0.2157, -0.3255],\n",
            "           ...,\n",
            "           [ 0.0980,  0.0667,  0.0588,  ..., -0.2627, -0.2627, -0.2627],\n",
            "           [ 0.0980,  0.0667,  0.0510,  ..., -0.2549, -0.2627, -0.2627],\n",
            "           [ 0.0902,  0.0588,  0.0510,  ..., -0.2706, -0.2549, -0.2706]],\n",
            "\n",
            "          [[ 0.4588,  0.4588,  0.4588,  ..., -0.0431, -0.1765, -0.4118],\n",
            "           [ 0.4196,  0.4039,  0.3961,  ..., -0.3725, -0.1686, -0.1843],\n",
            "           [ 0.2706,  0.2549,  0.2392,  ..., -0.2549, -0.2157, -0.3255],\n",
            "           ...,\n",
            "           [ 0.0980,  0.0667,  0.0588,  ..., -0.2627, -0.2627, -0.2627],\n",
            "           [ 0.0980,  0.0667,  0.0510,  ..., -0.2549, -0.2627, -0.2627],\n",
            "           [ 0.0902,  0.0588,  0.0510,  ..., -0.2706, -0.2549, -0.2706]],\n",
            "\n",
            "          [[ 0.4588,  0.4588,  0.4588,  ..., -0.0431, -0.1765, -0.4118],\n",
            "           [ 0.4196,  0.4039,  0.3961,  ..., -0.3725, -0.1686, -0.1843],\n",
            "           [ 0.2706,  0.2549,  0.2392,  ..., -0.2549, -0.2157, -0.3255],\n",
            "           ...,\n",
            "           [ 0.0980,  0.0667,  0.0588,  ..., -0.2627, -0.2627, -0.2627],\n",
            "           [ 0.0980,  0.0667,  0.0510,  ..., -0.2549, -0.2627, -0.2627],\n",
            "           [ 0.0902,  0.0588,  0.0510,  ..., -0.2706, -0.2549, -0.2706]]]],\n",
            "\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 0.9922,  0.9922,  0.9922,  ...,  0.9686,  0.9843,  0.9843],\n",
            "           [ 0.9843,  0.9843,  0.9765,  ..., -0.0118,  0.3804,  0.9294],\n",
            "           [ 0.7961,  0.6157,  0.4667,  ..., -0.9216, -0.5216,  0.8039],\n",
            "           ...,\n",
            "           [-0.7098, -0.3255,  0.3176,  ...,  0.7569,  0.4902, -0.7490],\n",
            "           [-0.9216, -0.9529, -0.9294,  ...,  0.0902,  0.0902, -0.7804],\n",
            "           [-0.9922, -1.0000, -0.9765,  ..., -0.8824, -0.8196, -0.8353]],\n",
            "\n",
            "          [[ 0.9922,  0.9922,  0.9922,  ...,  0.9686,  0.9843,  0.9843],\n",
            "           [ 0.9843,  0.9843,  0.9765,  ..., -0.0118,  0.3804,  0.9294],\n",
            "           [ 0.7961,  0.6157,  0.4667,  ..., -0.9216, -0.5216,  0.8039],\n",
            "           ...,\n",
            "           [-0.7098, -0.3255,  0.3176,  ...,  0.7569,  0.4902, -0.7490],\n",
            "           [-0.9216, -0.9529, -0.9294,  ...,  0.0902,  0.0902, -0.7804],\n",
            "           [-0.9922, -1.0000, -0.9765,  ..., -0.8824, -0.8196, -0.8353]],\n",
            "\n",
            "          [[ 0.9922,  0.9922,  0.9922,  ...,  0.9686,  0.9843,  0.9843],\n",
            "           [ 0.9843,  0.9843,  0.9765,  ..., -0.0118,  0.3804,  0.9294],\n",
            "           [ 0.7961,  0.6157,  0.4667,  ..., -0.9216, -0.5216,  0.8039],\n",
            "           ...,\n",
            "           [-0.7098, -0.3255,  0.3176,  ...,  0.7569,  0.4902, -0.7490],\n",
            "           [-0.9216, -0.9529, -0.9294,  ...,  0.0902,  0.0902, -0.7804],\n",
            "           [-0.9922, -1.0000, -0.9765,  ..., -0.8824, -0.8196, -0.8353]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[-0.9216, -0.9216, -0.9216,  ..., -0.9451, -0.9451, -0.9451],\n",
            "           [-0.9216, -0.9216, -0.9216,  ..., -0.9451, -0.9451, -0.9451],\n",
            "           [-0.9216, -0.9216, -0.9137,  ..., -0.9451, -0.9529, -0.9529],\n",
            "           ...,\n",
            "           [-0.9451, -0.9451, -0.9451,  ..., -0.9922, -0.9922, -0.9922],\n",
            "           [-0.9451, -0.9529, -0.9529,  ..., -0.9922, -0.9922, -0.9922],\n",
            "           [-0.9451, -0.9529, -0.9529,  ..., -0.9922, -0.9922, -0.9922]],\n",
            "\n",
            "          [[-0.9216, -0.9216, -0.9216,  ..., -0.9451, -0.9451, -0.9451],\n",
            "           [-0.9216, -0.9216, -0.9216,  ..., -0.9451, -0.9451, -0.9451],\n",
            "           [-0.9216, -0.9216, -0.9137,  ..., -0.9451, -0.9529, -0.9529],\n",
            "           ...,\n",
            "           [-0.9451, -0.9451, -0.9451,  ..., -0.9922, -0.9922, -0.9922],\n",
            "           [-0.9451, -0.9529, -0.9529,  ..., -0.9922, -0.9922, -0.9922],\n",
            "           [-0.9451, -0.9529, -0.9529,  ..., -0.9922, -0.9922, -0.9922]],\n",
            "\n",
            "          [[-0.9216, -0.9216, -0.9216,  ..., -0.9451, -0.9451, -0.9451],\n",
            "           [-0.9216, -0.9216, -0.9216,  ..., -0.9451, -0.9451, -0.9451],\n",
            "           [-0.9216, -0.9216, -0.9137,  ..., -0.9451, -0.9529, -0.9529],\n",
            "           ...,\n",
            "           [-0.9451, -0.9451, -0.9451,  ..., -0.9922, -0.9922, -0.9922],\n",
            "           [-0.9451, -0.9529, -0.9529,  ..., -0.9922, -0.9922, -0.9922],\n",
            "           [-0.9451, -0.9529, -0.9529,  ..., -0.9922, -0.9922, -0.9922]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
            "\n",
            "          [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
            "\n",
            "          [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]]]]])\n",
            "caption: [\"#VOZ_F33 - Phim Tết 'Sáng đèn' rút khỏi rạp\\nLink ở comment\", 'Phóng viên chụp dìm quá nha', 'Yêu', \".: 2000s girl you shouldn't mess with starter pack :.\\nGói trang bị hoàng kim của mọi cô gái thập niên 2000s\", 'mua to gio’ lon khong bang bao~ o trong long 💔⛈️', 'Vjp', 'Thôi thì tìm chú rể khác vậy', '👍🏻 Ác', 'Life hack ✨', 'đừng lo bbi, anh sẽ bảo vệ em :))\\n#phetphaikhong', 'Ngày xưa trốn ngủ trưa đuổi bắt, nghịch dại bị mẹ tóm được quất cho bao lần đến giờ vẫn nhớ. Tưởng rằng những thứ ấy chỉ còn trong kí ức vì các cháu nhỏ bây giờ có điện thoại, máy tính nên hết hào hứng với hoạt động vui chơi ngoài trời. \\nKhông ngờ mới đây ngay giữa thành phố biển Nha Trang lại được chứng kiến một cuộc thi chạy Marathon với sự góp mặt của 2000 Runner nhí độ tuổi từ 6 đến 10. \\nCác em còn được giao lưu, tỉ thí sức bền cùng Mr Cần Trô aka chú Xuân Nghị :X nên dù kết thúc đường chạy mọi người đều Thái Bình mồ hôi rơi nhưng ai cũng thở hổn hển :&lt; đã mệt rồi mà vẫn phải cười ạ, vui ghê.\\nVòng quay cuộc sống cơm áo gạo tiền lắm khi cuốn chúng ta đi mất, quên đi những kí ức đẹp thời thơ ấu. Để đến thế hệ con cháu mình thì chỉ có ăn rồi lại học. Tiền bạc, sự nỗ lực trong cuộc sống cũng quan trọng nhưng xin nhớ rằng tuổi thơ cũng trôi qua rất nhanh. Cho con ra ngoài khám phá thế giới sẽ tạo dựng những phút giây không thể nào quên được.', 'No shjt Sherlock\\n#interpool', 'may mà gặp được tôi', 'Đúng là con trai của ta.', 'Khá may mắn khi tôi được lớn lên ở cái thời mà Nhạc Rock vẫn rất thịnh, không chỉ ở Việt Nam mà trên khắp thế giới.\\nNgày ấy, tối muộn cứ bật VTV3 lên là kiểu gì cũng được MC Diễm Quỳnh cùng MC Anh Tuấn giới thiệu 1 list nhạc nước ngoài kiểu như It\\'s My Life, In The End, Chop Suey hay Californication còn nếu như bố mẹ bắt ngủ sớm, chỉ được xem tầm 8h 8 rưỡi thì thời điểm ấy là khung giờ vàng của nhóm nhạc Bức Tường!\\nHầu như bất kỳ chương trình nào cũng của sự góp mặt hoặc góp giọng của Trần Lập cùng các thành viên. Từ SV96, SV2000, Chuyên mục Văn hóa, Khoảnh khắc giao thời, Người Đương Thời cho đến Đường lên đỉnh Olympia hay mãi tận Rung Chuông Vàng của sau này, đâu đâu cũng là Bức Tường và từ ấy dần mặc định trong tôi 1 điều rằng Bức Tường là tượng đài của Rock Việt còn Trần Lập là người thủ lĩnh của tượng đài ấy.\\nNhiều năm trôi qua và cũng có thể gọi là đã từng nếm trải với không ít thăng trầm nhưng với tôi thì những ở điều trên vẫn luôn là bất biến, luôn là chân lý, luôn là \"đức tin\".\\nHôm nay là sinh nhật của người sở hữu đôi bàn tay thắp lửa.\\nXin được chúc mừng sinh nhật cố nghệ sĩ Trần Lập - thủ lĩnh nhóm nhạc Bức Tường!', 'Ở đây còn ai dễ thương hông?']\n",
            "label: tensor([[1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.]])\n",
            "Batch 2:\n",
            "image: tensor([[[[[-0.8039, -0.8039, -0.8039,  ..., -0.8745, -0.8745, -0.8745],\n",
            "           [-0.8118, -0.8118, -0.8118,  ..., -0.8745, -0.8745, -0.8745],\n",
            "           [-0.8196, -0.8196, -0.8196,  ..., -0.8745, -0.8745, -0.8745],\n",
            "           ...,\n",
            "           [ 0.7412,  0.7412,  0.7412,  ...,  0.7412,  0.7412,  0.7412],\n",
            "           [ 0.7412,  0.7412,  0.7412,  ...,  0.7412,  0.7412,  0.7412],\n",
            "           [ 0.7412,  0.7412,  0.7412,  ...,  0.7412,  0.7412,  0.7412]],\n",
            "\n",
            "          [[-0.8039, -0.8039, -0.8039,  ..., -0.8745, -0.8745, -0.8745],\n",
            "           [-0.8118, -0.8118, -0.8118,  ..., -0.8745, -0.8745, -0.8745],\n",
            "           [-0.8196, -0.8196, -0.8196,  ..., -0.8745, -0.8745, -0.8745],\n",
            "           ...,\n",
            "           [ 0.7412,  0.7412,  0.7412,  ...,  0.7412,  0.7412,  0.7412],\n",
            "           [ 0.7412,  0.7412,  0.7412,  ...,  0.7412,  0.7412,  0.7412],\n",
            "           [ 0.7412,  0.7412,  0.7412,  ...,  0.7412,  0.7412,  0.7412]],\n",
            "\n",
            "          [[-0.8039, -0.8039, -0.8039,  ..., -0.8745, -0.8745, -0.8745],\n",
            "           [-0.8118, -0.8118, -0.8118,  ..., -0.8745, -0.8745, -0.8745],\n",
            "           [-0.8196, -0.8196, -0.8196,  ..., -0.8745, -0.8745, -0.8745],\n",
            "           ...,\n",
            "           [ 0.7412,  0.7412,  0.7412,  ...,  0.7412,  0.7412,  0.7412],\n",
            "           [ 0.7412,  0.7412,  0.7412,  ...,  0.7412,  0.7412,  0.7412],\n",
            "           [ 0.7412,  0.7412,  0.7412,  ...,  0.7412,  0.7412,  0.7412]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[-0.5765, -0.5765, -0.5765,  ..., -0.1137, -0.1137, -0.1137],\n",
            "           [-0.5765, -0.5765, -0.5765,  ..., -0.1137, -0.1137, -0.1137],\n",
            "           [-0.5765, -0.5765, -0.5765,  ..., -0.1137, -0.1137, -0.1137],\n",
            "           ...,\n",
            "           [-0.7098, -0.7098, -0.7255,  ..., -0.7490, -0.7333, -0.7333],\n",
            "           [-0.7647, -0.7882, -0.7961,  ..., -0.7569, -0.7725, -0.7569],\n",
            "           [-0.8667, -0.8667, -0.8353,  ..., -0.7647, -0.7882, -0.7647]],\n",
            "\n",
            "          [[-0.5765, -0.5765, -0.5765,  ..., -0.1137, -0.1137, -0.1137],\n",
            "           [-0.5765, -0.5765, -0.5765,  ..., -0.1137, -0.1137, -0.1137],\n",
            "           [-0.5765, -0.5765, -0.5765,  ..., -0.1137, -0.1137, -0.1137],\n",
            "           ...,\n",
            "           [-0.7098, -0.7098, -0.7255,  ..., -0.7490, -0.7333, -0.7333],\n",
            "           [-0.7647, -0.7882, -0.7961,  ..., -0.7569, -0.7725, -0.7569],\n",
            "           [-0.8667, -0.8667, -0.8353,  ..., -0.7647, -0.7882, -0.7647]],\n",
            "\n",
            "          [[-0.5765, -0.5765, -0.5765,  ..., -0.1137, -0.1137, -0.1137],\n",
            "           [-0.5765, -0.5765, -0.5765,  ..., -0.1137, -0.1137, -0.1137],\n",
            "           [-0.5765, -0.5765, -0.5765,  ..., -0.1137, -0.1137, -0.1137],\n",
            "           ...,\n",
            "           [-0.7098, -0.7098, -0.7255,  ..., -0.7490, -0.7333, -0.7333],\n",
            "           [-0.7647, -0.7882, -0.7961,  ..., -0.7569, -0.7725, -0.7569],\n",
            "           [-0.8667, -0.8667, -0.8353,  ..., -0.7647, -0.7882, -0.7647]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [ 0.4275,  0.3490,  0.3725,  ...,  0.4431,  0.4902,  0.4824],\n",
            "           [ 0.4196,  0.3725,  0.4039,  ...,  0.4510,  0.4980,  0.4745],\n",
            "           [ 0.4118,  0.3725,  0.4118,  ...,  0.4431,  0.4902,  0.4824]],\n",
            "\n",
            "          [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [ 0.4275,  0.3490,  0.3725,  ...,  0.4431,  0.4902,  0.4824],\n",
            "           [ 0.4196,  0.3725,  0.4039,  ...,  0.4510,  0.4980,  0.4745],\n",
            "           [ 0.4118,  0.3725,  0.4118,  ...,  0.4431,  0.4902,  0.4824]],\n",
            "\n",
            "          [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [ 0.4275,  0.3490,  0.3725,  ...,  0.4431,  0.4902,  0.4824],\n",
            "           [ 0.4196,  0.3725,  0.4039,  ...,  0.4510,  0.4980,  0.4745],\n",
            "           [ 0.4118,  0.3725,  0.4118,  ...,  0.4431,  0.4902,  0.4824]]]],\n",
            "\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 0.3020,  0.0745,  0.3882,  ..., -0.1373, -0.1373, -0.1373],\n",
            "           [ 0.1843,  0.1216,  0.2078,  ..., -0.0980, -0.0980, -0.0980],\n",
            "           [ 0.1608,  0.1059, -0.0667,  ..., -0.0824, -0.0824, -0.0745],\n",
            "           ...,\n",
            "           [-0.3412, -0.7255, -0.8510,  ..., -0.8588, -0.7176, -0.3490],\n",
            "           [-0.3412, -0.6314, -0.7412,  ..., -0.7490, -0.6314, -0.3412],\n",
            "           [-0.3333, -0.3490, -0.3647,  ..., -0.3647, -0.3490, -0.3412]],\n",
            "\n",
            "          [[ 0.3020,  0.0745,  0.3882,  ..., -0.1373, -0.1373, -0.1373],\n",
            "           [ 0.1843,  0.1216,  0.2078,  ..., -0.0980, -0.0980, -0.0980],\n",
            "           [ 0.1608,  0.1059, -0.0667,  ..., -0.0824, -0.0824, -0.0745],\n",
            "           ...,\n",
            "           [-0.3412, -0.7255, -0.8510,  ..., -0.8588, -0.7176, -0.3490],\n",
            "           [-0.3412, -0.6314, -0.7412,  ..., -0.7490, -0.6314, -0.3412],\n",
            "           [-0.3333, -0.3490, -0.3647,  ..., -0.3647, -0.3490, -0.3412]],\n",
            "\n",
            "          [[ 0.3020,  0.0745,  0.3882,  ..., -0.1373, -0.1373, -0.1373],\n",
            "           [ 0.1843,  0.1216,  0.2078,  ..., -0.0980, -0.0980, -0.0980],\n",
            "           [ 0.1608,  0.1059, -0.0667,  ..., -0.0824, -0.0824, -0.0745],\n",
            "           ...,\n",
            "           [-0.3412, -0.7255, -0.8510,  ..., -0.8588, -0.7176, -0.3490],\n",
            "           [-0.3412, -0.6314, -0.7412,  ..., -0.7490, -0.6314, -0.3412],\n",
            "           [-0.3333, -0.3490, -0.3647,  ..., -0.3647, -0.3490, -0.3412]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 0.9608,  0.9608,  0.9373,  ...,  0.9922,  0.9843,  0.9843],\n",
            "           [ 0.9373,  0.7176,  0.5216,  ...,  0.9686,  0.9922,  0.9843],\n",
            "           [ 0.8275,  0.1137, -0.5529,  ...,  0.9765,  0.9765,  0.9922],\n",
            "           ...,\n",
            "           [ 0.9922,  0.9843,  0.9922,  ...,  0.9843,  0.9922,  0.9922],\n",
            "           [ 1.0000,  0.9765,  0.9922,  ...,  0.9922,  0.9922,  0.9922],\n",
            "           [ 1.0000,  0.9922,  1.0000,  ...,  1.0000,  0.9922,  1.0000]],\n",
            "\n",
            "          [[ 0.9608,  0.9608,  0.9373,  ...,  0.9922,  0.9843,  0.9843],\n",
            "           [ 0.9373,  0.7176,  0.5216,  ...,  0.9686,  0.9922,  0.9843],\n",
            "           [ 0.8275,  0.1137, -0.5529,  ...,  0.9765,  0.9765,  0.9922],\n",
            "           ...,\n",
            "           [ 0.9922,  0.9843,  0.9922,  ...,  0.9843,  0.9922,  0.9922],\n",
            "           [ 1.0000,  0.9765,  0.9922,  ...,  0.9922,  0.9922,  0.9922],\n",
            "           [ 1.0000,  0.9922,  1.0000,  ...,  1.0000,  0.9922,  1.0000]],\n",
            "\n",
            "          [[ 0.9608,  0.9608,  0.9373,  ...,  0.9922,  0.9843,  0.9843],\n",
            "           [ 0.9373,  0.7176,  0.5216,  ...,  0.9686,  0.9922,  0.9843],\n",
            "           [ 0.8275,  0.1137, -0.5529,  ...,  0.9765,  0.9765,  0.9922],\n",
            "           ...,\n",
            "           [ 0.9922,  0.9843,  0.9922,  ...,  0.9843,  0.9922,  0.9922],\n",
            "           [ 1.0000,  0.9765,  0.9922,  ...,  0.9922,  0.9922,  0.9922],\n",
            "           [ 1.0000,  0.9922,  1.0000,  ...,  1.0000,  0.9922,  1.0000]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[-0.6627, -0.6549, -0.6627,  ...,  0.0431,  0.0431,  0.0431],\n",
            "           [-0.6549, -0.6549, -0.6627,  ...,  0.0431,  0.0431,  0.0431],\n",
            "           [-0.6706, -0.6706, -0.6627,  ...,  0.0431,  0.0431,  0.0431],\n",
            "           ...,\n",
            "           [-0.2392, -0.2392, -0.2235,  ...,  0.3098,  0.2941,  0.3020],\n",
            "           [-0.2784, -0.2706, -0.2392,  ...,  0.3176,  0.3412,  0.3569],\n",
            "           [-0.2314, -0.1922, -0.1451,  ...,  0.3412,  0.3882,  0.4196]],\n",
            "\n",
            "          [[-0.6627, -0.6549, -0.6627,  ...,  0.0431,  0.0431,  0.0431],\n",
            "           [-0.6549, -0.6549, -0.6627,  ...,  0.0431,  0.0431,  0.0431],\n",
            "           [-0.6706, -0.6706, -0.6627,  ...,  0.0431,  0.0431,  0.0431],\n",
            "           ...,\n",
            "           [-0.2392, -0.2392, -0.2235,  ...,  0.3098,  0.2941,  0.3020],\n",
            "           [-0.2784, -0.2706, -0.2392,  ...,  0.3176,  0.3412,  0.3569],\n",
            "           [-0.2314, -0.1922, -0.1451,  ...,  0.3412,  0.3882,  0.4196]],\n",
            "\n",
            "          [[-0.6627, -0.6549, -0.6627,  ...,  0.0431,  0.0431,  0.0431],\n",
            "           [-0.6549, -0.6549, -0.6627,  ...,  0.0431,  0.0431,  0.0431],\n",
            "           [-0.6706, -0.6706, -0.6627,  ...,  0.0431,  0.0431,  0.0431],\n",
            "           ...,\n",
            "           [-0.2392, -0.2392, -0.2235,  ...,  0.3098,  0.2941,  0.3020],\n",
            "           [-0.2784, -0.2706, -0.2392,  ...,  0.3176,  0.3412,  0.3569],\n",
            "           [-0.2314, -0.1922, -0.1451,  ...,  0.3412,  0.3882,  0.4196]]]]])\n",
            "caption: ['Ủy ban Nhân dân thành phố Đà Nẵng vừa có văn bản về việc sẽ không tổ chức phun lửa, phun nước cầu Rồng và không quay cầu sông Hàn trong các đêm trình diễn Lễ hội pháo hoa Quốc tế Đà Nẵng (DIFF) 2023 để nhằm đảm bảo an toàn, hạn chế ùn tắc giao thông tại các cây cầu qua sông và khu vực trung tâm thành phố.\\nCụ thể, không phun nước, phun lửa cầu Rồng trong các đêm 2/6, 10/6, 17/6, 24/6 và 8/7/2023. Không quay cầu sông Hàn trong các đêm 10/6, 17/6, 24/6 và 8/7/2023.', 'Trai tài, gái xỉu :v\\nĐúng là thuận vợ thuận chồng thì tát biển đông cũng cạn \\U0001f979', 'Phi công này 1 người lái thôi, ai đụng vào là chớt với chuỵ', 'KHÔNG AI BỊ BỎ QUÊN TRONG MÙA VU LAN\\nNằm trong chiến dịch \"Ecopark cùng bạn nói lời cảm ơn\", mang theo hàng nghìn bông sen rực rỡ, tập đoàn Ecopark ghé thăm những viện dưỡng lão ở Hà Nội nhân dịp mùa Vu Lan đang đến gần. \\nNghe tin có đoàn tình nguyên viên đến thăm và tặng hoa, bà Nguyễn Thị Xuân bỗng vui và háo hức hơn hẳn. Nhắc đến lễ Vu Lan, bà lại nhớ nhà, nhớ con cháu. \\nBà Xuân kể, các con đều thành đạt, định cư ở nước ngoài nên bà chỉ sống một mình trong căn nhà ở Gia Lâm. Từ ngày sức khỏe yếu, con cái không thể ở bên chăm sóc nên tài trợ cho bà nghỉ dưỡng tại viện dưỡng lão.\\nKhông chỉ bà Xuân, nhiều người khác đều không giấu được niềm vui, sự xúc động khi nhận trên tay bó hoa sen đặc biệt mùa lễ Vu Lan. Buổi sáng hôm nay là một buổi sáng đặc biệt hơn mọi ngày. Ông bà bảo rằng \"Gần 70 năm vất vả, đây là lần đầu tiên tôi nhận được bó hoa đẹp như thế này\". \\nTháng 7 mùa Vu Lan về là dịp để chúng ta nhớ đến bố mẹ, gia đình, người thân với tình yêu thương nhất. Những bông hoa này cũng thay cho lời Tập đoàn Ecopark muốn gửi gắm đến tất cả mọi người. \\nRời viện dưỡng lão, bệnh viện, tập đoàn Ecopark tiếp tục dành tặng những đóa hoa sen cho những người cha, người mẹ vất vả, lam lũ trên phố với thông điệp \"sẽ không ai bị lãng quên\" trong mùa Vu Lan.\\nTháng 7 mùa Vu Lan về là dịp để chúng ta nhớ đến bố mẹ, gia đình, người thân với tình yêu thương nhất. Những bông hoa này cũng thay cho lời Tập đoàn Ecopark muốn gửi gắm đến tất cả mọi người. Hãy tặng bông hoa này thay cho lời cảm ơn gửi đến những người bạn yêu thương nhất.', 'Sống như anh, thật đáng sống\\n- Ryan Gosling\\n#interpool', 'Khác màu mà, chắc không phải đâu nhỉ', 'Nguyên văn chia sẻ của Trấn Thành:\\n✔️Tôi là khách hay đi xem phim nên tôi luôn liên lạc với các bạn manager ở các rạp để hỏi suất chiếu và đặt vé trước từ sớm. \\n✔️ Gần 11h tối, tôi đến rạp để lấy vé, rạp khá vắng. Tôi được một bạn nhân viên quầy vé mời vào quầy để trả tiền và nhận vé đã được đặt trước đó.\\n✔️ Trong lúc tôi trả tiền, có một người khách (có thể là bạn V) khều tôi bảo: \"Anh ơi! Anh mua hết vé rồi hả? Anh nhường em 2 vé đi\"! \\n- Tôi bảo: \"Xin lỗi, không được em, anh đã đặt riêng và mua hết vé phòng này rồi\"! \\n- Bạn vẫn khư khư: \"Anh nhường em 2 vé thôi. 2 vé thôi mà\"! \\n- Lúc đó tôi mới nói câu: \"Không được đâu em ơi! Anh cần sự riêng tư\"! \\n✔️ Bạn tỏ ra rất khó chịu với tôi. Trong khi đó, tôi cũng không thật sự thấy thoải mái lắm khi bất ngờ có một người không quen biết, chưa hỏi mình xem phim gì, chưa hỏi mình xem suất mấy giờ mà yêu cầu mình nhường 2 vé cho bằng được trong khi vé đó tôi đã đặt trước và chỉ chờ để thanh toán. \\n🔴 Trấn Thành mong quý vị khán giả hãy hiểu đúng bản chất của sự việc trên và lắng nghe câu chuyện từ 2 phía để có góc nhìn công tâm và khách quan hơn.', 'Đã 3 mùa rồi, Rap Việt có nên đổi MC mới?', '#VOZ_F33 - Ông Nguyễn Tử Quảng nên trả nợ lương cho nhân viên trước khi nói chuyện to tát\\nhttps://voz.vn/.../ong-nguyen-tu-quang-nen-tra-no-luong.../', 'Bộ ba Vicky Nhung, Thiên An, Lu Đỗ khoe visual sang xịn mịn trong sản phẩm âm nhạc sắp ra mắt \"Ai Cũng Có Một Mối Tình Buồn\". Được biết, đây là dự án được Vicky Nhung đầu tư kỹ lưỡng về hiệu ứng hình ảnh và áp dụng trí tuệ nhân tạo (AI) trong khâu sản xuất.\\nMV dự kiến ra mắt vào 19h30 ngày 28/05/2023', 'NHANH TAY THÌ CÒN CHẬM TAY THÌ TIẾC HÙI HỤI, CHIẾC #CHEAP_MOMENT DỄ ĐU CÙNG 2 SẾP CHỈ CÒN TRONG THÁNG NÀY! 😘\\nĐể thỏa mãn tinh thần còn thở là còn #cheap_moment với hai Sếp, để chiều lòng \"nhân viên\", Yamaha Motor Việt Nam tung ra chương trình khuyến mãi cực hấp dẫn. Theo đó, khi mua xe Yamaha Janus, \"nhân viên\" sẽ rinh liền tay điện thoại Samsung Galaxy A15 xịn sò trị giá 4.990.000đ! 🔥\\nĐánh bay cái nóng mùa hè với Yamaha Janus. 💥\\n- Thiết kế trendy, bắt trend cực nhanh - \"Sống ảo\" mọi lúc mọi nơi!\\n- Động cơ mạnh mẽ, tiết kiệm \"đáng gờm\" - Chinh phục mọi hành trình!\\n- Thể hiện đẳng cấp fan cứng của 2 Sếp \"hot hit\" - Rước quà khủng về nhà!\\n2 Sếp đã \"ẵm trọn\" quà xịn, bạn còn chần chừ gì nữa? \"Xông pha\" ngay thôi nào, hè 2024 là của bạn! 🎁\\n👉 Chương trình \"khuấy đảo\" hè 2024 chỉ áp dụng từ nay đến 30/06 - Nhanh tay \"xuống tay quăng lưới\" deal đỉnh này!\\n#Yamaha #YamahaVietnam #Janus #FreeGo #Promotion #CheapMoment', 'Tự nhiên hiểu\\n#phetphaikhong', '- Nay bị Urus tạt đầu... Ờ', 'Động lực đi tù ?', 'Anh hùng thật sự', 'MC Liêu Hà Trinh hạ sinh con trai đầu lòng - Luka (Đặng Liêu Nhật Khoa) ♥️']\n",
            "label: tensor([[1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.]])\n",
            "Batch 3:\n",
            "image: tensor([[[[[-0.8902, -0.9059, -0.8588,  ..., -0.7882, -0.8275, -0.8353],\n",
            "           [-0.9137, -0.9294, -0.8902,  ..., -0.8588, -0.8824, -0.9059],\n",
            "           [-0.9373, -0.9529, -0.9059,  ..., -0.9294, -0.9451, -0.9451],\n",
            "           ...,\n",
            "           [ 0.7255,  0.7255,  0.6235,  ...,  0.0902,  0.0824,  0.0824],\n",
            "           [ 0.7098,  0.3647, -0.2078,  ...,  0.0824,  0.0824,  0.0745],\n",
            "           [ 0.0980,  0.3333,  0.3490,  ...,  0.0902,  0.0824,  0.0667]],\n",
            "\n",
            "          [[-0.8902, -0.9059, -0.8588,  ..., -0.7882, -0.8275, -0.8353],\n",
            "           [-0.9137, -0.9294, -0.8902,  ..., -0.8588, -0.8824, -0.9059],\n",
            "           [-0.9373, -0.9529, -0.9059,  ..., -0.9294, -0.9451, -0.9451],\n",
            "           ...,\n",
            "           [ 0.7255,  0.7255,  0.6235,  ...,  0.0902,  0.0824,  0.0824],\n",
            "           [ 0.7098,  0.3647, -0.2078,  ...,  0.0824,  0.0824,  0.0745],\n",
            "           [ 0.0980,  0.3333,  0.3490,  ...,  0.0902,  0.0824,  0.0667]],\n",
            "\n",
            "          [[-0.8902, -0.9059, -0.8588,  ..., -0.7882, -0.8275, -0.8353],\n",
            "           [-0.9137, -0.9294, -0.8902,  ..., -0.8588, -0.8824, -0.9059],\n",
            "           [-0.9373, -0.9529, -0.9059,  ..., -0.9294, -0.9451, -0.9451],\n",
            "           ...,\n",
            "           [ 0.7255,  0.7255,  0.6235,  ...,  0.0902,  0.0824,  0.0824],\n",
            "           [ 0.7098,  0.3647, -0.2078,  ...,  0.0824,  0.0824,  0.0745],\n",
            "           [ 0.0980,  0.3333,  0.3490,  ...,  0.0902,  0.0824,  0.0667]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [ 0.0196,  0.0275,  0.0196,  ...,  0.0196, -0.0431, -0.1451],\n",
            "           [ 0.0118,  0.0196,  0.0118,  ..., -0.2078,  0.0118,  0.0196],\n",
            "           [ 0.0353,  0.0667,  0.0667,  ..., -0.4353, -0.2706, -0.0745]],\n",
            "\n",
            "          [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [ 0.0196,  0.0275,  0.0196,  ...,  0.0196, -0.0431, -0.1451],\n",
            "           [ 0.0118,  0.0196,  0.0118,  ..., -0.2078,  0.0118,  0.0196],\n",
            "           [ 0.0353,  0.0667,  0.0667,  ..., -0.4353, -0.2706, -0.0745]],\n",
            "\n",
            "          [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [ 0.0196,  0.0275,  0.0196,  ...,  0.0196, -0.0431, -0.1451],\n",
            "           [ 0.0118,  0.0196,  0.0118,  ..., -0.2078,  0.0118,  0.0196],\n",
            "           [ 0.0353,  0.0667,  0.0667,  ..., -0.4353, -0.2706, -0.0745]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 0.3333,  0.3255,  0.3176,  ..., -0.5765, -0.5373, -0.5765],\n",
            "           [ 0.3333,  0.3255,  0.3176,  ..., -0.5765, -0.5529, -0.5843],\n",
            "           [ 0.3333,  0.3255,  0.3176,  ..., -0.5843, -0.5686, -0.6000],\n",
            "           ...,\n",
            "           [-0.8667, -0.8353, -0.8118,  ..., -0.4980, -0.4667, -0.5373],\n",
            "           [-0.7804, -0.7176, -0.7255,  ..., -0.5216, -0.5529, -0.6314],\n",
            "           [-0.7412, -0.6784, -0.6941,  ..., -0.5451, -0.6000, -0.6706]],\n",
            "\n",
            "          [[ 0.3333,  0.3255,  0.3176,  ..., -0.5765, -0.5373, -0.5765],\n",
            "           [ 0.3333,  0.3255,  0.3176,  ..., -0.5765, -0.5529, -0.5843],\n",
            "           [ 0.3333,  0.3255,  0.3176,  ..., -0.5843, -0.5686, -0.6000],\n",
            "           ...,\n",
            "           [-0.8667, -0.8353, -0.8118,  ..., -0.4980, -0.4667, -0.5373],\n",
            "           [-0.7804, -0.7176, -0.7255,  ..., -0.5216, -0.5529, -0.6314],\n",
            "           [-0.7412, -0.6784, -0.6941,  ..., -0.5451, -0.6000, -0.6706]],\n",
            "\n",
            "          [[ 0.3333,  0.3255,  0.3176,  ..., -0.5765, -0.5373, -0.5765],\n",
            "           [ 0.3333,  0.3255,  0.3176,  ..., -0.5765, -0.5529, -0.5843],\n",
            "           [ 0.3333,  0.3255,  0.3176,  ..., -0.5843, -0.5686, -0.6000],\n",
            "           ...,\n",
            "           [-0.8667, -0.8353, -0.8118,  ..., -0.4980, -0.4667, -0.5373],\n",
            "           [-0.7804, -0.7176, -0.7255,  ..., -0.5216, -0.5529, -0.6314],\n",
            "           [-0.7412, -0.6784, -0.6941,  ..., -0.5451, -0.6000, -0.6706]]]],\n",
            "\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [-0.8588, -0.8588, -0.8588,  ..., -0.8275, -0.8118, -0.8196],\n",
            "           [-0.8588, -0.8588, -0.8588,  ..., -0.8275, -0.8196, -0.8196],\n",
            "           [-0.8588, -0.8588, -0.8588,  ..., -0.8275, -0.8353, -0.8431]],\n",
            "\n",
            "          [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [-0.8588, -0.8588, -0.8588,  ..., -0.8275, -0.8118, -0.8196],\n",
            "           [-0.8588, -0.8588, -0.8588,  ..., -0.8275, -0.8196, -0.8196],\n",
            "           [-0.8588, -0.8588, -0.8588,  ..., -0.8275, -0.8353, -0.8431]],\n",
            "\n",
            "          [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [-0.8588, -0.8588, -0.8588,  ..., -0.8275, -0.8118, -0.8196],\n",
            "           [-0.8588, -0.8588, -0.8588,  ..., -0.8275, -0.8196, -0.8196],\n",
            "           [-0.8588, -0.8588, -0.8588,  ..., -0.8275, -0.8353, -0.8431]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[-0.5843, -0.6000, -0.6000,  ..., -0.6392, -0.6627, -0.6314],\n",
            "           [-0.5529, -0.5608, -0.5451,  ..., -0.7020, -0.6863, -0.6627],\n",
            "           [-0.5529, -0.5765, -0.5765,  ..., -0.7569, -0.7333, -0.6392],\n",
            "           ...,\n",
            "           [ 0.7569,  0.7569,  0.7569,  ...,  0.7569,  0.7569,  0.7569],\n",
            "           [ 0.7569,  0.7569,  0.7569,  ...,  0.7569,  0.7569,  0.7569],\n",
            "           [ 0.7569,  0.7569,  0.7569,  ...,  0.7569,  0.7569,  0.7569]],\n",
            "\n",
            "          [[-0.5843, -0.6000, -0.6000,  ..., -0.6392, -0.6627, -0.6314],\n",
            "           [-0.5529, -0.5608, -0.5451,  ..., -0.7020, -0.6863, -0.6627],\n",
            "           [-0.5529, -0.5765, -0.5765,  ..., -0.7569, -0.7333, -0.6392],\n",
            "           ...,\n",
            "           [ 0.7569,  0.7569,  0.7569,  ...,  0.7569,  0.7569,  0.7569],\n",
            "           [ 0.7569,  0.7569,  0.7569,  ...,  0.7569,  0.7569,  0.7569],\n",
            "           [ 0.7569,  0.7569,  0.7569,  ...,  0.7569,  0.7569,  0.7569]],\n",
            "\n",
            "          [[-0.5843, -0.6000, -0.6000,  ..., -0.6392, -0.6627, -0.6314],\n",
            "           [-0.5529, -0.5608, -0.5451,  ..., -0.7020, -0.6863, -0.6627],\n",
            "           [-0.5529, -0.5765, -0.5765,  ..., -0.7569, -0.7333, -0.6392],\n",
            "           ...,\n",
            "           [ 0.7569,  0.7569,  0.7569,  ...,  0.7569,  0.7569,  0.7569],\n",
            "           [ 0.7569,  0.7569,  0.7569,  ...,  0.7569,  0.7569,  0.7569],\n",
            "           [ 0.7569,  0.7569,  0.7569,  ...,  0.7569,  0.7569,  0.7569]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [-0.5608, -0.5294, -0.5451,  ...,  0.7882,  0.7647,  0.8745],\n",
            "           [-0.4588, -0.4039, -0.3961,  ...,  0.8353,  0.7333,  0.8196],\n",
            "           [-0.2863, -0.3490, -0.3569,  ...,  0.7961,  0.7412,  0.7176]],\n",
            "\n",
            "          [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [-0.5608, -0.5294, -0.5451,  ...,  0.7882,  0.7647,  0.8745],\n",
            "           [-0.4588, -0.4039, -0.3961,  ...,  0.8353,  0.7333,  0.8196],\n",
            "           [-0.2863, -0.3490, -0.3569,  ...,  0.7961,  0.7412,  0.7176]],\n",
            "\n",
            "          [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [-0.5608, -0.5294, -0.5451,  ...,  0.7882,  0.7647,  0.8745],\n",
            "           [-0.4588, -0.4039, -0.3961,  ...,  0.8353,  0.7333,  0.8196],\n",
            "           [-0.2863, -0.3490, -0.3569,  ...,  0.7961,  0.7412,  0.7176]]]]])\n",
            "caption: ['Ai rồi cũng khác', 'Biển miền Trung nước đẹp nhỉ', 'Mụn những ngày thường:\\nMụn ngày bạn đi ăn cưới:', 'GIỤC NHỎ CƯỚI LIỀN ĐI 🤤\\n12 kí phô mai không phải là món quà rẻ, chưa kể đó có thể chính là hàng của nhà anh trai nông dân kia tự làm ra nữa 🤤 Nói chung đây là 1 món quà quá ý nghĩa luôn. \\nPhô mai được làm bằng phương pháp kết đông và lên men từ các loại sữa bò, cừu, dê hoặc một số động vật có vú khác. Phong cách, kết cấu và hương vị của chúng lại rất đa dạng, quốc gia nào cũng có những kiểu phô mai của riêng mình cả. 🧀\\n- Kiến Lang Thang -', 'Chả Ram 🤤', 'VĂN MƯỢN XE BÁN TẢI (Cập nhật 24 Tết; Ngày Quý Dậu, Tháng Quý Sửu, Tiết Tiểu hàn)\\n*Mở đầu: Khen ngầu, thiết kế hầm hố, vững chãi. Khen bền bỉ với xe Nhật, khen hợp đi uống cà phê đối với xe Mỹ!\\n**Thân bài: Đi sâu vào nội thất, tính năng, trang bị &amp; đặc biệt là khen công năng tuyệt vời khi sở hữu, khen động cơ dầu + hộp số + hệ dẫn động! Bày tỏ mong muốn mình sẽ sở hữu bán tải trong tương lai gần trước mặt chủ xe! \\n***Kết bài: “Cho mình mượn chở Mai/Đào/Quất… Nhớ hẹn giờ trả, có thể đổ đầy hoặc nửa bình tùy vào số KM” \\n****Kiêng kỵ đề cập đến đồ độ trên xe, chuyện đi đăng kiểm, cách chạy của chủ xe mà mài đã từng chứng kiến trước đó!\\nChúc bạn thành công!!!  \\nThầy Cụt,\\n#Meme #j4f\\n#Triton', 'Pink Venom', 'Xinh quá chứ', 'Mang ngay Manchester City đến đâyyyyyyyyyyyyyyyy', 'Quá đúng!', 'Winter (Aespa) và chiếc story của cô ấy khi đến Việt Nam :))))', 'Ủa người nào ta? 🤔', 'Không biết shipper với khách ai quá đáng hơn luôn á 😅', '4 sự thật thời đi học :))\\nCre: MV cô ấy của anh ấy', 'Chiều 12/5, Sở Giáo dục và Đào tạo TP Hồ Chí Minh công bố số nguyện vọng đăng ký xét tuyển lớp 10 công lập của thí sinh. Đông nhất là trường THPT Gia Định với 1.913 thí sinh đăng ký nguyện vọng 1, tiếp đó là THPT Mạc Đĩnh Chi với 1.741 lượt đăng ký. Các trường khác có số thí sinh đăng ký cao là THPT Phú Nhuận, Hùng Vương, Bùi Thị Xuân, Marie Curie.\\nSo với tổng chỉ tiêu toàn thành phố, số thí sinh không có suất vào lớp 10 công lập ở TP Hồ Chí Minh năm nay là 18.780. Như vậy, tỷ lệ thí sinh trượt lớp 10 công lập là 19,5%, giảm so với tỷ lệ 21,7% của năm ngoái.\\nNăm nay, TP Hồ Chí Minh có khoảng 110.000 học sinh tốt nghiệp THCS. Học sinh không đăng ký thi vào lớp 10 có thể học tiếp tại các trường THPT ngoài công lập, trung tâm giáo dục thường xuyên, trường nghề.', 'Lý Hải trẻ quá trời']\n",
            "label: tensor([[1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.]])\n",
            "Batch 4:\n",
            "image: tensor([[[[[-0.9059, -0.9059, -0.9059,  ..., -0.7882, -0.8039, -0.7882],\n",
            "           [-0.8667, -0.8667, -0.8667,  ..., -0.8039, -0.7882, -0.7725],\n",
            "           [-0.8980, -0.8980, -0.8980,  ..., -0.7020, -0.7098, -0.7176],\n",
            "           ...,\n",
            "           [-0.5216, -0.5137, -0.5137,  ..., -0.3490, -0.3490, -0.3490],\n",
            "           [-0.5294, -0.5216, -0.5137,  ..., -0.3569, -0.3725, -0.3804],\n",
            "           [-0.5451, -0.5216, -0.5137,  ..., -0.3725, -0.3725, -0.3804]],\n",
            "\n",
            "          [[-0.9059, -0.9059, -0.9059,  ..., -0.7882, -0.8039, -0.7882],\n",
            "           [-0.8667, -0.8667, -0.8667,  ..., -0.8039, -0.7882, -0.7725],\n",
            "           [-0.8980, -0.8980, -0.8980,  ..., -0.7020, -0.7098, -0.7176],\n",
            "           ...,\n",
            "           [-0.5216, -0.5137, -0.5137,  ..., -0.3490, -0.3490, -0.3490],\n",
            "           [-0.5294, -0.5216, -0.5137,  ..., -0.3569, -0.3725, -0.3804],\n",
            "           [-0.5451, -0.5216, -0.5137,  ..., -0.3725, -0.3725, -0.3804]],\n",
            "\n",
            "          [[-0.9059, -0.9059, -0.9059,  ..., -0.7882, -0.8039, -0.7882],\n",
            "           [-0.8667, -0.8667, -0.8667,  ..., -0.8039, -0.7882, -0.7725],\n",
            "           [-0.8980, -0.8980, -0.8980,  ..., -0.7020, -0.7098, -0.7176],\n",
            "           ...,\n",
            "           [-0.5216, -0.5137, -0.5137,  ..., -0.3490, -0.3490, -0.3490],\n",
            "           [-0.5294, -0.5216, -0.5137,  ..., -0.3569, -0.3725, -0.3804],\n",
            "           [-0.5451, -0.5216, -0.5137,  ..., -0.3725, -0.3725, -0.3804]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[-0.0667,  0.0510,  0.0824,  ..., -0.2784,  0.7176,  0.4902],\n",
            "           [-0.2784, -0.1765, -0.2471,  ..., -0.6627, -0.0745, -0.0275],\n",
            "           [ 0.0039,  0.1451, -0.0667,  ..., -0.8039, -0.8196, -0.8353],\n",
            "           ...,\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
            "\n",
            "          [[-0.0667,  0.0510,  0.0824,  ..., -0.2784,  0.7176,  0.4902],\n",
            "           [-0.2784, -0.1765, -0.2471,  ..., -0.6627, -0.0745, -0.0275],\n",
            "           [ 0.0039,  0.1451, -0.0667,  ..., -0.8039, -0.8196, -0.8353],\n",
            "           ...,\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
            "\n",
            "          [[-0.0667,  0.0510,  0.0824,  ..., -0.2784,  0.7176,  0.4902],\n",
            "           [-0.2784, -0.1765, -0.2471,  ..., -0.6627, -0.0745, -0.0275],\n",
            "           [ 0.0039,  0.1451, -0.0667,  ..., -0.8039, -0.8196, -0.8353],\n",
            "           ...,\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[-0.4745, -0.4902, -0.5608,  ..., -0.5059, -0.5137, -0.4902],\n",
            "           [-0.4745, -0.4824, -0.6078,  ..., -0.5765, -0.5529, -0.5608],\n",
            "           [-0.5137, -0.5451, -0.5373,  ..., -0.5608, -0.5922, -0.5922],\n",
            "           ...,\n",
            "           [-0.4980, -0.4431, -0.5529,  ..., -0.7490, -0.7020, -0.8353],\n",
            "           [-0.4824, -0.7412, -0.7569,  ..., -0.7020, -0.7569, -0.7961],\n",
            "           [-0.8275, -0.8196, -0.7176,  ..., -0.7725, -0.7333, -0.8118]],\n",
            "\n",
            "          [[-0.4745, -0.4902, -0.5608,  ..., -0.5059, -0.5137, -0.4902],\n",
            "           [-0.4745, -0.4824, -0.6078,  ..., -0.5765, -0.5529, -0.5608],\n",
            "           [-0.5137, -0.5451, -0.5373,  ..., -0.5608, -0.5922, -0.5922],\n",
            "           ...,\n",
            "           [-0.4980, -0.4431, -0.5529,  ..., -0.7490, -0.7020, -0.8353],\n",
            "           [-0.4824, -0.7412, -0.7569,  ..., -0.7020, -0.7569, -0.7961],\n",
            "           [-0.8275, -0.8196, -0.7176,  ..., -0.7725, -0.7333, -0.8118]],\n",
            "\n",
            "          [[-0.4745, -0.4902, -0.5608,  ..., -0.5059, -0.5137, -0.4902],\n",
            "           [-0.4745, -0.4824, -0.6078,  ..., -0.5765, -0.5529, -0.5608],\n",
            "           [-0.5137, -0.5451, -0.5373,  ..., -0.5608, -0.5922, -0.5922],\n",
            "           ...,\n",
            "           [-0.4980, -0.4431, -0.5529,  ..., -0.7490, -0.7020, -0.8353],\n",
            "           [-0.4824, -0.7412, -0.7569,  ..., -0.7020, -0.7569, -0.7961],\n",
            "           [-0.8275, -0.8196, -0.7176,  ..., -0.7725, -0.7333, -0.8118]]]],\n",
            "\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "\n",
            "        [[[[-0.4431, -0.4431, -0.4431,  ..., -0.4431, -0.4431, -0.4431],\n",
            "           [-0.4431, -0.4431, -0.4431,  ..., -0.4431, -0.4431, -0.4431],\n",
            "           [-0.4431, -0.4431, -0.4431,  ..., -0.4431, -0.4431, -0.4431],\n",
            "           ...,\n",
            "           [-0.4431, -0.4431, -0.4431,  ..., -0.4431, -0.4431, -0.4431],\n",
            "           [-0.4431, -0.4431, -0.4431,  ..., -0.4431, -0.4431, -0.4431],\n",
            "           [-0.4431, -0.4431, -0.4431,  ..., -0.4431, -0.4431, -0.4431]],\n",
            "\n",
            "          [[-0.4431, -0.4431, -0.4431,  ..., -0.4431, -0.4431, -0.4431],\n",
            "           [-0.4431, -0.4431, -0.4431,  ..., -0.4431, -0.4431, -0.4431],\n",
            "           [-0.4431, -0.4431, -0.4431,  ..., -0.4431, -0.4431, -0.4431],\n",
            "           ...,\n",
            "           [-0.4431, -0.4431, -0.4431,  ..., -0.4431, -0.4431, -0.4431],\n",
            "           [-0.4431, -0.4431, -0.4431,  ..., -0.4431, -0.4431, -0.4431],\n",
            "           [-0.4431, -0.4431, -0.4431,  ..., -0.4431, -0.4431, -0.4431]],\n",
            "\n",
            "          [[-0.4431, -0.4431, -0.4431,  ..., -0.4431, -0.4431, -0.4431],\n",
            "           [-0.4431, -0.4431, -0.4431,  ..., -0.4431, -0.4431, -0.4431],\n",
            "           [-0.4431, -0.4431, -0.4431,  ..., -0.4431, -0.4431, -0.4431],\n",
            "           ...,\n",
            "           [-0.4431, -0.4431, -0.4431,  ..., -0.4431, -0.4431, -0.4431],\n",
            "           [-0.4431, -0.4431, -0.4431,  ..., -0.4431, -0.4431, -0.4431],\n",
            "           [-0.4431, -0.4431, -0.4431,  ..., -0.4431, -0.4431, -0.4431]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 0.8510,  0.8510,  0.8510,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           ...,\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.7804,  0.7804,  0.7804]],\n",
            "\n",
            "          [[ 0.8510,  0.8510,  0.8510,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           ...,\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.7804,  0.7804,  0.7804]],\n",
            "\n",
            "          [[ 0.8510,  0.8510,  0.8510,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           ...,\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.7804,  0.7804,  0.7804],\n",
            "           [ 0.8510,  0.8510,  0.8510,  ...,  0.7804,  0.7804,  0.7804]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[-0.8118, -0.8118, -0.8118,  ..., -0.8118, -0.8118, -0.8118],\n",
            "           [-0.8039, -0.8039, -0.8039,  ..., -0.8039, -0.8039, -0.8039],\n",
            "           [-0.7961, -0.7961, -0.7961,  ..., -0.7961, -0.7961, -0.7961],\n",
            "           ...,\n",
            "           [-0.7098, -0.7098, -0.7098,  ..., -0.7098, -0.7098, -0.7098],\n",
            "           [-0.7098, -0.7098, -0.7098,  ..., -0.7098, -0.7098, -0.7098],\n",
            "           [-0.7098, -0.7098, -0.7098,  ..., -0.7098, -0.7098, -0.7098]],\n",
            "\n",
            "          [[-0.8118, -0.8118, -0.8118,  ..., -0.8118, -0.8118, -0.8118],\n",
            "           [-0.8039, -0.8039, -0.8039,  ..., -0.8039, -0.8039, -0.8039],\n",
            "           [-0.7961, -0.7961, -0.7961,  ..., -0.7961, -0.7961, -0.7961],\n",
            "           ...,\n",
            "           [-0.7098, -0.7098, -0.7098,  ..., -0.7098, -0.7098, -0.7098],\n",
            "           [-0.7098, -0.7098, -0.7098,  ..., -0.7098, -0.7098, -0.7098],\n",
            "           [-0.7098, -0.7098, -0.7098,  ..., -0.7098, -0.7098, -0.7098]],\n",
            "\n",
            "          [[-0.8118, -0.8118, -0.8118,  ..., -0.8118, -0.8118, -0.8118],\n",
            "           [-0.8039, -0.8039, -0.8039,  ..., -0.8039, -0.8039, -0.8039],\n",
            "           [-0.7961, -0.7961, -0.7961,  ..., -0.7961, -0.7961, -0.7961],\n",
            "           ...,\n",
            "           [-0.7098, -0.7098, -0.7098,  ..., -0.7098, -0.7098, -0.7098],\n",
            "           [-0.7098, -0.7098, -0.7098,  ..., -0.7098, -0.7098, -0.7098],\n",
            "           [-0.7098, -0.7098, -0.7098,  ..., -0.7098, -0.7098, -0.7098]]]]])\n",
            "caption: ['Nhan sắc của Jun Vũ trong layout Giai Nhân tại fashion show của NTK Đỗ Long ❤️', 'QUẢ LÀ MỘT NĂM ĐÁNG NHỚ CỦA TĂNG DUY TÂN VÀ BIG ARTS ENTERTAINMENT!🏆\\nTrong lễ trao giải Làn Sóng Xanh 2023 vừa qua, một mình Tăng Duy Tân ôm phát 4 giải, cộng thêm các giải khác từ dàn nghệ sĩ cùng nhà như Vũ Phụng Tiên, Hiếu Bae, Drum 7 đã mang về vô vàn tiếng thơm không chỉ cho riêng mỗi anh mà còn là niềm tự hào của Big Arts Entertainment!!\\nXin chúc năm tới anh và Big Arts Entertainment sẽ đạt được nhiều thành tích khủng hơn, cho ra đời các tác phẩm bắt tai hơn nữa! Cheers! 🥂', 'Khí cầu do thám của TQ đổ bộ nước Mỹ (2023, đã phục chế có màu).\\n#interpool', 'Hóa ra Phan Anh đúng là Thủ tướng tương lai, nhưng của Đất nước Mặt trời mọc', 'Ví dụ mà đặt thì có ai nhận thật không ta 🤔', 'Việt Nam, map game sinh tồn khó nhất thế giới.\\n-The Deep\\n#interpool', 'MC Liêu Hà Trinh hạ sinh con trai đầu lòng - Luka (Đặng Liêu Nhật Khoa) ♥️', 'Kim Soohyun nhận giải  Popularity tại Giải thưởng Nghệ thuật Baeksang lần thứ 60! Anh phát biểu trên sân khấu: \\n“Xin chào, đây là Kim Soohyun. Tôi rất vui khi nhận được một giải thưởng quý giá như vậy sau khi đóng một phim truyền hình hay như này. Trong tương lai, tôi sẽ cố gắng trở thành một diễn viên giỏi có thể mang lại hạnh phúc cho mọi người và cho cả bản thân mình. Cảm ơn mọi người.\"', '🤩 Bạn có hẹn với Facebook IRL cuối tuần này! \\n🌟 Cuối tuần này, bạn đã có kế hoạch gì chưa? Đừng bỏ lỡ không gian trải nghiệm Facebook IRL nhé!\\n💙 Bên cạnh những hoạt động workshop thú vị, Facebook IRL còn mang tới các Booth trải nghiệm của Facebook, Instagram, Messenger, Threads để các bạn tha hồ quay chụp đó nha!\\n👉🏻 Còn chờ gì mà tag vài người bạn cùng tham gia Facebook IRL ngay nhỉ!\\n#facebook IRL', 'đi thôi, sẵn sàng r', 'Sơn Tùng M-TP tại Phố đi bộ mang cả \"xe vali gấu bông\" tặng khán giả ♥️♥️♥️', 'Chiếc xe mà tôi đang đi… chiếc ở xa xa ý', 'XIUMIN ĐƯỢC FAN TÌNH CỜ BẮT GẶP VỚI VISUAL KHÔNG TUỔI\\nAnh cùng ekip đang ở Hải Phòng nghỉ ngơi để chuẩn bị cho Đại nhạc hội hoành tráng, khai trương Phố đi bộ - Công viên Vũ Yên tại Thành phố Đảo Hoàng Gia Vinhomes Royal Island (Vũ Yên, Hải Phòng) vào tối mai (1/6)\\nĐược biết, XIUMIN sẽ có buổi tổng duyệt vào lúc 10h (1/6) và trình diễn vào lúc 20h cùng ngày.', 'Cái hãng này đọc chuẩn ra là Mi Xuể. \\nKhông phải Mi Xu, không phải Mi Xê, không phải Mai Xuê.', '#VOZ_F33 - Hạ viện Mỹ tuyên “án tử” TikTok, ai sẽ thôn tính?\\nhttps://voz.vn/.../ha-vien-my-tuyen-an-tu-tiktok-ai-se.../', 'Chắc là nắc cụt rồi\\n#phetphaikhong']\n",
            "label: tensor([[1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "Batch 5:\n",
            "image: tensor([[[[[ 0.4667,  0.4667,  0.4667,  ...,  0.7098,  0.7098,  0.7098],\n",
            "           [ 0.4667,  0.4667,  0.4667,  ...,  0.7098,  0.7098,  0.7098],\n",
            "           [ 0.4667,  0.4667,  0.4667,  ...,  0.7098,  0.7098,  0.7098],\n",
            "           ...,\n",
            "           [ 0.4353,  0.4353,  0.4353,  ...,  0.5451,  0.5451,  0.5451],\n",
            "           [ 0.4431,  0.4431,  0.4353,  ...,  0.5529,  0.5529,  0.5451],\n",
            "           [ 0.4510,  0.4431,  0.4431,  ...,  0.5529,  0.5529,  0.5529]],\n",
            "\n",
            "          [[ 0.4667,  0.4667,  0.4667,  ...,  0.7098,  0.7098,  0.7098],\n",
            "           [ 0.4667,  0.4667,  0.4667,  ...,  0.7098,  0.7098,  0.7098],\n",
            "           [ 0.4667,  0.4667,  0.4667,  ...,  0.7098,  0.7098,  0.7098],\n",
            "           ...,\n",
            "           [ 0.4353,  0.4353,  0.4353,  ...,  0.5451,  0.5451,  0.5451],\n",
            "           [ 0.4431,  0.4431,  0.4353,  ...,  0.5529,  0.5529,  0.5451],\n",
            "           [ 0.4510,  0.4431,  0.4431,  ...,  0.5529,  0.5529,  0.5529]],\n",
            "\n",
            "          [[ 0.4667,  0.4667,  0.4667,  ...,  0.7098,  0.7098,  0.7098],\n",
            "           [ 0.4667,  0.4667,  0.4667,  ...,  0.7098,  0.7098,  0.7098],\n",
            "           [ 0.4667,  0.4667,  0.4667,  ...,  0.7098,  0.7098,  0.7098],\n",
            "           ...,\n",
            "           [ 0.4353,  0.4353,  0.4353,  ...,  0.5451,  0.5451,  0.5451],\n",
            "           [ 0.4431,  0.4431,  0.4353,  ...,  0.5529,  0.5529,  0.5451],\n",
            "           [ 0.4510,  0.4431,  0.4431,  ...,  0.5529,  0.5529,  0.5529]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[-0.0902, -0.1294, -0.1294,  ..., -0.1765, -0.1765, -0.1843],\n",
            "           [-0.1294, -0.2078, -0.1922,  ..., -0.1843, -0.1686, -0.1765],\n",
            "           [-0.1922, -0.2078, -0.2549,  ..., -0.1922, -0.1686, -0.1686],\n",
            "           ...,\n",
            "           [-0.1686, -0.1529, -0.1843,  ..., -0.3098, -0.3569, -0.3804],\n",
            "           [-0.1529, -0.1765, -0.2157,  ..., -0.3255, -0.3412, -0.3882],\n",
            "           [-0.1608, -0.1765, -0.1843,  ..., -0.3020, -0.3569, -0.3333]],\n",
            "\n",
            "          [[-0.0902, -0.1294, -0.1294,  ..., -0.1765, -0.1765, -0.1843],\n",
            "           [-0.1294, -0.2078, -0.1922,  ..., -0.1843, -0.1686, -0.1765],\n",
            "           [-0.1922, -0.2078, -0.2549,  ..., -0.1922, -0.1686, -0.1686],\n",
            "           ...,\n",
            "           [-0.1686, -0.1529, -0.1843,  ..., -0.3098, -0.3569, -0.3804],\n",
            "           [-0.1529, -0.1765, -0.2157,  ..., -0.3255, -0.3412, -0.3882],\n",
            "           [-0.1608, -0.1765, -0.1843,  ..., -0.3020, -0.3569, -0.3333]],\n",
            "\n",
            "          [[-0.0902, -0.1294, -0.1294,  ..., -0.1765, -0.1765, -0.1843],\n",
            "           [-0.1294, -0.2078, -0.1922,  ..., -0.1843, -0.1686, -0.1765],\n",
            "           [-0.1922, -0.2078, -0.2549,  ..., -0.1922, -0.1686, -0.1686],\n",
            "           ...,\n",
            "           [-0.1686, -0.1529, -0.1843,  ..., -0.3098, -0.3569, -0.3804],\n",
            "           [-0.1529, -0.1765, -0.2157,  ..., -0.3255, -0.3412, -0.3882],\n",
            "           [-0.1608, -0.1765, -0.1843,  ..., -0.3020, -0.3569, -0.3333]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[-0.5529, -0.5059, -0.4588,  ...,  0.0902,  0.1059,  0.1059],\n",
            "           [-0.5529, -0.5059, -0.4588,  ...,  0.1373,  0.1529,  0.1529],\n",
            "           [-0.5529, -0.5059, -0.4588,  ...,  0.1765,  0.1922,  0.1922],\n",
            "           ...,\n",
            "           [ 0.0745,  0.1059,  0.1216,  ...,  0.5216,  0.4667,  0.5216],\n",
            "           [ 0.0667,  0.0980,  0.1216,  ...,  0.5922,  0.5529,  0.6078],\n",
            "           [ 0.0510,  0.0745,  0.0902,  ...,  0.6784,  0.6706,  0.7020]],\n",
            "\n",
            "          [[-0.5529, -0.5059, -0.4588,  ...,  0.0902,  0.1059,  0.1059],\n",
            "           [-0.5529, -0.5059, -0.4588,  ...,  0.1373,  0.1529,  0.1529],\n",
            "           [-0.5529, -0.5059, -0.4588,  ...,  0.1765,  0.1922,  0.1922],\n",
            "           ...,\n",
            "           [ 0.0745,  0.1059,  0.1216,  ...,  0.5216,  0.4667,  0.5216],\n",
            "           [ 0.0667,  0.0980,  0.1216,  ...,  0.5922,  0.5529,  0.6078],\n",
            "           [ 0.0510,  0.0745,  0.0902,  ...,  0.6784,  0.6706,  0.7020]],\n",
            "\n",
            "          [[-0.5529, -0.5059, -0.4588,  ...,  0.0902,  0.1059,  0.1059],\n",
            "           [-0.5529, -0.5059, -0.4588,  ...,  0.1373,  0.1529,  0.1529],\n",
            "           [-0.5529, -0.5059, -0.4588,  ...,  0.1765,  0.1922,  0.1922],\n",
            "           ...,\n",
            "           [ 0.0745,  0.1059,  0.1216,  ...,  0.5216,  0.4667,  0.5216],\n",
            "           [ 0.0667,  0.0980,  0.1216,  ...,  0.5922,  0.5529,  0.6078],\n",
            "           [ 0.0510,  0.0745,  0.0902,  ...,  0.6784,  0.6706,  0.7020]]]],\n",
            "\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 0.7098,  0.5059,  0.3804,  ...,  0.8431,  0.8431,  0.8431],\n",
            "           [ 0.4980,  0.2235,  0.2549,  ...,  0.8431,  0.8431,  0.8431],\n",
            "           [ 0.2784,  0.3020,  0.5608,  ...,  0.8431,  0.8431,  0.8431],\n",
            "           ...,\n",
            "           [ 0.1059,  0.1373,  0.1608,  ..., -0.1922, -0.1765, -0.1765],\n",
            "           [ 0.0902,  0.1294,  0.1529,  ..., -0.1922, -0.1765, -0.1686],\n",
            "           [ 0.1059,  0.1373,  0.1686,  ..., -0.1922, -0.1765, -0.1686]],\n",
            "\n",
            "          [[ 0.7098,  0.5059,  0.3804,  ...,  0.8431,  0.8431,  0.8431],\n",
            "           [ 0.4980,  0.2235,  0.2549,  ...,  0.8431,  0.8431,  0.8431],\n",
            "           [ 0.2784,  0.3020,  0.5608,  ...,  0.8431,  0.8431,  0.8431],\n",
            "           ...,\n",
            "           [ 0.1059,  0.1373,  0.1608,  ..., -0.1922, -0.1765, -0.1765],\n",
            "           [ 0.0902,  0.1294,  0.1529,  ..., -0.1922, -0.1765, -0.1686],\n",
            "           [ 0.1059,  0.1373,  0.1686,  ..., -0.1922, -0.1765, -0.1686]],\n",
            "\n",
            "          [[ 0.7098,  0.5059,  0.3804,  ...,  0.8431,  0.8431,  0.8431],\n",
            "           [ 0.4980,  0.2235,  0.2549,  ...,  0.8431,  0.8431,  0.8431],\n",
            "           [ 0.2784,  0.3020,  0.5608,  ...,  0.8431,  0.8431,  0.8431],\n",
            "           ...,\n",
            "           [ 0.1059,  0.1373,  0.1608,  ..., -0.1922, -0.1765, -0.1765],\n",
            "           [ 0.0902,  0.1294,  0.1529,  ..., -0.1922, -0.1765, -0.1686],\n",
            "           [ 0.1059,  0.1373,  0.1686,  ..., -0.1922, -0.1765, -0.1686]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[-0.6941, -0.6549, -0.6314,  ..., -0.8353, -0.8745, -0.8667],\n",
            "           [-0.4824, -0.4196, -0.3490,  ..., -0.8667, -0.8824, -0.8824],\n",
            "           [-0.3490, -0.2314, -0.0902,  ..., -0.7882, -0.8667, -0.9137],\n",
            "           ...,\n",
            "           [-0.9843, -0.9843, -0.9843,  ..., -0.9765, -0.9765, -0.9765],\n",
            "           [-0.9843, -0.9843, -0.9843,  ..., -0.9765, -0.9765, -0.9765],\n",
            "           [-0.9843, -0.9843, -0.9843,  ..., -0.9765, -0.9765, -0.9765]],\n",
            "\n",
            "          [[-0.6941, -0.6549, -0.6314,  ..., -0.8353, -0.8745, -0.8667],\n",
            "           [-0.4824, -0.4196, -0.3490,  ..., -0.8667, -0.8824, -0.8824],\n",
            "           [-0.3490, -0.2314, -0.0902,  ..., -0.7882, -0.8667, -0.9137],\n",
            "           ...,\n",
            "           [-0.9843, -0.9843, -0.9843,  ..., -0.9765, -0.9765, -0.9765],\n",
            "           [-0.9843, -0.9843, -0.9843,  ..., -0.9765, -0.9765, -0.9765],\n",
            "           [-0.9843, -0.9843, -0.9843,  ..., -0.9765, -0.9765, -0.9765]],\n",
            "\n",
            "          [[-0.6941, -0.6549, -0.6314,  ..., -0.8353, -0.8745, -0.8667],\n",
            "           [-0.4824, -0.4196, -0.3490,  ..., -0.8667, -0.8824, -0.8824],\n",
            "           [-0.3490, -0.2314, -0.0902,  ..., -0.7882, -0.8667, -0.9137],\n",
            "           ...,\n",
            "           [-0.9843, -0.9843, -0.9843,  ..., -0.9765, -0.9765, -0.9765],\n",
            "           [-0.9843, -0.9843, -0.9843,  ..., -0.9765, -0.9765, -0.9765],\n",
            "           [-0.9843, -0.9843, -0.9843,  ..., -0.9765, -0.9765, -0.9765]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[-0.5922, -0.5922, -0.5922,  ..., -0.5373, -0.5373, -0.5373],\n",
            "           [-0.5922, -0.5922, -0.5922,  ..., -0.5373, -0.5373, -0.5373],\n",
            "           [-0.5922, -0.5922, -0.5922,  ..., -0.5216, -0.5216, -0.5216],\n",
            "           ...,\n",
            "           [-0.2941, -0.1608, -0.1059,  ..., -0.4745, -0.4667, -0.4431],\n",
            "           [-0.2627, -0.2000, -0.1451,  ..., -0.4824, -0.4824, -0.4667],\n",
            "           [-0.1216, -0.1294, -0.1137,  ..., -0.4667, -0.4745, -0.4588]],\n",
            "\n",
            "          [[-0.5922, -0.5922, -0.5922,  ..., -0.5373, -0.5373, -0.5373],\n",
            "           [-0.5922, -0.5922, -0.5922,  ..., -0.5373, -0.5373, -0.5373],\n",
            "           [-0.5922, -0.5922, -0.5922,  ..., -0.5216, -0.5216, -0.5216],\n",
            "           ...,\n",
            "           [-0.2941, -0.1608, -0.1059,  ..., -0.4745, -0.4667, -0.4431],\n",
            "           [-0.2627, -0.2000, -0.1451,  ..., -0.4824, -0.4824, -0.4667],\n",
            "           [-0.1216, -0.1294, -0.1137,  ..., -0.4667, -0.4745, -0.4588]],\n",
            "\n",
            "          [[-0.5922, -0.5922, -0.5922,  ..., -0.5373, -0.5373, -0.5373],\n",
            "           [-0.5922, -0.5922, -0.5922,  ..., -0.5373, -0.5373, -0.5373],\n",
            "           [-0.5922, -0.5922, -0.5922,  ..., -0.5216, -0.5216, -0.5216],\n",
            "           ...,\n",
            "           [-0.2941, -0.1608, -0.1059,  ..., -0.4745, -0.4667, -0.4431],\n",
            "           [-0.2627, -0.2000, -0.1451,  ..., -0.4824, -0.4824, -0.4667],\n",
            "           [-0.1216, -0.1294, -0.1137,  ..., -0.4667, -0.4745, -0.4588]]]]])\n",
            "caption: ['Hoạt hình với thời lượng dài nhất từ trước tới nay. Hãy chuẩn bị tạm biệt 21p cuộc đời ❤', 'Một con chim rất bình thường vào thời Trung Cổ nước Pháp, thế kỉ 11... 👁🐦', 'Ăn cái cùi chỏ tều cả môi', 'Bỏ bà Trang Hí vô tội nghiệp vậy', 'Vừa livestream Sơn Tùng M-TP đã bị sập live vi phạm tiêu chuẩn cộng đồng vì lái xe trên TikTok.', 'JAYTEE HỨA SẼ RA NHẠC VÀO NGÀY NÀY CỦA 11 NĂM SAU !!!\\nBất ngờ với buổi tiệc sinh nhật vô cùng hoành tráng bao gồm 1 bánh gato và 4 quả lựu do anh Hưng Bươn cùng Miss Audition Emily tổ chức, mới đây rapper Jaytee hay còn tự nhận là Justin Timbertee đã gửi lời cảm ơn chân thành tới toàn thể bạn bè gần xa, bà con khôi phố cũng như hàng chục fan hâm mộ trên khắp các diễn đàn không gian mạng.\\nJaytee rất cảm kích vì những lời chúc, những món quà và được biết trong thời gian tới đây, cụ thể là 11 năm nữa tức 21 giờ 11 phút ngày 21/11/2022, Jaytee sẽ ra sản phẩm âm nhạc mới mang tên Forget About Her!\\nBật mí nhỏ là sản phẩm được quay bằng Nokia 7610 lỗi cam nên dù nét như Sony nhưng chỉ có 2 màu đen trắng như kiểu 1280 👍 \\nMời quý bạn và các vị 11 năm nữa quay lại. Bye!', 'FT: Man United 3-2 Newcastle\\nMấy cậu trẻ khét đèn đẹt :v', 'Top 1 app các bạn nữ nên có để kiểm tra, theo dõi kỳ🍓 Các bạn nam nếu thích thì tải cũng được 😗', \"Một người đàn ông đã mất vì đau 🫀 khi đang xem bộ phim ‘Avatar 2’ mới phát hành gần đây tại bang Andhra Pradesh, Ấn Độ. \\nNạn nhân được xác định là Lakshmireddy Srinu, đã đến một rạp chiếu phim ở địa phương cùng với em trai Raju để xem bộ phim Avatar 2. Nhưng đến giữa phim, Srinu gục ngã. \\nEm trai của anh, Raju ngay lập tức đưa anh đến bệnh viện, nơi các bác sĩ tuyên bố anh đã qua đời 😢\\nĐược biết vào năm 2010, khi phần đầu tiên của 'Avatar' được chiếu thì đã có vụ tương tự. Một người đàn ông 42 tuổi, ở Đài Loan đã qua đời vì đột quỵ. Nguyên nhân có thể do quá phấn khích khi xem siêu phẩm 3D Avatar 🙁 \\n- Kiến Điện Ảnh -\", 'Lên ae \\U0001faf6🏻', '-kem', 'Cầu được ước thấy', 'Không biết nói tiếng Anh thì bảo người ta nói tiếng Việt, thế mà trước giờ không nghĩ ra', 'Kim Soo Hyun đã xuất hiện tại Baeksang 2024 ❤️', 'Cái nào dễ thì mình làm :))', 'Cảm ơn toàn thể mọi người đã tới đám cưới của tôi!\\nĐiều đáng tiếc nhất trong ngày hôm qua đó là em ruột tôi lại bận đi thi hát hò nên không có mặt để chúc mừng hạnh phúc anh nó. Từng sát cánh bên nhau, luôn luôn có nhau, vậy mà lúc tôi cần nhất lại không hề xuất hiện.\\nThôi cũng không sao. Sự nghiệp vẫn cứ là quan trọng mà. Mong em thi Việt Nam Idol sẽ dành danh hiệu quán quân, lấy được chồng giàu cơ mà dù thành công đến mấy thì \"ruột thịt\" vẫn mãi là \"ruột thịt\" em nhé :xxxxxxxxxxxxxx :xxxxxxxxxxx Yêu em :xxxxxxxxxx']\n",
            "label: tensor([[1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "Batch 6:\n",
            "image: tensor([[[[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [ 0.9137,  0.9059,  0.8902,  ..., -0.8745, -0.8980, -0.9294],\n",
            "           [ 0.9059,  0.9137,  0.9216,  ..., -0.7961, -0.7333, -0.5765],\n",
            "           [ 0.7647,  0.7961,  0.8431,  ..., -0.2314, -0.3020, -0.3098]],\n",
            "\n",
            "          [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [ 0.9137,  0.9059,  0.8902,  ..., -0.8745, -0.8980, -0.9294],\n",
            "           [ 0.9059,  0.9137,  0.9216,  ..., -0.7961, -0.7333, -0.5765],\n",
            "           [ 0.7647,  0.7961,  0.8431,  ..., -0.2314, -0.3020, -0.3098]],\n",
            "\n",
            "          [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "           ...,\n",
            "           [ 0.9137,  0.9059,  0.8902,  ..., -0.8745, -0.8980, -0.9294],\n",
            "           [ 0.9059,  0.9137,  0.9216,  ..., -0.7961, -0.7333, -0.5765],\n",
            "           [ 0.7647,  0.7961,  0.8431,  ..., -0.2314, -0.3020, -0.3098]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[-0.8902, -0.8824, -0.8980,  ..., -0.4510, -0.4431, -0.4431],\n",
            "           [-0.8824, -0.8824, -0.8824,  ..., -0.4431, -0.4431, -0.4510],\n",
            "           [-0.8745, -0.8667, -0.8588,  ..., -0.4431, -0.4353, -0.4588],\n",
            "           ...,\n",
            "           [-0.9059, -0.8980, -0.8902,  ..., -0.9608, -0.9608, -0.9608],\n",
            "           [-0.8275, -0.8431, -0.8275,  ..., -0.9765, -0.9765, -0.9765],\n",
            "           [-0.8431, -0.8431, -0.8431,  ..., -0.9765, -0.9765, -0.9765]],\n",
            "\n",
            "          [[-0.8902, -0.8824, -0.8980,  ..., -0.4510, -0.4431, -0.4431],\n",
            "           [-0.8824, -0.8824, -0.8824,  ..., -0.4431, -0.4431, -0.4510],\n",
            "           [-0.8745, -0.8667, -0.8588,  ..., -0.4431, -0.4353, -0.4588],\n",
            "           ...,\n",
            "           [-0.9059, -0.8980, -0.8902,  ..., -0.9608, -0.9608, -0.9608],\n",
            "           [-0.8275, -0.8431, -0.8275,  ..., -0.9765, -0.9765, -0.9765],\n",
            "           [-0.8431, -0.8431, -0.8431,  ..., -0.9765, -0.9765, -0.9765]],\n",
            "\n",
            "          [[-0.8902, -0.8824, -0.8980,  ..., -0.4510, -0.4431, -0.4431],\n",
            "           [-0.8824, -0.8824, -0.8824,  ..., -0.4431, -0.4431, -0.4510],\n",
            "           [-0.8745, -0.8667, -0.8588,  ..., -0.4431, -0.4353, -0.4588],\n",
            "           ...,\n",
            "           [-0.9059, -0.8980, -0.8902,  ..., -0.9608, -0.9608, -0.9608],\n",
            "           [-0.8275, -0.8431, -0.8275,  ..., -0.9765, -0.9765, -0.9765],\n",
            "           [-0.8431, -0.8431, -0.8431,  ..., -0.9765, -0.9765, -0.9765]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[-0.4275, -0.4431, -0.4431,  ..., -0.4196, -0.5608, -0.5922],\n",
            "           [-0.4353, -0.4431, -0.4353,  ..., -0.3961, -0.4824, -0.4745],\n",
            "           [-0.4275, -0.4196, -0.4118,  ..., -0.3882, -0.3961, -0.4039],\n",
            "           ...,\n",
            "           [-0.3647, -0.3490, -0.3490,  ...,  0.1137,  0.4745,  0.7020],\n",
            "           [-0.3569, -0.3490, -0.3490,  ..., -0.2706,  0.0745,  0.6235],\n",
            "           [-0.3490, -0.3490, -0.3569,  ..., -0.3725, -0.2627,  0.4196]],\n",
            "\n",
            "          [[-0.4275, -0.4431, -0.4431,  ..., -0.4196, -0.5608, -0.5922],\n",
            "           [-0.4353, -0.4431, -0.4353,  ..., -0.3961, -0.4824, -0.4745],\n",
            "           [-0.4275, -0.4196, -0.4118,  ..., -0.3882, -0.3961, -0.4039],\n",
            "           ...,\n",
            "           [-0.3647, -0.3490, -0.3490,  ...,  0.1137,  0.4745,  0.7020],\n",
            "           [-0.3569, -0.3490, -0.3490,  ..., -0.2706,  0.0745,  0.6235],\n",
            "           [-0.3490, -0.3490, -0.3569,  ..., -0.3725, -0.2627,  0.4196]],\n",
            "\n",
            "          [[-0.4275, -0.4431, -0.4431,  ..., -0.4196, -0.5608, -0.5922],\n",
            "           [-0.4353, -0.4431, -0.4353,  ..., -0.3961, -0.4824, -0.4745],\n",
            "           [-0.4275, -0.4196, -0.4118,  ..., -0.3882, -0.3961, -0.4039],\n",
            "           ...,\n",
            "           [-0.3647, -0.3490, -0.3490,  ...,  0.1137,  0.4745,  0.7020],\n",
            "           [-0.3569, -0.3490, -0.3490,  ..., -0.2706,  0.0745,  0.6235],\n",
            "           [-0.3490, -0.3490, -0.3569,  ..., -0.3725, -0.2627,  0.4196]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 0.7961,  0.8039,  0.8039,  ...,  0.8353,  0.8353,  0.8353],\n",
            "           [ 0.8039,  0.8118,  0.8118,  ...,  0.8588,  0.8431,  0.8275],\n",
            "           [ 0.8039,  0.8118,  0.8118,  ...,  0.8510,  0.8431,  0.8431],\n",
            "           ...,\n",
            "           [ 0.6392,  0.6627,  0.6471,  ...,  0.7412,  0.7333,  0.7333],\n",
            "           [ 0.6314,  0.6549,  0.6549,  ...,  0.7490,  0.7412,  0.7333],\n",
            "           [ 0.6235,  0.6392,  0.6392,  ...,  0.7333,  0.7333,  0.7333]],\n",
            "\n",
            "          [[ 0.7961,  0.8039,  0.8039,  ...,  0.8353,  0.8353,  0.8353],\n",
            "           [ 0.8039,  0.8118,  0.8118,  ...,  0.8588,  0.8431,  0.8275],\n",
            "           [ 0.8039,  0.8118,  0.8118,  ...,  0.8510,  0.8431,  0.8431],\n",
            "           ...,\n",
            "           [ 0.6392,  0.6627,  0.6471,  ...,  0.7412,  0.7333,  0.7333],\n",
            "           [ 0.6314,  0.6549,  0.6549,  ...,  0.7490,  0.7412,  0.7333],\n",
            "           [ 0.6235,  0.6392,  0.6392,  ...,  0.7333,  0.7333,  0.7333]],\n",
            "\n",
            "          [[ 0.7961,  0.8039,  0.8039,  ...,  0.8353,  0.8353,  0.8353],\n",
            "           [ 0.8039,  0.8118,  0.8118,  ...,  0.8588,  0.8431,  0.8275],\n",
            "           [ 0.8039,  0.8118,  0.8118,  ...,  0.8510,  0.8431,  0.8431],\n",
            "           ...,\n",
            "           [ 0.6392,  0.6627,  0.6471,  ...,  0.7412,  0.7333,  0.7333],\n",
            "           [ 0.6314,  0.6549,  0.6549,  ...,  0.7490,  0.7412,  0.7333],\n",
            "           [ 0.6235,  0.6392,  0.6392,  ...,  0.7333,  0.7333,  0.7333]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[-0.3647, -0.3647, -0.3804,  ..., -0.0588, -0.0588, -0.0588],\n",
            "           [-0.3647, -0.3647, -0.3725,  ..., -0.0588, -0.0588, -0.0588],\n",
            "           [-0.3647, -0.3647, -0.3725,  ..., -0.0588, -0.0510, -0.0667],\n",
            "           ...,\n",
            "           [-0.6157, -0.6392, -0.6392,  ..., -0.3647, -0.3725, -0.3725],\n",
            "           [-0.6157, -0.6235, -0.6549,  ..., -0.3412, -0.3490, -0.3412],\n",
            "           [-0.6471, -0.6235, -0.6471,  ..., -0.3333, -0.3333, -0.3176]],\n",
            "\n",
            "          [[-0.3647, -0.3647, -0.3804,  ..., -0.0588, -0.0588, -0.0588],\n",
            "           [-0.3647, -0.3647, -0.3725,  ..., -0.0588, -0.0588, -0.0588],\n",
            "           [-0.3647, -0.3647, -0.3725,  ..., -0.0588, -0.0510, -0.0667],\n",
            "           ...,\n",
            "           [-0.6157, -0.6392, -0.6392,  ..., -0.3647, -0.3725, -0.3725],\n",
            "           [-0.6157, -0.6235, -0.6549,  ..., -0.3412, -0.3490, -0.3412],\n",
            "           [-0.6471, -0.6235, -0.6471,  ..., -0.3333, -0.3333, -0.3176]],\n",
            "\n",
            "          [[-0.3647, -0.3647, -0.3804,  ..., -0.0588, -0.0588, -0.0588],\n",
            "           [-0.3647, -0.3647, -0.3725,  ..., -0.0588, -0.0588, -0.0588],\n",
            "           [-0.3647, -0.3647, -0.3725,  ..., -0.0588, -0.0510, -0.0667],\n",
            "           ...,\n",
            "           [-0.6157, -0.6392, -0.6392,  ..., -0.3647, -0.3725, -0.3725],\n",
            "           [-0.6157, -0.6235, -0.6549,  ..., -0.3412, -0.3490, -0.3412],\n",
            "           [-0.6471, -0.6235, -0.6471,  ..., -0.3333, -0.3333, -0.3176]]]]])\n",
            "caption: ['Cũng là độ mà là độ body😆', 'Kai Havertz = Sterling = 9 bàn.\\n2 vua phá lưới của Chelsea mùa này.', 'Zalo tiện thật, kết nối được cả âm dương', 'Mời được idol này nữa thì phải nói là đỉnh', '2 bài học rút ra từ Turning Red:\\n1. Các bà mẹ Châu Á không bao giờ nói xin lỗi.\\n2. Vẫn là các bà mẹ Châu Á, sẽ nấu đi nấu lại một món ăn nếu bạn khen ngon, nấu đến khi bạn ngán đến tận cổ và nói rằng đừng nấu món đó nữa thì thôi. Cha mẹ rất yêu thương chúng ta, chỉ có điều cách thể hiện đôi khi làm chúng ta thấy không thoải mái. Khoảng cách thế hệ chăng?\\n-Kiến Điện Ảnh-\\nPhim: Turning Red - Gấu đỏ biến hình.']\n",
            "label: tensor([[0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))\n",
        "print(batch['image_transformed'][0], batch['caption'][0], batch['label'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DdBia7sTAg_W",
        "outputId": "bcc090a7-b6b8-49d0-c971-be1ba598660e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
            "\n",
            "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
            "\n",
            "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]]]) Nhật thực, nguyệt thực từ lâu đã là những hiện tượng thiên văn thu hút sự chú ý của loài người. Cơ mà định nghĩa “thiết thực” dưới đây thì có vẻ vẫn còn khá mới mẻ với nhiều người.\n",
            "Theo đó, khác với nhật thực và nguyệt thực hiếm khi xảy ra, sự “thiết thực” có thể được tạo ra bất cứ khi nào khách hàng sử dụng tính năng thanh FacePay của TPBank. Vào khoảnh khắc khuôn mặt của khách hàng chuyển động tịnh tiến trên một đường thẳng với máy scan FacePay và nhân viên thu ngân, bill mua hàng sẽ tự động được thanh toán một cách “thần kỳ” mà không cần có sự tác động của tiền mặt, thẻ hay bất kỳ thiết bị thông minh nào khác.\n",
            "Hiện tượng này được giải thích là phương thức thanh toán dựa trên công nghệ nhận dạng khuôn mặt, là một trong những tính năng nổi bật trong app TPBank. Có ai đã từng tự mình trải nghiệm hiện tượng thú vị này chưa?\n",
            "#TPBank #BankingĐậmChấtTÔI #FacePay tensor([1., 0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch['image_transformed'].shape)\n",
        "print(batch['label'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_2w8W7FgRXN",
        "outputId": "754ce268-8a86-482f-88ad-6b0dc079c46d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3, 224, 224])\n",
            "torch.Size([16, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(train_loader, batch):\n",
        "  \"\"\"\n",
        "    Plot image + caption + label\n",
        "\n",
        "    Notion:\n",
        "      Get image with 3 dimension [height, width, channel]\n",
        "  \"\"\"\n",
        "  for i in range(len(batch)):\n",
        "      image = batch['image_transformed'][i] # -> 3 dimension [channel, height, width]\n",
        "      caption = batch['caption'][i]\n",
        "      label = batch['label'][i]\n",
        "      # Reshape to [height, width, channels]\n",
        "      image_reshaped = image.permute(1, 2, 0).numpy() # [height, width, channel]\n",
        "      # Unnormalize the image\n",
        "      mean = [0.485, 0.456, 0.406]\n",
        "      std = [0.229, 0.224, 0.225]\n",
        "      image = std * image_reshaped + mean\n",
        "      image = np.clip(image, 0, 1)\n",
        "      plt.figure(figsize=(50,30))\n",
        "      plt.subplot(3,1, i+1)\n",
        "      plt.imshow(image)\n",
        "      plt.title(f'caption: {caption} / \\nlabel: {label}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plot(train_loader, batch)"
      ],
      "metadata": {
        "id": "snnIMlt1APR_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1be0c4c2-75bd-4fd5-8645-e0a3069e0927",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/events.py:89: UserWarning: Glyph 128293 (\\N{FIRE}) missing from current font.\n",
            "  func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/events.py:89: UserWarning: Glyph 128165 (\\N{COLLISION SYMBOL}) missing from current font.\n",
            "  func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/events.py:89: UserWarning: Glyph 127873 (\\N{WRAPPED PRESENT}) missing from current font.\n",
            "  func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/events.py:89: UserWarning: Glyph 128073 (\\N{WHITE RIGHT POINTING BACKHAND INDEX}) missing from current font.\n",
            "  func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/events.py:89: UserWarning: Glyph 128308 (\\N{LARGE RED CIRCLE}) missing from current font.\n",
            "  func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 5000x3000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 128293 (\\N{FIRE}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 128165 (\\N{COLLISION SYMBOL}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 127873 (\\N{WRAPPED PRESENT}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 128073 (\\N{WHITE RIGHT POINTING BACKHAND INDEX}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 5000x3000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 128308 (\\N{LARGE RED CIRCLE}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 5000x3000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer, VisualBertModel"
      ],
      "metadata": {
        "id": "IGFbMUh2nKP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "sBZ_vqKpaKu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407,
          "referenced_widgets": [
            "8e55535a13da4b3d95269cf10da80c37",
            "2bae44cf673f40d3a20ec9ca8f1374d7",
            "020d65bc86744457aeea90c3c60c2196",
            "83ead130d62949a386a77d636902b65d",
            "48e87188873f4d98ac2bdef2caab5aeb",
            "d071061d78d843fe81d39e62a2e5fb9a",
            "9482f444f2e846bd9b75f96d9b9b590d",
            "d2991976398441c79a7b6f1df2f88ecd",
            "be0f8b60d0b4422cbebfe501bb32f7ee",
            "e2a4d8210957420eb36817f09b0f8eae",
            "115c8f7239284d43b42fa1911d9781ef",
            "4a0503460b0941ffbb65dba52439fdf0",
            "b8399452c47543f4b18bde487f272cb2",
            "27839091c91946c191714bf97af69564",
            "ad2046faa5244eaa8358c71be1c11b31",
            "770b2de78ace44ab9d8e57ff7a07ad83",
            "caede153314140da943b965b8cd71379",
            "f2435bb4301c4b2e98513792de39c196",
            "9d240b59a5374c3aa1a13e834c86dade",
            "d415f05371a64d339a8a5a8bf2a5e8f2",
            "7eb3ddcf7bcf47ad872d184d9dfcbda3",
            "884b6d85f86942f1812249be4efc2fa5",
            "57c8981d26c44f30962bab92e575238d",
            "a8a14ad946d3488db1dc677f89345f9f",
            "caa6a58e63334d758e4951e768574513",
            "5a8f7f8517a84e16a98b44df7f3240c8",
            "c552adef3466493e8a2145b9bab0b8d9",
            "01733bdd6f544210a64e4fecd10655db",
            "99b88962681c4bc18f7a80ec144076f4",
            "c7b15053c4524031af278e2455b2788a",
            "da0ef2eea0cb4665829113e7b974177c",
            "f18e33b7723247be97ddb77829393a50",
            "d53c9fd1e8fa4c32999cec1032c83238",
            "0c6ae9ad8d514e1d83c5da8eba0cb4f1",
            "ed19d3c6cc774b0a81133f4b9bb59306",
            "8a179329f89243b88d57200a58cf84cc",
            "6c843da549a24d3385f4b9c54e5b6675",
            "f9a5e4989296406dbe6e9057d5012cbe",
            "32cf42639b6642eaaa9fcdd742b7f23d",
            "92cb03b127c94193b5ef6d1ba54674a1",
            "1983c74f0fcc496eb0e6d93ae83ef01f",
            "090ad52ce9964babb7ac6726a6fd1742",
            "af84f31944bc46bebb5acdc12b7baaa6",
            "6d2ae9f3837b4852a5a9fac603dd557c",
            "d54fa4e6e8774ed1a686d64128d478a5",
            "98a3952e95144efc94c43ccb7c66a702",
            "8e7d5be9ddd745aa8e2e49f0a4fda505",
            "0a0eb7485eb743bc81bcf407b6d42267",
            "1cbd9975645549aaae8373ced3db4ae5",
            "52abcbe567724afab50d6bcb9af8ba6c",
            "134179b4b9eb4841a69d5eedd7608a3a",
            "6e314f8f820b4660b3437104e08c3489",
            "9f7f221acd7e4d7185e7db8f796dd0c5",
            "bb4880df420e44848134be75cdd34199",
            "8665d34ffcef4f4a8c5e3b5162c6eaf8",
            "7c679c9a9dd2491999cfd12beef887a5",
            "386c5ee4467840789fc559628b7aade5",
            "30f3895f78ee4141b8694a77caad0dac",
            "d7769cf25a5447288744b906637b3434",
            "c571ab994d484f2196f9284b55598102",
            "1e6addda3f0040fe9ca9892601232646",
            "3c4c71500d8a4bb4a3a006b82138318e",
            "3f8432f5a3b547be91817c70fe792eaf",
            "0c17750ae8ee4a5287e671fce4d42b17",
            "cf61a0f764db414a83557620a70d7da8",
            "37f7191e0ccd4227b5320c27ae4e4219",
            "0c3601016cb44fbda1c1aedd5519e78e",
            "201d031b32204092b4130cd416805fc7",
            "21a4e421252e4b32bf01edc988087c38",
            "efe1a1ec2eb147bca770f5d3469ec757",
            "d87960635bad4c8590f57ac41787c023",
            "963b25ddf3414b578935f470ec28c5c2",
            "fc519e5fcdb546f08d7c498a6c537281",
            "6e356da2be3b4913bcf64c79f719078e",
            "57af0a2fbc66462bb30412b1d6be6918",
            "40fa9d2a28c44a8ca2392432c03bc0aa",
            "d29c9b56d7d2474f9f821c73273a66c2"
          ]
        },
        "id": "fVXRZvM2Cift",
        "outputId": "00861c2d-4adf-4777-bd25-33cd06f88180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e55535a13da4b3d95269cf10da80c37"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/448M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a0503460b0941ffbb65dba52439fdf0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57c8981d26c44f30962bab92e575238d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/543M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c6ae9ad8d514e1d83c5da8eba0cb4f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d54fa4e6e8774ed1a686d64128d478a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c679c9a9dd2491999cfd12beef887a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c3601016cb44fbda1c1aedd5519e78e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phobert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyeBEP8dC9p-",
        "outputId": "26dcf190-2c62-4785-c6d6-e9e4e4458453",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): RobertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): RobertaPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5zfczTADgO5",
        "outputId": "19d047c8-f457-491b-8561-6882833f2a4f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisualBertModel(\n",
              "  (embeddings): VisualBertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (visual_token_type_embeddings): Embedding(2, 768)\n",
              "    (visual_position_embeddings): Embedding(512, 768)\n",
              "    (visual_projection): Linear(in_features=2048, out_features=768, bias=True)\n",
              "  )\n",
              "  (encoder): VisualBertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x VisualBertLayer(\n",
              "        (attention): VisualBertAttention(\n",
              "          (self): VisualBertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): VisualBertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): VisualBertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): VisualBertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): VisualBertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import VisualBertModel, AutoModel\n",
        "\n",
        "\n",
        "class VisualBertForClassification(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super(VisualBertForClassification, self).__init__()\n",
        "        self.visualbert = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
        "\n",
        "        # Replace the embedding layers to match PhoBERT\n",
        "        self.visualbert.embeddings.word_embeddings = self.phobert.embeddings.word_embeddings\n",
        "        self.visualbert.position_embeddings = self.phobert.embeddings.position_embeddings\n",
        "        self.visualbert.token_type_embeddings = self.phobert.embeddings.token_type_embeddings\n",
        "\n",
        "        # Use the correct size for the classifier\n",
        "        self.classifier = nn.Linear(self.visualbert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, visual_embeds, visual_token_type_ids, visual_attention_mask):\n",
        "\n",
        "        # Get outputs from VisualBERT\n",
        "        outputs = self.visualbert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            visual_embeds=visual_embeds,\n",
        "            visual_token_type_ids=visual_token_type_ids,\n",
        "            visual_attention_mask=visual_attention_mask\n",
        "        )\n",
        "\n",
        "        # Use the pooled output for classification\n",
        "        pooled_output = outputs.pooler_output  # Shape: (batch_size, 768)\n",
        "        print(\"Pooled output shape:\", pooled_output.shape)\n",
        "\n",
        "        # Compute logits from the pooled output\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "2kFyvpheH9O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = 4  # Adjust based on your classification task\n",
        "model = VisualBertForClassification(num_labels).to(device)"
      ],
      "metadata": {
        "id": "3-l7QEbXIA57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JU6t73p5JTnD",
        "outputId": "a5b0b520-0914-4e83-a6fe-2ec2fe0d3175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisualBertForClassification(\n",
              "  (visualbert): VisualBertModel(\n",
              "    (embeddings): VisualBertEmbeddings(\n",
              "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (visual_token_type_embeddings): Embedding(2, 768)\n",
              "      (visual_position_embeddings): Embedding(512, 768)\n",
              "      (visual_projection): Linear(in_features=2048, out_features=768, bias=True)\n",
              "    )\n",
              "    (encoder): VisualBertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x VisualBertLayer(\n",
              "          (attention): VisualBertAttention(\n",
              "            (self): VisualBertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): VisualBertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): VisualBertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): VisualBertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): VisualBertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "    (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "  )\n",
              "  (phobert): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor, ViTModel"
      ],
      "metadata": {
        "id": "XNTjMTu-gpBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visual_embeddings(image):\n",
        "    feature_extractor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "    model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\").to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Load and preprocess the image using the feature extractor for ViT\n",
        "    inputs = feature_extractor(images=image, return_tensors=\"pt\").to(device)\n",
        "    # inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Get the embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state\n",
        "\n",
        "    # Process the embeddings to match the shape (batch_size, 1, 2048)\n",
        "    embeddings = embeddings.mean(dim=1)  # Average over the sequence length\n",
        "    embeddings = embeddings.unsqueeze(1)  # Add sequence dimension\n",
        "    embeddings = torch.nn.functional.adaptive_avg_pool2d(embeddings, (1, 2048))  # Ensure the last dimension is 2048\n",
        "\n",
        "    return embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "UtKVKX-5h_kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "_iqJo8HRD5rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 2\n",
        "total_loss = 0\n",
        "torch.cuda.empty_cache()\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  for batch in tqdm(train_loader):\n",
        "      images = batch['image_transformed'].to(device)  # Shape: (batch_size, 3, 224, 224)\n",
        "      caption = batch['caption']\n",
        "      label = batch['label'].to(device)\n",
        "      images = images.squeeze(1).to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      images = (images + 1) / 2  # Assuming images are in range [-1, 1]\n",
        "      images = torch.clamp(images, 0, 1)  # Ensure values are in [0, 1]\n",
        "\n",
        "\n",
        "      # Tokenize captions\n",
        "      inputs = tokenizer(caption, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512).to(device)\n",
        "      visual_embeds = visual_embeddings(images).to(device)\n",
        "      print(\"Visual embeds shape after extraction:\", visual_embeds.shape)\n",
        "\n",
        "\n",
        "      # Create visual token type and attention masks\n",
        "      visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).to(device)\n",
        "      visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float).to(device)\n",
        "\n",
        "      #print(\"Input IDs shape:\", inputs['input_ids'].shape) # (batch, dimension)\n",
        "      #print(\"Attention mask shape:\", inputs['attention_mask'].shape) # (batch, dimension)\n",
        "      #print(\"Token type IDs shape:\", inputs['token_type_ids'].shape) # (batch, dimension)\n",
        "      #print(\"Visual embeds shape:\", visual_embeds.shape)             #(batch, num patch, dimension)\n",
        "      #print(\"Visual token type IDs shape:\", visual_token_type_ids.shape) # (batch, num patch)\n",
        "      #print(\"Visual attention mask shape:\", visual_attention_mask.shape) # (batch, num patch)\n",
        "\n",
        "      # Forward pass\n",
        "      inputs.update ({\n",
        "          \"visual_embeds\": visual_embeds,\n",
        "          \"visual_token_type_ids\": visual_token_type_ids,\n",
        "          \"visual_attention_mask\": visual_attention_mask,\n",
        "      }\n",
        "    )\n",
        "      outputs = model(**inputs)\n",
        "      # Calculate loss\n",
        "      loss_value = loss_fn(outputs, label)\n",
        "\n",
        "      # Backward pass and optimization\n",
        "      loss_value.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss += loss_value.item()\n",
        "      print(f\"Batch Loss: {loss_value.item()}\")  # Print loss for the current batch\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "  avg_epoch_loss = total_loss / len(train_loader)\n",
        "  print(f\"total test loss: {total_loss}\")\n",
        "  print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "  torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': total_loss\n",
        "            }, \"visual_bert_epoch_{epoch}.pth\")\n",
        "\n",
        "  print(f\"Model weights saved for epoch {epoch}.\")\n",
        "\n",
        "  # Evaluation\n",
        "  model.eval()  # Set the model to evaluation mode\n",
        "  total_test_loss = 0\n",
        "  with torch.no_grad():  # Disable gradient calculation\n",
        "      for batch in tqdm(test_loader):\n",
        "          images = batch['image_transformed'].to(device)\n",
        "          caption = batch['caption']\n",
        "          label = batch['label'].to(device)\n",
        "          images = images.squeeze(1).to(device)\n",
        "\n",
        "          images = (images + 1) / 2  # Normalize images\n",
        "          images = torch.clamp(images, 0, 1)\n",
        "\n",
        "          # Tokenize captions\n",
        "          inputs = tokenizer(caption, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512).to(device)\n",
        "          visual_embeds = visual_embeddings(images).to(device)\n",
        "\n",
        "          # Create visual token type and attention masks\n",
        "          visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).to(device)\n",
        "          visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float).to(device)\n",
        "\n",
        "          # Prepare inputs for the model\n",
        "          inputs.update({\n",
        "              \"visual_embeds\": visual_embeds,\n",
        "              \"visual_token_type_ids\": visual_token_type_ids,\n",
        "              \"visual_attention_mask\": visual_attention_mask,\n",
        "          })\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = model(**inputs)\n",
        "          # Calculate loss\n",
        "          loss_value = loss_fn(outputs, label)\n",
        "\n",
        "          total_test_loss += loss_value.item()\n",
        "\n",
        "          print(f\"Test Batch Loss: {loss_value.item()}\")  # Print loss for the current test batch\n",
        "\n",
        "  # Print average test loss\n",
        "  avg_test_loss = total_test_loss / len(test_loader)\n",
        "  print(f\"total test loss: {total_test_loss}\")\n",
        "  print(f\"Average Test Loss: {avg_test_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "opGf_MdCEhho",
        "outputId": "552102c8-4be4-4a73-baf0-f7dc770e419d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/676 [00:00<?, ?it/s]Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 1/676 [00:04<49:07,  4.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6590728759765625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 2/676 [00:07<43:51,  3.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4917442798614502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 3/676 [00:11<43:28,  3.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.48420092463493347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 4/676 [00:16<44:56,  4.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6610894203186035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 5/676 [00:19<42:34,  3.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8077300190925598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 6/676 [00:23<42:07,  3.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6613569259643555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 7/676 [00:26<40:30,  3.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7320696711540222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 8/676 [00:29<38:53,  3.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.262946367263794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|▏         | 9/676 [00:32<37:43,  3.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4639561176300049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|▏         | 10/676 [00:36<36:50,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.353525996208191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 11/676 [00:39<36:07,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.859021782875061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 12/676 [00:42<35:48,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6685160994529724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 13/676 [00:45<35:23,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9108179211616516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 14/676 [00:48<34:58,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7545038461685181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 15/676 [00:51<35:03,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9520711898803711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 16/676 [00:54<35:10,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.888340950012207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 17/676 [00:58<34:47,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6230150461196899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 18/676 [01:01<35:12,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5680500268936157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 19/676 [01:04<35:01,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6245974898338318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 20/676 [01:07<35:05,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5788359045982361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 21/676 [01:11<35:14,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6116063594818115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 22/676 [01:14<34:41,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.650629997253418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 23/676 [01:17<34:34,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.45300063490867615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▎         | 24/676 [01:20<34:40,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5425696969032288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▎         | 25/676 [01:23<34:22,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5036941766738892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 26/676 [01:26<34:13,  3.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5984042882919312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 27/676 [01:31<39:18,  3.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7141518592834473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 28/676 [01:34<38:18,  3.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6866910457611084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 29/676 [01:37<36:44,  3.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5475345849990845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 30/676 [01:41<35:52,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7018354535102844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▍         | 31/676 [01:44<35:22,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6262814998626709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▍         | 32/676 [01:47<34:55,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9409071803092957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▍         | 33/676 [01:50<34:28,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.675823450088501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 34/676 [01:53<34:44,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9710526466369629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 35/676 [01:57<36:01,  3.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.1147669553756714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 36/676 [02:00<35:17,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6624173521995544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 37/676 [02:03<34:40,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7654821276664734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 38/676 [02:06<34:08,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6775399446487427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 39/676 [02:10<34:06,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6844774484634399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 40/676 [02:13<34:32,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6136763095855713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 41/676 [02:17<35:55,  3.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7674607038497925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 42/676 [02:20<36:38,  3.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7134370803833008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▋         | 43/676 [02:24<35:46,  3.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5483995676040649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 44/676 [02:27<35:15,  3.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7849379777908325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 45/676 [02:30<34:28,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9420425891876221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 46/676 [02:33<33:52,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.537962794303894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 47/676 [02:36<33:40,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5341112017631531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 48/676 [02:40<34:19,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8123977184295654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 49/676 [02:43<33:50,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.576377272605896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 50/676 [02:46<33:23,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4977213740348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 51/676 [02:49<33:22,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5923901796340942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 52/676 [02:53<34:49,  3.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6412863731384277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 53/676 [02:56<33:59,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5009365677833557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 54/676 [02:59<33:27,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4732334315776825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 55/676 [03:02<33:17,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5154371857643127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 56/676 [03:06<33:28,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8227115273475647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 57/676 [03:09<32:59,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.1538796424865723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▊         | 58/676 [03:12<32:40,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4136389493942261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▊         | 59/676 [03:15<32:42,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7976950407028198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 60/676 [03:18<32:46,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5559716820716858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 61/676 [03:21<32:22,  3.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.46104997396469116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 62/676 [03:24<32:05,  3.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.527119517326355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 63/676 [03:27<32:00,  3.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9002867937088013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 64/676 [03:31<32:26,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9216875433921814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|▉         | 65/676 [03:34<33:44,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6916497349739075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|▉         | 66/676 [03:38<33:10,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5813779830932617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|▉         | 67/676 [03:41<33:11,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.0507724285125732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 68/676 [03:44<33:28,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9396696090698242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 69/676 [03:47<32:46,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8343261480331421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 70/676 [03:52<35:38,  3.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7331469655036926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 71/676 [03:55<34:48,  3.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7208120822906494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 72/676 [03:58<34:05,  3.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8356800675392151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 73/676 [04:01<33:15,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7851177453994751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 74/676 [04:04<32:45,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7269080877304077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 75/676 [04:08<32:43,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5304281115531921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 76/676 [04:11<32:27,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6710720062255859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█▏        | 77/676 [04:14<32:05,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5554304122924805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 78/676 [04:17<32:39,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5538932085037231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 79/676 [04:21<32:31,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.45315560698509216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 80/676 [04:24<32:37,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7582571506500244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 81/676 [04:27<32:38,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6163473129272461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 82/676 [04:31<33:21,  3.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6715431213378906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 83/676 [04:34<33:26,  3.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.45723703503608704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 84/676 [04:38<33:38,  3.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.0645356178283691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 85/676 [04:41<33:24,  3.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9654979705810547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 86/676 [04:44<32:38,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.903494656085968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 87/676 [04:47<32:22,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7808824181556702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 88/676 [04:51<33:25,  3.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8659390211105347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 89/676 [04:54<32:36,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.39912036061286926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 90/676 [04:57<32:09,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.745954155921936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 91/676 [05:01<32:12,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6639598608016968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▎        | 92/676 [05:04<32:38,  3.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7231086492538452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 93/676 [05:07<32:02,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5124417543411255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 94/676 [05:11<32:57,  3.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7370483875274658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 95/676 [05:14<32:32,  3.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7625930905342102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 96/676 [05:18<32:03,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4492650628089905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 97/676 [05:21<32:20,  3.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5204963088035583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 98/676 [05:24<31:48,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6671428680419922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▍        | 99/676 [05:28<32:03,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7280892133712769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▍        | 100/676 [05:31<31:47,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7198588848114014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▍        | 101/676 [05:34<31:30,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.25133758783340454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 102/676 [05:37<31:12,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.666199803352356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 103/676 [05:41<31:11,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5338195562362671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 104/676 [05:44<31:15,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8388785123825073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 105/676 [05:47<30:44,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6792884469032288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 106/676 [05:50<30:32,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4143936038017273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 107/676 [05:53<30:45,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7975520491600037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 108/676 [05:57<30:38,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6945696473121643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 109/676 [06:00<30:15,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.0550129413604736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▋        | 110/676 [06:03<31:33,  3.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9927585124969482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▋        | 111/676 [06:07<31:14,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6428179740905762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 112/676 [06:10<30:54,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5822471380233765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 113/676 [06:13<30:59,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6664870977401733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 114/676 [06:16<30:23,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5818893909454346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 115/676 [06:20<30:15,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.733759880065918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 116/676 [06:23<30:14,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7494578957557678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 117/676 [06:27<31:34,  3.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5764373540878296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 118/676 [06:30<30:50,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6245120167732239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 119/676 [06:33<30:40,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7555781602859497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 120/676 [06:37<31:40,  3.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6377685070037842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 121/676 [06:40<30:50,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8069754838943481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 122/676 [06:43<30:36,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7904325127601624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 123/676 [06:46<30:38,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8005802631378174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 124/676 [06:50<31:10,  3.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6718950271606445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 125/676 [06:53<31:13,  3.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6916384696960449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▊        | 126/676 [06:57<30:38,  3.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9458744525909424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 127/676 [07:00<30:35,  3.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6359907388687134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 128/676 [07:03<30:11,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8609414100646973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 129/676 [07:06<29:54,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8952003717422485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 130/676 [07:10<29:38,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5078310370445251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 131/676 [07:13<29:33,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7184535264968872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|█▉        | 132/676 [07:16<29:35,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4244503080844879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|█▉        | 133/676 [07:19<29:20,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6398607492446899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|█▉        | 134/676 [07:24<32:19,  3.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5958414077758789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|█▉        | 135/676 [07:27<31:54,  3.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8327031135559082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 136/676 [07:31<31:36,  3.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7271957397460938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 137/676 [07:34<31:22,  3.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6062268018722534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 138/676 [07:37<30:29,  3.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.370722770690918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██        | 139/676 [07:41<30:35,  3.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6615316867828369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██        | 140/676 [07:44<29:38,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.693362832069397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██        | 141/676 [07:47<29:09,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.477372407913208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██        | 142/676 [07:50<28:45,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8457013964653015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██        | 143/676 [07:54<30:06,  3.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9387848377227783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██▏       | 144/676 [07:57<30:41,  3.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7356665134429932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██▏       | 145/676 [08:01<30:01,  3.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.709125816822052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 146/676 [08:04<29:17,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.592736005783081\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 147/676 [08:07<29:36,  3.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.563202977180481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 148/676 [08:11<30:08,  3.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.54603111743927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 149/676 [08:14<29:21,  3.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8537499904632568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 150/676 [08:18<31:01,  3.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7639945149421692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 151/676 [08:21<30:38,  3.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7546611428260803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 152/676 [08:25<29:33,  3.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5166447162628174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 153/676 [08:28<28:49,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4734668433666229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 154/676 [08:31<28:18,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.575054943561554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 155/676 [08:34<28:37,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6406505703926086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 156/676 [08:37<28:12,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.715794563293457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 157/676 [08:40<27:54,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.727219820022583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 158/676 [08:44<27:38,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5068747401237488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▎       | 159/676 [08:47<27:55,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4919329881668091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▎       | 160/676 [08:50<27:54,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5304991602897644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 161/676 [08:53<27:31,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.0224586725234985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 162/676 [08:57<27:22,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6422364711761475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 163/676 [09:00<27:24,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.080517292022705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 164/676 [09:03<27:26,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5723545551300049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 165/676 [09:06<27:08,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8230593204498291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▍       | 166/676 [09:09<26:56,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7183641195297241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▍       | 167/676 [09:12<27:08,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7895597219467163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▍       | 168/676 [09:16<27:27,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6335232257843018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 169/676 [09:19<27:10,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5486811399459839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 170/676 [09:22<26:54,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5964247584342957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 171/676 [09:25<26:55,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5935210585594177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 172/676 [09:29<27:03,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6436245441436768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 173/676 [09:32<27:31,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8134611248970032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 174/676 [09:35<27:17,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8027199506759644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 175/676 [09:39<27:22,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5210719704627991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 176/676 [09:42<27:24,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6765369176864624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 177/676 [09:45<27:04,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.2888641953468323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▋       | 178/676 [09:48<26:47,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7695877552032471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▋       | 179/676 [09:51<26:42,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.66920006275177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 180/676 [09:55<26:44,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9518651366233826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 181/676 [09:58<26:31,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6885796189308167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 182/676 [10:01<26:19,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5881245732307434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 183/676 [10:05<27:45,  3.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9754796028137207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 184/676 [10:08<27:35,  3.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8243264555931091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 185/676 [10:11<27:03,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7556535005569458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 186/676 [10:15<27:13,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.3193906843662262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 187/676 [10:18<27:07,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5822744965553284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 188/676 [10:21<26:57,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.3269560635089874\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 189/676 [10:24<26:27,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9197636842727661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 190/676 [10:28<26:18,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6259397864341736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 191/676 [10:31<26:15,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.48327118158340454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 192/676 [10:34<26:12,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.41113942861557007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▊       | 193/676 [10:37<26:16,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7006675601005554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▊       | 194/676 [10:41<25:55,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8581743240356445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▉       | 195/676 [10:44<25:52,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5395345687866211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▉       | 196/676 [10:47<26:08,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7990266680717468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▉       | 197/676 [10:51<26:21,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.37090638279914856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▉       | 198/676 [10:54<26:06,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5064118504524231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▉       | 199/676 [10:57<25:57,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.961949348449707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|██▉       | 200/676 [11:00<25:54,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.805719792842865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|██▉       | 201/676 [11:03<25:34,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6941254138946533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|██▉       | 202/676 [11:07<25:18,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7196815013885498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 203/676 [11:10<26:30,  3.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8410063982009888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 204/676 [11:14<26:08,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5496346950531006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 205/676 [11:17<25:55,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8900119066238403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 206/676 [11:21<28:43,  3.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.1089471578598022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███       | 207/676 [11:26<29:52,  3.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8474140763282776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███       | 208/676 [11:29<28:08,  3.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5279737710952759\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███       | 209/676 [11:32<26:58,  3.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5325241684913635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███       | 210/676 [11:35<26:11,  3.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.828503429889679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███       | 211/676 [11:38<25:49,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.2115721702575684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███▏      | 212/676 [11:41<25:23,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5809370279312134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 213/676 [11:45<25:09,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.49336567521095276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 214/676 [11:48<24:50,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5944632291793823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 215/676 [11:51<25:07,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8498908281326294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 216/676 [11:54<24:47,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6711554527282715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 217/676 [11:57<24:28,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.785699725151062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 218/676 [12:01<24:23,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8094285726547241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 219/676 [12:04<24:21,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.629648745059967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 220/676 [12:07<24:30,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8635939359664917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 221/676 [12:10<24:16,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8701475858688354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 222/676 [12:13<24:06,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6761561036109924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 223/676 [12:17<24:10,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4512518644332886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 224/676 [12:20<23:59,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.666700005531311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 225/676 [12:23<23:49,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5871226191520691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 226/676 [12:26<23:51,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5447672605514526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▎      | 227/676 [12:29<23:57,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8101425170898438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▎      | 228/676 [12:33<23:59,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.791165828704834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 229/676 [12:36<23:49,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6458463668823242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 230/676 [12:39<23:37,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9784712791442871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 231/676 [12:42<24:02,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6536601185798645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 232/676 [12:45<23:43,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9636685848236084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 233/676 [12:49<23:56,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9660558700561523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▍      | 234/676 [12:52<23:36,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6842902302742004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▍      | 235/676 [12:55<23:43,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6581462025642395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▍      | 236/676 [12:58<23:33,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.023209571838379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 237/676 [13:01<23:26,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.3830627202987671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 238/676 [13:05<23:21,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.0760400295257568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 239/676 [13:08<23:27,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5685545802116394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 240/676 [13:11<23:22,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9816602468490601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 241/676 [13:14<23:38,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5772103071212769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 242/676 [13:18<23:52,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9817848205566406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 243/676 [13:21<23:44,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8069127798080444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 244/676 [13:24<23:24,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7647542953491211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 245/676 [13:27<23:10,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6722656488418579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▋      | 246/676 [13:31<23:06,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.48170459270477295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 247/676 [13:34<23:06,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6107970476150513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 248/676 [13:38<23:52,  3.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6651885509490967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 249/676 [13:41<23:46,  3.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5601642727851868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 250/676 [13:44<23:45,  3.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8034741282463074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 251/676 [13:48<23:52,  3.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7937731742858887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 252/676 [13:51<23:31,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6463197469711304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 253/676 [13:54<23:08,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7153314352035522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 254/676 [13:57<23:03,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7289447784423828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 255/676 [14:01<22:55,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7486796975135803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 256/676 [14:05<25:35,  3.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7857398390769958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 257/676 [14:08<24:49,  3.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5427896976470947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 258/676 [14:12<24:24,  3.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6673153638839722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 259/676 [14:15<23:37,  3.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.35886651277542114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 260/676 [14:18<23:02,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.743652880191803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▊      | 261/676 [14:21<22:30,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6637038588523865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▉      | 262/676 [14:24<22:28,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.49709492921829224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▉      | 263/676 [14:28<22:19,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5270225405693054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▉      | 264/676 [14:31<22:06,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.1491179466247559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▉      | 265/676 [14:34<21:54,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6082150936126709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▉      | 266/676 [14:37<22:02,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.38295629620552063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▉      | 267/676 [14:41<22:08,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6657893061637878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|███▉      | 268/676 [14:44<22:14,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5803855657577515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|███▉      | 269/676 [14:47<21:56,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6642438173294067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|███▉      | 270/676 [14:50<21:57,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6836944222450256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 271/676 [14:54<21:50,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7080634236335754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 272/676 [14:57<21:30,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7112706303596497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 273/676 [15:00<21:22,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7092212438583374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41%|████      | 274/676 [15:03<21:26,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9661734700202942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41%|████      | 275/676 [15:06<21:29,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5646941065788269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41%|████      | 276/676 [15:09<21:18,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6162635684013367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41%|████      | 277/676 [15:13<21:06,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6015758514404297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41%|████      | 278/676 [15:16<21:09,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.1431564092636108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41%|████▏     | 279/676 [15:19<21:09,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4824528396129608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41%|████▏     | 280/676 [15:22<20:59,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5195940732955933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 281/676 [15:25<21:05,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5108505487442017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 282/676 [15:29<20:52,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6888390779495239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 283/676 [15:32<20:55,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6356527805328369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 284/676 [15:35<21:23,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7188733220100403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 285/676 [15:38<21:12,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.586777925491333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 286/676 [15:42<21:08,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5948769450187683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 287/676 [15:45<21:14,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4893531799316406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 288/676 [15:48<20:54,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6130202412605286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 289/676 [15:51<20:58,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7829059362411499\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 290/676 [15:55<20:47,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5391495227813721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 291/676 [15:58<20:38,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.603665828704834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 292/676 [16:01<20:29,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5692734718322754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 293/676 [16:04<20:14,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5745841264724731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 294/676 [16:07<20:11,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.805087149143219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▎     | 295/676 [16:10<20:13,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5845986008644104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 296/676 [16:14<20:03,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8113901615142822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 297/676 [16:17<19:58,  3.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6827013492584229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 298/676 [16:21<22:27,  3.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6617690324783325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 299/676 [16:25<22:11,  3.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7313810586929321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 300/676 [16:28<21:22,  3.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7112332582473755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▍     | 301/676 [16:31<20:46,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9773105978965759\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▍     | 302/676 [16:34<20:48,  3.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.780792772769928\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▍     | 303/676 [16:38<20:34,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7781139016151428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▍     | 304/676 [16:41<20:08,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5656986832618713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 305/676 [16:44<20:11,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9492988586425781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 306/676 [16:47<20:04,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5439683198928833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 307/676 [16:51<20:08,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7583131194114685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 308/676 [16:54<19:50,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4451656639575958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 309/676 [16:57<19:35,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5533475875854492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 310/676 [17:00<19:35,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5659758448600769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 311/676 [17:03<19:35,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5345439314842224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 312/676 [17:06<19:23,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.2062697410583496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▋     | 313/676 [17:10<19:12,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8684015274047852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▋     | 314/676 [17:13<19:11,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7221605181694031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 315/676 [17:16<19:12,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.2218255996704102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 316/676 [17:19<19:10,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8753037452697754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 317/676 [17:22<19:05,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5635022521018982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 318/676 [17:26<19:10,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5073308944702148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 319/676 [17:29<19:44,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.702009916305542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 320/676 [17:32<19:23,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.683154284954071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 321/676 [17:35<19:05,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.1434059143066406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 322/676 [17:39<18:53,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7250282764434814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 323/676 [17:42<19:08,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6572573184967041\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 324/676 [17:45<18:57,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7135247588157654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 325/676 [17:48<18:44,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5854276418685913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 326/676 [17:51<18:39,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6185660362243652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 327/676 [17:55<18:51,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6665500402450562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 49%|████▊     | 328/676 [17:58<18:50,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6433403491973877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 49%|████▊     | 329/676 [18:01<18:43,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.36046186089515686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 49%|████▉     | 330/676 [18:04<18:30,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5827558040618896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 49%|████▉     | 331/676 [18:08<18:29,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6712223887443542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 49%|████▉     | 332/676 [18:11<18:59,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6330533027648926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 49%|████▉     | 333/676 [18:14<18:39,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7397744655609131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 49%|████▉     | 334/676 [18:18<18:52,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4977271556854248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|████▉     | 335/676 [18:22<19:38,  3.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5652780532836914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|████▉     | 336/676 [18:25<19:12,  3.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7097734212875366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|████▉     | 337/676 [18:28<18:48,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5357714295387268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 338/676 [18:31<18:25,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4595043957233429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 339/676 [18:34<18:22,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6241418719291687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 340/676 [18:38<18:14,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6574689149856567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 341/676 [18:41<17:57,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6528565883636475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 51%|█████     | 342/676 [18:44<18:16,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8658714294433594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 51%|█████     | 343/676 [18:47<18:11,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7899720668792725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 51%|█████     | 344/676 [18:51<17:58,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7234131693840027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 51%|█████     | 345/676 [18:54<17:44,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6022576093673706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 51%|█████     | 346/676 [18:57<17:31,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.568761944770813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 51%|█████▏    | 347/676 [19:00<17:29,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.758797287940979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 51%|█████▏    | 348/676 [19:03<17:29,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8152647018432617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 349/676 [19:07<17:47,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6982305645942688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 350/676 [19:10<17:34,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.817211389541626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 351/676 [19:13<17:30,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.1206743717193604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 352/676 [19:16<17:30,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.521506130695343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 353/676 [19:20<17:17,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5409579873085022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 354/676 [19:23<17:50,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6027657985687256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 355/676 [19:26<17:36,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6477530002593994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 356/676 [19:30<17:28,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.661880612373352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 357/676 [19:33<17:15,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.595268726348877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 358/676 [19:36<17:02,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8709372878074646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 359/676 [19:39<17:02,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9359365701675415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 360/676 [19:42<16:59,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5314412117004395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 361/676 [19:46<16:54,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5315871238708496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▎    | 362/676 [19:49<16:51,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5415900945663452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▎    | 363/676 [19:52<16:49,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6375704407691956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 364/676 [19:55<16:46,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7302007079124451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 365/676 [19:58<16:40,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7205712795257568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 366/676 [20:02<16:28,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9883174896240234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 367/676 [20:05<16:29,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.42220497131347656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 368/676 [20:08<16:27,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7510901093482971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▍    | 369/676 [20:11<16:15,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7562538385391235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▍    | 370/676 [20:14<16:08,  3.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9960684776306152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▍    | 371/676 [20:17<16:03,  3.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6335570812225342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 372/676 [20:21<16:08,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.49768826365470886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 373/676 [20:24<16:00,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8899856209754944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 374/676 [20:27<16:01,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.1520817279815674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 375/676 [20:30<15:51,  3.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9292047023773193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 376/676 [20:33<16:07,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5509688854217529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 377/676 [20:37<15:57,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7247122526168823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 378/676 [20:40<15:49,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7912370562553406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 379/676 [20:43<15:43,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5721985101699829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 380/676 [20:46<16:06,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5525094270706177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▋    | 381/676 [20:50<16:22,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.539023756980896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 382/676 [20:53<16:00,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7223725914955139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 383/676 [20:56<15:49,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.822099506855011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 384/676 [20:59<15:45,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6102612018585205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 385/676 [21:03<15:47,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6105802059173584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 386/676 [21:06<15:36,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.049980878829956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 387/676 [21:09<15:26,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8019933700561523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 388/676 [21:12<15:35,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5495229363441467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 389/676 [21:16<15:32,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6104627847671509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 390/676 [21:19<15:23,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.44819405674934387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 391/676 [21:22<15:11,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7778152227401733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 392/676 [21:25<15:10,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5367244482040405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 393/676 [21:28<15:08,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6109358072280884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 394/676 [21:32<14:58,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5495606660842896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 395/676 [21:35<14:54,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.49719107151031494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59%|█████▊    | 396/676 [21:38<14:53,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7951854467391968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59%|█████▊    | 397/676 [21:41<14:58,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.38633137941360474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59%|█████▉    | 398/676 [21:45<15:59,  3.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.1232850551605225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59%|█████▉    | 399/676 [21:48<15:31,  3.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.0594531297683716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59%|█████▉    | 400/676 [21:52<15:17,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7818030714988708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59%|█████▉    | 401/676 [21:55<15:08,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6123433709144592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59%|█████▉    | 402/676 [21:58<15:04,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6649414896965027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|█████▉    | 403/676 [22:01<14:47,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6402772665023804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|█████▉    | 404/676 [22:04<14:43,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5922148823738098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|█████▉    | 405/676 [22:08<14:44,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9432653188705444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 406/676 [22:11<14:29,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6968739032745361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 407/676 [22:14<14:22,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5764250159263611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 408/676 [22:17<14:20,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7710227966308594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████    | 409/676 [22:21<14:20,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6124720573425293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████    | 410/676 [22:24<14:13,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7246464490890503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████    | 411/676 [22:27<14:05,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4542214870452881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████    | 412/676 [22:30<14:11,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.47420212626457214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████    | 413/676 [22:33<14:08,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.395501971244812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████    | 414/676 [22:37<13:58,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7366600036621094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████▏   | 415/676 [22:40<13:53,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5495401620864868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 416/676 [22:43<14:12,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8675979375839233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 417/676 [22:46<14:08,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7298890352249146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 418/676 [22:50<13:59,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6741865277290344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 419/676 [22:53<13:57,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.560858964920044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 420/676 [22:57<14:27,  3.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.34907934069633484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 421/676 [23:00<14:11,  3.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7727484107017517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 422/676 [23:03<13:53,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8706833720207214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 423/676 [23:06<13:43,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9622757434844971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 424/676 [23:09<13:37,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.2806728482246399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 425/676 [23:13<13:39,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.3060259222984314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 426/676 [23:16<13:33,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7134342193603516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 427/676 [23:19<13:23,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.0576412677764893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 428/676 [23:22<13:19,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.469957560300827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 429/676 [23:26<13:15,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5526587963104248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▎   | 430/676 [23:29<13:07,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4029805660247803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 431/676 [23:32<13:01,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.41535472869873047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 432/676 [23:35<12:54,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.3414397239685059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 433/676 [23:38<12:55,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7856233716011047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 434/676 [23:41<12:51,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6631503701210022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 435/676 [23:45<12:48,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7700667381286621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 436/676 [23:48<12:47,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6409115791320801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▍   | 437/676 [23:51<12:51,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8299891352653503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▍   | 438/676 [23:54<12:43,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5782668590545654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▍   | 439/676 [23:57<12:37,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.761485755443573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 440/676 [24:01<12:31,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.610352635383606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 441/676 [24:04<13:02,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6075193285942078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 442/676 [24:08<12:59,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5492538213729858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▌   | 443/676 [24:11<12:46,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5782161951065063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▌   | 444/676 [24:14<12:32,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.39058518409729004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▌   | 445/676 [24:17<12:37,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7862133979797363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▌   | 446/676 [24:21<12:34,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.499052494764328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▌   | 447/676 [24:24<12:20,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.823472797870636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▋   | 448/676 [24:27<12:08,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9141806364059448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▋   | 449/676 [24:30<12:10,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5418686866760254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 450/676 [24:33<12:09,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7301499247550964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 451/676 [24:37<12:04,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.46848398447036743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 452/676 [24:40<11:56,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6123127341270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 453/676 [24:43<11:56,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7458817958831787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 454/676 [24:46<11:56,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.878045916557312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 455/676 [24:49<11:49,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6152119040489197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 456/676 [24:53<11:45,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6329982280731201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 457/676 [24:56<11:46,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5854769349098206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 458/676 [24:59<11:43,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.2542705535888672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 459/676 [25:02<11:39,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4560001492500305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 460/676 [25:06<11:38,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6976777911186218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 461/676 [25:09<11:38,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4695543646812439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 462/676 [25:12<11:33,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.730484127998352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 463/676 [25:15<11:23,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5548533201217651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|██████▊   | 464/676 [25:18<11:25,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7410438656806946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|██████▉   | 465/676 [25:22<11:53,  3.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6537857055664062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|██████▉   | 466/676 [25:26<11:45,  3.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.49199849367141724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|██████▉   | 467/676 [25:29<11:31,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5461418032646179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|██████▉   | 468/676 [25:32<11:19,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.2815921902656555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|██████▉   | 469/676 [25:35<11:15,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.3526378273963928\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|██████▉   | 470/676 [25:38<11:09,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8216768503189087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|██████▉   | 471/676 [25:42<11:03,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.0296225547790527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|██████▉   | 472/676 [25:45<10:57,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5614956021308899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|██████▉   | 473/676 [25:48<10:54,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8396130204200745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 474/676 [25:52<11:25,  3.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.590937614440918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 475/676 [25:55<11:14,  3.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5776824951171875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 476/676 [25:58<10:58,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.809069812297821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████   | 477/676 [26:01<10:54,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7352986335754395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████   | 478/676 [26:05<10:45,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5151954889297485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████   | 479/676 [26:08<10:41,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5254912376403809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████   | 480/676 [26:11<10:38,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.41571110486984253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████   | 481/676 [26:15<11:31,  3.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.0407150983810425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████▏  | 482/676 [26:19<11:09,  3.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5237616300582886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████▏  | 483/676 [26:22<10:51,  3.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5584636330604553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 484/676 [26:25<10:36,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8930821418762207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 485/676 [26:28<10:30,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5328125953674316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 486/676 [26:31<10:22,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8264966607093811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 487/676 [26:35<10:10,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5908832550048828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 488/676 [26:38<10:02,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.064655065536499\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 489/676 [26:41<10:11,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4912477135658264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 490/676 [26:44<10:06,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.1312382221221924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 491/676 [26:48<09:56,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7622197866439819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 492/676 [26:51<09:50,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8683751821517944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 493/676 [26:54<09:56,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9105972051620483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 494/676 [26:57<09:51,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6768133044242859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 495/676 [27:01<10:11,  3.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6385577917098999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 496/676 [27:04<09:55,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5047575235366821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▎  | 497/676 [27:08<09:56,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5850074291229248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▎  | 498/676 [27:11<09:47,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8986237645149231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 499/676 [27:15<10:12,  3.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5979592800140381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 500/676 [27:18<10:16,  3.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6842072606086731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 501/676 [27:22<10:07,  3.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7662805318832397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 502/676 [27:25<09:58,  3.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6020623445510864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 503/676 [27:28<09:40,  3.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6988935470581055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▍  | 504/676 [27:31<09:27,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8142832517623901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▍  | 505/676 [27:35<09:28,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8306575417518616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▍  | 506/676 [27:38<09:23,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.46688276529312134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 507/676 [27:41<09:12,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6890788674354553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 508/676 [27:44<09:01,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7436381578445435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 509/676 [27:48<09:00,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9844443798065186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 510/676 [27:51<09:00,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6184864044189453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 511/676 [27:54<08:51,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7474098205566406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 512/676 [27:57<08:44,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4471205472946167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 513/676 [28:00<08:45,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7651543021202087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 514/676 [28:04<08:44,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.3571726977825165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 515/676 [28:07<08:36,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5102311968803406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▋  | 516/676 [28:10<08:30,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.49824488162994385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▋  | 517/676 [28:13<08:37,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7144291400909424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|███████▋  | 518/676 [28:17<08:33,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.3457079231739044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|███████▋  | 519/676 [28:20<08:25,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5830692052841187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|███████▋  | 520/676 [28:23<08:20,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.38974976539611816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|███████▋  | 521/676 [28:26<08:19,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7450505495071411\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|███████▋  | 522/676 [28:29<08:17,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9632176160812378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|███████▋  | 523/676 [28:33<08:13,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6028162837028503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 524/676 [28:36<08:10,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7399828433990479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 525/676 [28:40<08:28,  3.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.1008638143539429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 526/676 [28:43<08:18,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7696208357810974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 527/676 [28:46<08:06,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8384936451911926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 528/676 [28:50<08:41,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6853569746017456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 529/676 [28:53<08:26,  3.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7731852531433105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 530/676 [28:57<08:33,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.48233965039253235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|███████▊  | 531/676 [29:00<08:14,  3.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6619136333465576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|███████▊  | 532/676 [29:03<08:00,  3.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7115031480789185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|███████▉  | 533/676 [29:07<07:54,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8004059195518494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|███████▉  | 534/676 [29:10<07:48,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.47988009452819824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|███████▉  | 535/676 [29:13<07:38,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6252635717391968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|███████▉  | 536/676 [29:16<07:30,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5023529529571533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|███████▉  | 537/676 [29:20<07:39,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8546789884567261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|███████▉  | 538/676 [29:23<07:33,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.291061282157898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|███████▉  | 539/676 [29:26<07:25,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.45633959770202637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|███████▉  | 540/676 [29:29<07:17,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6335414052009583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 541/676 [29:32<07:15,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.41297584772109985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 542/676 [29:36<07:17,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.875291109085083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 543/676 [29:39<07:08,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.415701150894165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 544/676 [29:42<07:03,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5867511034011841\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%|████████  | 545/676 [29:46<07:27,  3.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7860386371612549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%|████████  | 546/676 [29:50<07:40,  3.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6262681484222412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%|████████  | 547/676 [29:53<07:21,  3.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5275906324386597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%|████████  | 548/676 [29:56<07:11,  3.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6558635830879211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%|████████  | 549/676 [30:00<07:06,  3.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5213282108306885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%|████████▏ | 550/676 [30:03<07:03,  3.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.0067660808563232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 551/676 [30:06<06:55,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6504162549972534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 552/676 [30:10<06:54,  3.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7914817333221436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 553/676 [30:13<06:49,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7867170572280884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 554/676 [30:16<06:48,  3.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5873899459838867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 555/676 [30:19<06:41,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5611362457275391\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 556/676 [30:23<06:35,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4982095956802368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 557/676 [30:26<06:30,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.0799767971038818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 558/676 [30:29<06:24,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6449980735778809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 559/676 [30:32<06:20,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6257758140563965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 560/676 [30:36<06:17,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5087238550186157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 561/676 [30:39<06:14,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7936984896659851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 562/676 [30:42<06:12,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8439157009124756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 563/676 [30:46<06:14,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9504795074462891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 564/676 [30:49<06:07,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.44475066661834717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▎ | 565/676 [30:52<06:04,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8003109693527222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▎ | 566/676 [30:56<06:05,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6644214987754822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 567/676 [30:59<05:55,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6360641717910767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 568/676 [31:02<05:49,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.47656646370887756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 569/676 [31:06<06:10,  3.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5366379022598267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 570/676 [31:09<05:58,  3.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7775435447692871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 571/676 [31:12<05:47,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5303362607955933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▍ | 572/676 [31:16<05:55,  3.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.1001309156417847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▍ | 573/676 [31:19<05:47,  3.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8146092295646667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▍ | 574/676 [31:22<05:40,  3.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6927371025085449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 575/676 [31:25<05:31,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6806495189666748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 576/676 [31:29<05:25,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6922957301139832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 577/676 [31:32<05:23,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.828149676322937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 578/676 [31:35<05:18,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4513775706291199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 579/676 [31:38<05:13,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7557003498077393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 580/676 [31:42<05:11,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5958640575408936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 581/676 [31:45<05:19,  3.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9065006971359253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 582/676 [31:49<05:14,  3.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.827884316444397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 583/676 [31:52<05:07,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6464758515357971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▋ | 584/676 [31:55<05:00,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4662104547023773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 585/676 [31:58<04:57,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9808705449104309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 586/676 [32:01<04:52,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8462226390838623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 587/676 [32:05<04:47,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7513431310653687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 588/676 [32:08<04:51,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6674550771713257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 589/676 [32:11<04:49,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5534641742706299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 590/676 [32:15<04:55,  3.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5908520221710205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 591/676 [32:18<04:44,  3.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7163509130477905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 592/676 [32:22<04:39,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6489999294281006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 593/676 [32:25<04:34,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7201159000396729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 594/676 [32:28<04:30,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5530269145965576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 595/676 [32:32<04:35,  3.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6639225482940674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 596/676 [32:35<04:26,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.45373138785362244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 597/676 [32:38<04:21,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.44239938259124756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 598/676 [32:42<04:18,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4812870919704437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▊ | 599/676 [32:45<04:11,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.917564868927002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▉ | 600/676 [32:48<04:05,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7165782451629639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▉ | 601/676 [32:51<04:03,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.40113693475723267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▉ | 602/676 [32:55<04:07,  3.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.40045225620269775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▉ | 603/676 [32:58<04:04,  3.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.3545255661010742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▉ | 604/676 [33:01<03:57,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8710918426513672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▉ | 605/676 [33:04<03:52,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6267812848091125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|████████▉ | 606/676 [33:08<04:00,  3.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8890259861946106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|████████▉ | 607/676 [33:12<03:53,  3.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6408907771110535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|████████▉ | 608/676 [33:15<03:45,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.2503550052642822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 609/676 [33:18<03:40,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8640071153640747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 610/676 [33:21<03:37,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8999258279800415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 611/676 [33:24<03:32,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.3057233691215515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████ | 612/676 [33:28<03:28,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7354967594146729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████ | 613/676 [33:31<03:25,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5532431602478027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████ | 614/676 [33:34<03:24,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.0301275253295898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████ | 615/676 [33:37<03:18,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.621406078338623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████ | 616/676 [33:41<03:17,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6419863700866699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████▏| 617/676 [33:45<03:22,  3.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4070972204208374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████▏| 618/676 [33:48<03:13,  3.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.036766767501831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 619/676 [33:51<03:07,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6564257740974426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 620/676 [33:54<03:04,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8884756565093994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 621/676 [33:58<03:01,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5135354399681091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 622/676 [34:01<02:56,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6256543397903442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 623/676 [34:04<02:51,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.1379759311676025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 624/676 [34:07<02:50,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8095348477363586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 625/676 [34:11<02:49,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4574527442455292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 626/676 [34:14<02:48,  3.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.018855333328247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 627/676 [34:17<02:42,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6514904499053955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 628/676 [34:21<02:36,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6689897179603577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 629/676 [34:24<02:40,  3.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.594344973564148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 630/676 [34:27<02:34,  3.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5355268716812134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 631/676 [34:31<02:28,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9625979661941528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 632/676 [34:34<02:23,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4647301435470581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▎| 633/676 [34:37<02:20,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.443724662065506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 634/676 [34:40<02:16,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9283098578453064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 635/676 [34:43<02:12,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8476205468177795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 636/676 [34:47<02:09,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5709381103515625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 637/676 [34:50<02:06,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5437240600585938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 638/676 [34:54<02:12,  3.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5987629890441895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▍| 639/676 [34:57<02:05,  3.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.44678303599357605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▍| 640/676 [35:00<01:59,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6188433170318604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▍| 641/676 [35:04<01:55,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4786480665206909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▍| 642/676 [35:07<01:51,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6268407702445984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 643/676 [35:10<01:47,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7912640571594238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 644/676 [35:13<01:43,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5567786693572998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 645/676 [35:16<01:40,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8985390067100525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 646/676 [35:20<01:37,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4855668842792511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 647/676 [35:23<01:33,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4576013684272766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 648/676 [35:26<01:29,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5651075839996338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 649/676 [35:29<01:26,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.748512864112854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 650/676 [35:32<01:23,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.978149950504303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▋| 651/676 [35:36<01:20,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6581813097000122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▋| 652/676 [35:39<01:16,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.0472619533538818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 653/676 [35:42<01:13,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.3386181592941284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 654/676 [35:45<01:10,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.46120524406433105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 655/676 [35:48<01:07,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7130971550941467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 656/676 [35:52<01:03,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7144538760185242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 657/676 [35:55<01:00,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.730616569519043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 658/676 [35:58<00:57,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7447289228439331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 659/676 [36:01<00:54,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7846441268920898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 660/676 [36:04<00:50,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.43266454339027405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 661/676 [36:07<00:47,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5327804088592529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 662/676 [36:11<00:44,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6741968393325806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 663/676 [36:14<00:41,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4404798448085785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 664/676 [36:17<00:38,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4789547920227051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 665/676 [36:20<00:35,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8699091672897339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 99%|█████████▊| 666/676 [36:24<00:32,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5150815844535828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 99%|█████████▊| 667/676 [36:27<00:29,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6404119729995728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 99%|█████████▉| 668/676 [36:30<00:26,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7347759008407593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 99%|█████████▉| 669/676 [36:34<00:22,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6273079514503479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 99%|█████████▉| 670/676 [36:37<00:19,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6795949339866638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 99%|█████████▉| 671/676 [36:40<00:16,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.1361898183822632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 99%|█████████▉| 672/676 [36:44<00:13,  3.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8772064447402954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|█████████▉| 673/676 [36:47<00:09,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6659822463989258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|█████████▉| 674/676 [36:50<00:06,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7841736674308777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|█████████▉| 675/676 [36:53<00:03,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5092819929122925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([5, 1, 2048])\n",
            "Pooled output shape: torch.Size([5, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 676/676 [36:55<00:00,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.3727576434612274\n",
            "total test loss: 465.9285236299038\n",
            "Epoch 1/2, Average Loss: 0.6892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights saved for epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/89 [00:00<?, ?it/s]Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 1/89 [00:02<03:34,  2.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.167236566543579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 2/89 [00:04<03:27,  2.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1956325769424438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 3/89 [00:06<03:17,  2.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.9281535148620605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 4/89 [00:09<03:21,  2.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.2994139194488525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 5/89 [00:11<03:14,  2.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.0567599534988403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 6/89 [00:14<03:13,  2.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.9771162867546082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 7/89 [00:16<03:16,  2.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.7925744652748108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 8/89 [00:18<03:13,  2.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.040109395980835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 9/89 [00:21<03:06,  2.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1195180416107178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 10/89 [00:23<03:01,  2.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1528453826904297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 11/89 [00:25<02:57,  2.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.3099679946899414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 12/89 [00:27<02:56,  2.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.183112621307373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▍        | 13/89 [00:30<02:56,  2.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.0339505672454834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 14/89 [00:32<02:52,  2.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1611632108688354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 15/89 [00:34<02:48,  2.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1622796058654785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 16/89 [00:37<02:45,  2.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.9760729074478149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 17/89 [00:39<02:41,  2.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.9474636912345886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 18/89 [00:41<02:39,  2.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.0973422527313232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██▏       | 19/89 [00:43<02:41,  2.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.216623067855835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 20/89 [00:46<02:37,  2.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.2556079626083374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▎       | 21/89 [00:48<02:33,  2.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.249383807182312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▍       | 22/89 [00:50<02:31,  2.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.4156262874603271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 23/89 [00:52<02:27,  2.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.0508267879486084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 24/89 [00:55<02:26,  2.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1503474712371826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 25/89 [00:57<02:25,  2.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1888768672943115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▉       | 26/89 [00:59<02:22,  2.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.128903865814209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 27/89 [01:01<02:19,  2.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.8798173666000366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███▏      | 28/89 [01:04<02:16,  2.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.4005484580993652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 29/89 [01:06<02:14,  2.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1222922801971436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▎      | 30/89 [01:08<02:14,  2.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.174035906791687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▍      | 31/89 [01:11<02:14,  2.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.887371301651001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 32/89 [01:13<02:09,  2.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1675364971160889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 33/89 [01:15<02:14,  2.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.8977358341217041\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 34/89 [01:18<02:09,  2.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.8134632110595703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▉      | 35/89 [01:20<02:07,  2.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.116025686264038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 36/89 [01:23<02:06,  2.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.19960355758667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 37/89 [01:25<02:09,  2.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.9708044528961182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 38/89 [01:28<02:14,  2.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1763601303100586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 39/89 [01:31<02:06,  2.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.145649790763855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▍     | 40/89 [01:33<02:00,  2.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.9600069522857666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 41/89 [01:35<01:56,  2.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.0457301139831543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 42/89 [01:38<01:52,  2.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.2817041873931885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 43/89 [01:40<01:47,  2.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.0955835580825806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 49%|████▉     | 44/89 [01:42<01:43,  2.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.2451567649841309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 51%|█████     | 45/89 [01:44<01:40,  2.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.2187494039535522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 46/89 [01:47<01:38,  2.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.876639723777771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 47/89 [01:49<01:36,  2.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.230655550956726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 48/89 [01:51<01:33,  2.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1627627611160278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 49/89 [01:53<01:29,  2.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1581051349639893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 50/89 [01:55<01:27,  2.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.0037415027618408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 51/89 [01:58<01:24,  2.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.2494161128997803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 52/89 [02:00<01:24,  2.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1788129806518555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|█████▉    | 53/89 [02:02<01:22,  2.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.237396001815796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████    | 54/89 [02:05<01:19,  2.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.2167062759399414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 55/89 [02:07<01:17,  2.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1485682725906372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 56/89 [02:09<01:14,  2.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1553086042404175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 57/89 [02:12<01:21,  2.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1116650104522705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 58/89 [02:15<01:16,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.8126764297485352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▋   | 59/89 [02:17<01:12,  2.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.3020132780075073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 60/89 [02:19<01:08,  2.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1719694137573242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|██████▊   | 61/89 [02:21<01:04,  2.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1400659084320068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|██████▉   | 62/89 [02:24<01:02,  2.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1705652475357056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████   | 63/89 [02:26<01:02,  2.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.2931139469146729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 64/89 [02:29<01:00,  2.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.2301812171936035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 65/89 [02:31<00:56,  2.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.9532210230827332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 66/89 [02:33<00:53,  2.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.0514674186706543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 67/89 [02:35<00:50,  2.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.2231554985046387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▋  | 68/89 [02:38<00:49,  2.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.9867231845855713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 69/89 [02:40<00:48,  2.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.958823025226593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|███████▊  | 70/89 [02:43<00:45,  2.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.8839763402938843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|███████▉  | 71/89 [02:45<00:41,  2.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.2182955741882324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%|████████  | 72/89 [02:47<00:40,  2.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.0509322881698608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 73/89 [02:50<00:37,  2.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.8272466659545898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 74/89 [02:52<00:35,  2.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.7924196720123291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 75/89 [02:54<00:32,  2.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.326623797416687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 76/89 [02:57<00:30,  2.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.0976656675338745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 77/89 [02:59<00:27,  2.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.8243111968040466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 78/89 [03:01<00:24,  2.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.9802748560905457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▉ | 79/89 [03:03<00:22,  2.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.257523536682129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|████████▉ | 80/89 [03:06<00:21,  2.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.381665825843811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████ | 81/89 [03:08<00:18,  2.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.9032096862792969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 82/89 [03:11<00:16,  2.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.9001035094261169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 83/89 [03:13<00:13,  2.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.8313341736793518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 84/89 [03:15<00:11,  2.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.0272873640060425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 85/89 [03:17<00:09,  2.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 0.8720780611038208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 86/89 [03:20<00:07,  2.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1019911766052246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 87/89 [03:22<00:04,  2.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.1182856559753418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 99%|█████████▉| 88/89 [03:24<00:02,  2.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch Loss: 1.2054214477539062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 89/89 [03:26<00:00,  2.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled output shape: torch.Size([5, 768])\n",
            "Test Batch Loss: 0.9613356590270996\n",
            "total test loss: 97.64082020521164\n",
            "Average Test Loss: 1.0971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/676 [00:00<?, ?it/s]Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 1/676 [00:03<36:23,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7238386869430542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 2/676 [00:06<37:26,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6722146272659302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 3/676 [00:09<37:00,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.792107343673706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 4/676 [00:13<36:15,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.649448573589325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 5/676 [00:16<35:53,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6799384951591492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 6/676 [00:19<36:56,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.551459789276123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 7/676 [00:23<38:11,  3.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7208267450332642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 8/676 [00:26<37:43,  3.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7371383309364319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|▏         | 9/676 [00:29<36:57,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5057317018508911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|▏         | 10/676 [00:33<36:57,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5168825387954712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 11/676 [00:36<36:40,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7383716106414795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 12/676 [00:39<35:56,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6759291887283325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 13/676 [00:42<35:52,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.37365347146987915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 14/676 [00:46<36:18,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.3452828824520111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 15/676 [00:49<35:47,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.78103107213974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 16/676 [00:52<35:24,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5448722839355469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 17/676 [00:55<35:05,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5166589021682739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 18/676 [00:58<35:18,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5958996415138245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 19/676 [01:02<35:10,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.3788731098175049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 20/676 [01:05<34:44,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4099607467651367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 21/676 [01:08<34:35,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9828866720199585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 22/676 [01:11<34:45,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6985349059104919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 23/676 [01:14<34:40,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7268770337104797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▎         | 24/676 [01:17<34:31,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4437544345855713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▎         | 25/676 [01:21<34:14,  3.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.765274167060852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 26/676 [01:24<34:29,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.756385326385498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 27/676 [01:27<34:52,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4611511826515198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 28/676 [01:30<34:47,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.53194260597229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 29/676 [01:34<34:41,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6357070207595825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 30/676 [01:37<34:36,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9327720403671265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▍         | 31/676 [01:40<34:33,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4792754650115967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▍         | 32/676 [01:43<34:19,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.38178759813308716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▍         | 33/676 [01:46<34:03,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5324059724807739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 34/676 [01:50<34:17,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.3900224566459656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 35/676 [01:53<34:15,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.670687735080719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 36/676 [01:56<34:12,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5828936100006104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 37/676 [01:59<33:56,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8096446394920349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 38/676 [02:02<33:41,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.35085394978523254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 39/676 [02:05<33:55,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6060348749160767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 40/676 [02:09<33:38,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.028996467590332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 41/676 [02:12<33:23,  3.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6438103914260864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 42/676 [02:15<33:16,  3.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4944263696670532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▋         | 43/676 [02:18<33:43,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5583579540252686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 44/676 [02:22<34:34,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.3777770698070526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 45/676 [02:25<34:30,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.19332364201545715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 46/676 [02:28<34:08,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.48667868971824646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 47/676 [02:32<36:35,  3.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.923096776008606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 48/676 [02:35<35:22,  3.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8948219418525696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 49/676 [02:38<34:33,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.532063901424408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 50/676 [02:42<34:27,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7025596499443054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 51/676 [02:45<34:12,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8260616064071655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 52/676 [02:48<33:34,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9636936187744141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 53/676 [02:51<33:11,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7090933322906494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 54/676 [02:55<36:06,  3.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6540059447288513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 55/676 [02:58<35:10,  3.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.37900274991989136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 56/676 [03:02<34:15,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6089796423912048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 57/676 [03:05<33:48,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.2486226558685303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▊         | 58/676 [03:08<33:35,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4463357925415039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▊         | 59/676 [03:11<33:19,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.635306715965271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 60/676 [03:15<33:59,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5230884552001953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 61/676 [03:18<33:25,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4967484474182129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 62/676 [03:21<33:25,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6852283477783203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 63/676 [03:25<36:47,  3.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5486191511154175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 64/676 [03:29<36:05,  3.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.546436071395874\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|▉         | 65/676 [03:32<34:46,  3.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5371883511543274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|▉         | 66/676 [03:35<34:32,  3.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.3732927739620209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|▉         | 67/676 [03:39<33:44,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8631504774093628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 68/676 [03:42<33:15,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.49237266182899475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 69/676 [03:45<32:51,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.29992860555648804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 70/676 [03:49<34:19,  3.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7937986254692078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 71/676 [03:52<33:40,  3.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4587373435497284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 72/676 [03:55<33:17,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8407430052757263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 73/676 [03:59<34:16,  3.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8707889318466187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 74/676 [04:02<34:28,  3.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.956635594367981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 75/676 [04:05<33:31,  3.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.531248927116394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 76/676 [04:08<32:49,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.472772479057312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█▏        | 77/676 [04:12<32:18,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.515404224395752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 78/676 [04:15<32:18,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9498128890991211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 79/676 [04:18<31:59,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5450209379196167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 80/676 [04:21<32:06,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6620602011680603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 81/676 [04:24<31:49,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.561331570148468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 82/676 [04:28<32:11,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.534997284412384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 83/676 [04:31<32:20,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.005958080291748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 84/676 [04:34<31:48,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5831296443939209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 85/676 [04:37<31:30,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6749183535575867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 86/676 [04:41<31:39,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5013902187347412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 87/676 [04:44<33:16,  3.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.46197861433029175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 88/676 [04:48<34:04,  3.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6443281173706055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 89/676 [04:51<33:26,  3.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.2639954090118408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 90/676 [04:55<33:05,  3.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 1.238126516342163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 91/676 [04:58<32:36,  3.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5965542197227478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▎        | 92/676 [05:02<33:29,  3.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.9795516729354858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 93/676 [05:06<35:56,  3.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5033517479896545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 94/676 [05:09<34:58,  3.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6824344396591187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 95/676 [05:13<34:06,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7053155303001404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 96/676 [05:16<33:07,  3.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8121750354766846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 97/676 [05:19<32:12,  3.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6150423288345337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 98/676 [05:22<32:13,  3.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8348672389984131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▍        | 99/676 [05:26<31:46,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.6293237805366516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▍        | 100/676 [05:29<31:32,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5164333581924438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▍        | 101/676 [05:32<31:07,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7025931477546692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 102/676 [05:35<31:05,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7904879450798035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 103/676 [05:38<31:00,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7469592094421387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 104/676 [05:42<30:42,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4118514060974121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 105/676 [05:45<30:38,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.675291895866394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 106/676 [05:48<30:49,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5360323786735535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 107/676 [05:51<30:53,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8013303875923157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 108/676 [05:55<30:29,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.47942233085632324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 109/676 [05:58<30:11,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7251759171485901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▋        | 110/676 [06:01<30:17,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.45361512899398804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▋        | 111/676 [06:04<30:51,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5293961763381958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 112/676 [06:08<30:33,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.5561646819114685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 113/676 [06:11<30:06,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7017934322357178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 114/676 [06:14<30:08,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.3363824486732483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 115/676 [06:18<31:36,  3.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.7638689279556274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 116/676 [06:21<32:02,  3.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.4077415466308594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 117/676 [06:25<32:32,  3.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.8756738901138306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual embeds shape after extraction: torch.Size([16, 1, 2048])\n",
            "Pooled output shape: torch.Size([16, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 118/676 [06:29<33:35,  3.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Loss: 0.745746374130249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            " 17%|█▋        | 118/676 [06:31<30:50,  3.32s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-3feb23ef372a>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0;31m# Tokenize captions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mvisual_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisual_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Visual embeds shape after extraction:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisual_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-7bc4daa17104>\u001b[0m in \u001b[0;36mvisual_embeddings\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Load and preprocess the image using the feature extractor for ViT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m# inputs = {k: v.to(device) for k, v in inputs.items()}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;34m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m                 )\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mvalid_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/vit/image_processing_vit.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_resize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             images = [\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/vit/image_processing_vit.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_resize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             images = [\n\u001b[0;32m--> 251\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/vit/image_processing_vit.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The `size` dictionary must contain the keys `height` and `width`. Got {size.keys()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0moutput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"height\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"width\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         return resize(\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;31m# If an image was rescaled to be in the range [0, 255] before converting to a PIL image, then we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;31m# rescale it back to the original range.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mresized_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdo_rescale\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresized_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresized_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36mrescale\u001b[0;34m(image, scale, data_format, dtype, input_data_format)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mrescaled_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_channel_dimension_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescaled_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mrescaled_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrescaled_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrescaled_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisualBertForClassification(num_labels).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint_path = \"/content/visual_bert_epoch_3.pth\"  # Adjust the path as needed\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "# Load model and optimizer states\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "start_epoch = checkpoint['epoch'] + 1  # Start from the next epoch\n",
        "total_loss = checkpoint['loss']  # You can use this for logging\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Continue training\n",
        "num_epochs = 2\n",
        "for epoch in range(start_epoch, start_epoch + num_epochs):\n",
        "    model.train()\n",
        "    for batch in tqdm(train_loader):\n",
        "        images = batch['image_transformed'].to(device)\n",
        "        caption = batch['caption']\n",
        "        label = batch['label'].to(device)\n",
        "        images = images.squeeze(1).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images = (images + 1) / 2  # Normalize images\n",
        "        images = torch.clamp(images, 0, 1)\n",
        "\n",
        "        # Tokenize captions\n",
        "        inputs = tokenizer(caption, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512).to(device)\n",
        "        visual_embeds = visual_embeddings(images).to(device)\n",
        "\n",
        "        # Create visual token type and attention masks\n",
        "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).to(device)\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float).to(device)\n",
        "\n",
        "        # Prepare inputs for the model\n",
        "        inputs.update({\n",
        "            \"visual_embeds\": visual_embeds,\n",
        "            \"visual_token_type_ids\": visual_token_type_ids,\n",
        "            \"visual_attention_mask\": visual_attention_mask,\n",
        "        })\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        # Calculate loss\n",
        "        loss_value = loss_fn(outputs, label)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss_value.item()\n",
        "        print(f\"Batch Loss: {loss_value.item()}\")  # Loss for the current batch\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    avg_epoch_loss = total_loss / len(train_loader)\n",
        "    print(f\"total loss: {total_loss}\")\n",
        "    print(f\"Epoch {epoch + 1}/{start_epoch + num_epochs}, Average Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "    # Save the model checkpoint after each epoch\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': total_loss\n",
        "    }, f\"visual_bert_epoch_{epoch}.pth\")\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Model weights saved for epoch {epoch}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "2JylleNHnMCx",
        "outputId": "e0543dcd-6fa4-47e6-8f80-309826b910fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-079ed84259c3>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
            "  0%|          | 0/676 [00:00<?, ?it/s]Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0%|          | 0/676 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 11.06 MiB is free. Process 6215 has 14.73 GiB memory in use. Of the allocated memory 14.13 GiB is allocated by PyTorch, and 484.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-079ed84259c3>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-c6dc625bd3e3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, visual_embeds, visual_token_type_ids, visual_attention_mask)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Get outputs from VisualBERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         outputs = self.visualbert(\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/visual_bert/modeling_visual_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, visual_embeds, visual_attention_mask, visual_token_type_ids, image_text_alignment, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    825\u001b[0m                 \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/visual_bert/modeling_visual_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    413\u001b[0m                 )\n\u001b[1;32m    414\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m                 \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/visual_bert/modeling_visual_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     ):\n\u001b[0;32m--> 358\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    359\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/visual_bert/modeling_visual_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     ):\n\u001b[0;32m--> 300\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/visual_bert/modeling_visual_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 11.06 MiB is free. Process 6215 has 14.73 GiB memory in use. Of the allocated memory 14.13 GiB is allocated by PyTorch, and 484.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    }
  ]
}